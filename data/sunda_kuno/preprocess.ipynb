{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0]\n",
      "(29, 28, 28, 1)\n",
      "(13, 28, 28, 1)\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# from PIL import Image\n",
    "# from numpy import asarray\n",
    "# import numpy as np\n",
    "\n",
    "# # load the image\n",
    "# image = Image.open('HA_3.png')\n",
    "# # convert image to numpy array\n",
    "# data = asarray(image)\n",
    "# print(type(data))\n",
    "# # summarize shape\n",
    "# print(data.shape)\n",
    "\n",
    "# # create Pillow image\n",
    "# image2 = Image.fromarray(data)\n",
    "# print(type(image2))\n",
    "\n",
    "# # summarize image details\n",
    "# print(image2.mode)\n",
    "# print(image2.size)\n",
    "\n",
    "# im = np.array(Image.open('HA_3.png').convert('L').resize((30,30))) #you can pass multiple arguments in single line\n",
    "# print(type(im))\n",
    "\n",
    "# Image.fromarray(im).save('HA_3_transformed.png')\n",
    "\n",
    "# print(im)\n",
    "\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "from PIL import Image, ImageFilter, ImageEnhance\n",
    "import PIL.ImageOps\n",
    "import os\n",
    "import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def resize_only(file_name):\n",
    "\timg = Image.open(file_name).convert('LA')\n",
    "\t# img = img.filter(ImageFilter.SMOOTH)\n",
    "\t# img = img.filter(ImageFilter.SHARPEN)\n",
    "\timg = img.resize((28, 28))\n",
    "\t# img = PIL.ImageOps.invert(img)\n",
    "\timg.save(\"temp.png\")\n",
    "\timg = cv.imread('temp.png', 0)\n",
    "\timg = cv.bitwise_not(img)\n",
    "\timg =  Image.fromarray(img)\n",
    "\t\n",
    "\treturn img\n",
    "\n",
    "def resize(file_name, invert):\n",
    "\t# image = cv.imread('%s' % file_name,0)\n",
    "\timg = Image.open(file_name).convert('LA')\n",
    "# \tif invert==True:\n",
    "# \t\timg = PIL.ImageOps.invert(img)\n",
    "\tenhancer = ImageEnhance.Contrast(img)\n",
    "\t\n",
    "\tfactor = 2.0\n",
    "\timg = enhancer.enhance(factor)\n",
    "\timg = img.filter(ImageFilter.SMOOTH)\n",
    "\timg = img.filter(ImageFilter.SHARPEN)\n",
    "\timg = img.resize((28, 28))\n",
    "\t# print(type(img))    \n",
    "    \n",
    "\timg.save(\"temp.png\")\n",
    "    \n",
    "\timg = cv.imread('temp.png', 0)\n",
    "    \n",
    "\tif invert==True:\n",
    "\t\timg = cv.bitwise_not(img)\n",
    "\tret,th = cv.threshold(img,0,255,cv.THRESH_BINARY+cv.THRESH_OTSU)\n",
    "    \n",
    "\tblur = cv.GaussianBlur(img,(5,5),0)\n",
    "\tret2,th2 = cv.threshold(blur,0,255,cv.THRESH_BINARY+cv.THRESH_OTSU)\n",
    "    \n",
    "\tmask = np.where(th ==  0, th2, 255)\n",
    "    \n",
    "\timg = np.where(mask == 0, img, 255)\n",
    "\n",
    "    # create a CLAHE object (Arguments are optional).\n",
    "\tclahe = cv.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "\timg = clahe.apply(img)\n",
    "\n",
    "\t_,img_remove = cv.threshold(img,95,255,cv.THRESH_TOZERO)\n",
    "\timg_remove = cv.bitwise_not(img_remove)\n",
    "\t_,img = cv.threshold(img_remove,95,255,cv.THRESH_TOZERO)\n",
    "\n",
    "\t# img = cv.bitwise_not(img)\n",
    "    \n",
    "\timg =  Image.fromarray(img)\n",
    "    \n",
    "\t# print(img)\n",
    "\t# pix = np.array(img)\n",
    "\t# print(type(pix))\n",
    "\t# print(pix.shape)\n",
    "\t# print(pix)\n",
    "\t# pix = pix[:, :, 0]\n",
    "\t# print(type(pix))\n",
    "\t# print(pix.shape)\n",
    "\t# print(pix)\n",
    "\treturn img\n",
    "\t# img.save(\"resized_\"+file_name)\n",
    "\n",
    "def preprocess():\n",
    "\t# crop(str(sys.argv))\n",
    "\tscript_dir = os.path.abspath('')\n",
    "\ti=[\"A\",\"BA\",\"CA\",\"DA\",\"GA\",\"HA\",\"I\",\"JA\",\"KA\",\"LA\",\"MA\",\"NA\",\"NGA\",\"NYA\",\"PA\",\"PANELENG\",\"PANEULEUNG\",\"PANGHULU\",\"PANGLAYAR\",\"PANOLONG\",\"PANYUKU\",\"PATEN\",\"RA\",\"SA\",\"TA\",\"U\",\"WA\",\"YA\"]\n",
    "\tj=[30,47,19,67,37,27,16,21,60,60,56,120,25,14,61,42,63,60,23,35,36,84,56,90,78,18,24,22]\n",
    "\tk=[12,30,7,45,16,11,7,9,40,25,24,80,10,6,39,28,42,40,10,24,24,36,36,60,52,7,10,9]\n",
    "\tganed = [0,2,4,5,6,7,12,13,18,19,20,25,26,27]\n",
    "\n",
    "\tfinal_np = np.array([])\n",
    "\tfinal_label = np.array([])\n",
    "\n",
    "    #   train_image to ready_to_train\n",
    "\tfor x in range(0, len(i), 1):\n",
    "\t\tsumImg = 0\n",
    "\t\ttrain_directory_path = os.path.join(script_dir,\"train_image\")\n",
    "\t\ttrain_count_image = len(glob.glob1(train_directory_path,\"%s_*.png\" % (i[x])))\n",
    "\n",
    "\t\tfor y in range(1, train_count_image+1):\n",
    "\t\t\trel_path = \"train_image/%s_%s.png\" % (i[x],str(y))\n",
    "\t\t\tabs_file_path = os.path.join(script_dir, rel_path)\n",
    "            \n",
    "\t\t\timg = resize_only(abs_file_path)\n",
    "\t\t\timg.save(\"train-test_image/%s_%s.png\" % (i[x],str(y)))\n",
    "\t\t\t# img = resize(abs_file_path, invert=False)\n",
    "\t\t\t# img.save(\"ready_to_train_3/%s_%s.png\" % (i[x],str(y)))\n",
    "\t\tsumImg = sumImg + train_count_image\n",
    "\n",
    "\t\ttest_directory_path = os.path.join(script_dir,\"test_image/%s\" % i[x])\n",
    "\t\ttest_count_image = len(glob.glob1(test_directory_path,\"%s_*.png\" % (i[x])))\n",
    "\n",
    "\t\tfor y in range(1, test_count_image+1):\n",
    "\t\t\trel_path = os.path.join(test_directory_path,\"%s_%s.png\" % (i[x],str(y)))\n",
    "\t\t\tabs_file_path = os.path.join(script_dir, rel_path)\n",
    "            \n",
    "\t\t\timg = resize_only(abs_file_path)\n",
    "\t\t\timg.save(\"train-test_image/%s_%s.png\" % (i[x],str(y+sumImg)))\n",
    "\t\tsumImg = sumImg + test_count_image\n",
    "\n",
    "\t\tif x == 0:\n",
    "\t\t\tfor y in range(1, sumImg+1):\n",
    "\t\t\t\trel_path = os.path.join(\"train-test_image\",\"%s_%s.png\" % (i[x],str(y)))\n",
    "\t\t\t\tabs_file_path = os.path.join(script_dir, rel_path)\n",
    "\n",
    "\t\t\t\timg = Image.open(abs_file_path)\n",
    "\t\t\t\timg = np.array(img)\n",
    "\t\t\t\timg = img[:, :]\n",
    "\t\t\t\tfinal_np = np.append(final_np,img)\n",
    "\t\t\t\tfinal_label = np.append(final_label,x)\n",
    "\t\t\tfinal_np = final_np.reshape((sumImg, 28, 28, 1)).astype(np.float32)\n",
    "\t\t\tfinal_label = final_label.reshape((sumImg)).astype(np.int32)\n",
    "\t\t\tdata_train, data_test, labels_train, labels_test = train_test_split(final_np, final_label, test_size=0.30, random_state=42)\n",
    "\t\t\tprint(final_label)\n",
    "\t\t\tprint(data_train.shape)\n",
    "\t\t\tprint(data_test.shape)\n",
    "\t\t\tprint(labels_train)\n",
    "\t\t\tprint(labels_test)\n",
    "# #   GAN_image to ready_to_train\n",
    "# \tfor x in ganed:\n",
    "# \t\tfor y in range(1, 26):\n",
    "# \t\t\trel_path = \"GAN_generated_images/%s_%s.png\" % (i[x],str(y+j[x]))\n",
    "# # \t\t\trel_path = \"ready_to_train/%s_%s.png\" % (i[x],str(y))\n",
    "# \t\t\tabs_file_path = os.path.join(script_dir, rel_path)\n",
    "            \n",
    "# \t\t\timg = resize(abs_file_path, invert=True)\n",
    "# \t\t\timg.save(\"ready_to_train_3/%s_%s.png\" % (i[x],str(y+j[x])))\n",
    "\n",
    "# #   test_image to ready_to_test (validation)\n",
    "# \tsumImg = 0\n",
    "# \tfinal_np = np.array([])\n",
    "# \tfinal_label = np.array([])\n",
    "# \tfor x in range(0, len(i), 1):\n",
    "# \t\tsumImg = sumImg + k[x]\n",
    "# \t\tfor y in range(1, int((k[x])/2)+1):\n",
    "# \t\t\trel_path = \"test_image/%s/%s_%s.png\" % (i[x],i[x],str(y))\n",
    "# \t\t\t# rel_path = \"ready_to_train/%s_%s.png\" % (i[x],str(y))\n",
    "# \t\t\tabs_file_path = os.path.join(script_dir, rel_path)\n",
    "\n",
    "# \t\t\t# img = resize(abs_file_path, invert=False)\n",
    "# \t\t\t# img.save(\"ready_to_test_2/%s_%s.png\" % (i[x],str(y)))\n",
    "# \t\t\timg = resize_only(abs_file_path)\n",
    "# \t\t\timg.save(\"ready_to_test/%s_%s.png\" % (i[x],str(y)))\n",
    "\n",
    "# # \ttest_image to ready_to_test (test)\n",
    "# \tsumImg = 0\n",
    "# \tfor x in range(0, len(i), 1):\n",
    "# \t\tsumImg = sumImg - (-k[x]//2)\n",
    "# \t\tfor y in range((-(-k[x])//2)+1, k[x]+1):\n",
    "# #             rel_path = \"test_image/%s/%s_%s.png\" % (i[x],i[x],str(y))\n",
    "# \t\t\trel_path = \"test_image/%s/%s_%s.png\" % (i[x],i[x],str(y))\n",
    "# \t\t\t# rel_path = \"ready_to_train/%s_%s.png\" % (i[x],str(y))\n",
    "# \t\t\tabs_file_path = os.path.join(script_dir, rel_path)\n",
    "            \n",
    "# \t\t\timg = Image.open(abs_file_path)\n",
    "# \t\t\timg = np.array(img)\n",
    "# \t\t\timg = img[:, :, 0]\n",
    "# \t\t\timg = resize_only(abs_file_path)\n",
    "# \t\t\timg.save(\"ready_to_test/%s_%s.png\" % (i[x],str(y)))\n",
    "# \t\t\t# img = resize(abs_file_path, invert=False)\n",
    "# \t\t\t# img.save(\"ready_to_test_2/%s_%s.png\" % (i[x],str(y)))\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "# \tmain()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sunda(aksara):\n",
    "    script_dir = os.path.abspath('')\n",
    "    i=[\"A\",\"BA\",\"CA\",\"DA\",\"GA\",\"HA\",\"I\",\"JA\",\"KA\",\"LA\",\"MA\",\"NA\",\"NGA\",\"NYA\",\"PA\",\"PANELENG\",\"PANEULEUNG\",\"PANGHULU\",\"PANGLAYAR\",\"PANOLONG\",\"PANYUKU\",\"PATEN\",\"RA\",\"SA\",\"TA\",\"U\",\"WA\",\"YA\"]\n",
    "    j=[30,47,19,67,37,27,16,21,60,60,56,120,25,14,61,42,63,60,23,35,36,84,56,90,78,18,24,22]\n",
    "    k=[12,30,7,45,16,11,7,9,40,25,24,80,10,6,39,28,42,40,10,24,24,36,36,60,52,7,10,9]\n",
    "    ganed = [0,2,4,5,6,7,12,13,18,19,20,25,26,27]\n",
    "\n",
    "    final_np = np.array([])\n",
    "    final_label = np.array([])\n",
    "\n",
    "    #   train_image to ready_to_train\n",
    "#for x in range(0, len(i), 1):\n",
    "    sumImg = 0\n",
    "    directory_path = os.path.join(script_dir,\"train-test_image\")\n",
    "    count_image = len(glob.glob1(directory_path,\"%s_*.png\" % (i[aksara])))\n",
    "\n",
    "    for y in range(1, count_image+1):\n",
    "        abs_file_path = os.path.join(directory_path, \"%s_%s.png\" % (i[aksara],str(y)))\n",
    "        img = Image.open(abs_file_path)\n",
    "        img = np.array(img)\n",
    "        img = img[:, :]\n",
    "        final_np = np.append(final_np,img)\n",
    "        final_label = np.append(final_label,aksara)\n",
    "    sumImg = sumImg + count_image\n",
    "    final_np = final_np.reshape((sumImg, 28, 28, 1)).astype(np.float32)\n",
    "    final_label = final_label.reshape((sumImg)).astype(np.int32)\n",
    "    data_train, data_test, labels_train, labels_test = train_test_split(final_np, final_label, test_size=0.30, random_state=42)\n",
    "    return(data_train,sumImg)\n",
    "#     print(final_label)\n",
    "#     print(data_train.shape)\n",
    "#     print(data_test.shape)\n",
    "#     print(labels_train)\n",
    "#     print(labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\adjie\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\adjie\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\adjie\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\adjie\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\adjie\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\adjie\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\adjie\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 533,505\n",
      "Trainable params: 533,505\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 256)               25856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 784)               803600    \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 1,493,520\n",
      "Trainable params: 1,489,936\n",
      "Non-trainable params: 3,584\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From C:\\Users\\adjie\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adjie\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26-0 [D loss: 0.742610, acc.: 43.75%] [G loss: 0.857420]\n",
      "26-1000 [D loss: 0.962812, acc.: 35.42%] [G loss: 3.058529]\n",
      "26-2000 [D loss: 0.734340, acc.: 51.04%] [G loss: 1.886794]\n",
      "26-3000 [D loss: 0.317370, acc.: 85.42%] [G loss: 3.307196]\n",
      "26-4000 [D loss: 0.178927, acc.: 93.75%] [G loss: 3.203070]\n",
      "26-5000 [D loss: 0.098615, acc.: 96.88%] [G loss: 5.780303]\n",
      "26-6000 [D loss: 0.169417, acc.: 94.79%] [G loss: 3.939962]\n",
      "26-7000 [D loss: 0.104205, acc.: 95.83%] [G loss: 3.565294]\n",
      "26-8000 [D loss: 0.191808, acc.: 93.75%] [G loss: 3.142579]\n",
      "26-9000 [D loss: 0.260847, acc.: 87.50%] [G loss: 2.447149]\n",
      "26-10000 [D loss: 0.265107, acc.: 90.62%] [G loss: 3.378236]\n",
      "26-11000 [D loss: 0.133662, acc.: 95.83%] [G loss: 3.884812]\n",
      "26-12000 [D loss: 0.288578, acc.: 89.58%] [G loss: 2.994449]\n",
      "26-13000 [D loss: 0.174763, acc.: 93.75%] [G loss: 4.721326]\n",
      "26-14000 [D loss: 0.161531, acc.: 95.83%] [G loss: 4.100693]\n",
      "26-15000 [D loss: 0.222556, acc.: 91.67%] [G loss: 4.909410]\n",
      "26-16000 [D loss: 0.203225, acc.: 92.71%] [G loss: 3.462001]\n",
      "26-17000 [D loss: 0.114561, acc.: 97.92%] [G loss: 4.443793]\n",
      "26-18000 [D loss: 0.247781, acc.: 90.62%] [G loss: 4.202017]\n",
      "26-19000 [D loss: 0.276767, acc.: 91.67%] [G loss: 3.425659]\n",
      "26-20000 [D loss: 0.190030, acc.: 92.71%] [G loss: 3.668303]\n",
      "26-21000 [D loss: 0.174965, acc.: 93.75%] [G loss: 5.568208]\n",
      "26-22000 [D loss: 0.299230, acc.: 89.58%] [G loss: 4.186258]\n",
      "26-23000 [D loss: 0.192612, acc.: 93.75%] [G loss: 3.937064]\n",
      "26-24000 [D loss: 0.176250, acc.: 94.79%] [G loss: 3.062786]\n",
      "26-25000 [D loss: 0.196436, acc.: 93.75%] [G loss: 4.178473]\n",
      "26-26000 [D loss: 0.178591, acc.: 91.67%] [G loss: 4.018551]\n",
      "26-27000 [D loss: 0.101977, acc.: 95.83%] [G loss: 4.313894]\n",
      "26-28000 [D loss: 0.151756, acc.: 94.79%] [G loss: 3.087352]\n",
      "26-29000 [D loss: 0.166853, acc.: 94.79%] [G loss: 3.914813]\n",
      "26-30000 [D loss: 0.152417, acc.: 96.88%] [G loss: 3.621068]\n",
      "-----------------------------------------------------------------\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_2 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 533,505\n",
      "Trainable params: 533,505\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_11 (Dense)             (None, 256)               25856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)   (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 784)               803600    \n",
      "_________________________________________________________________\n",
      "reshape_2 (Reshape)          (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 1,493,520\n",
      "Trainable params: 1,489,936\n",
      "Non-trainable params: 3,584\n",
      "_________________________________________________________________\n",
      "27-0 [D loss: 0.715727, acc.: 47.92%] [G loss: 0.475097]\n",
      "27-1000 [D loss: 0.327738, acc.: 89.58%] [G loss: 4.178333]\n",
      "27-2000 [D loss: 0.189753, acc.: 95.83%] [G loss: 3.431266]\n",
      "27-3000 [D loss: 0.217759, acc.: 93.75%] [G loss: 3.851110]\n",
      "27-4000 [D loss: 0.247008, acc.: 90.62%] [G loss: 3.511636]\n",
      "27-5000 [D loss: 0.193296, acc.: 94.79%] [G loss: 3.130255]\n",
      "27-6000 [D loss: 0.235048, acc.: 91.67%] [G loss: 3.467942]\n",
      "27-7000 [D loss: 0.288950, acc.: 91.67%] [G loss: 3.597197]\n",
      "27-8000 [D loss: 0.235270, acc.: 89.58%] [G loss: 3.353027]\n",
      "27-9000 [D loss: 0.227282, acc.: 92.71%] [G loss: 4.480735]\n",
      "27-10000 [D loss: 0.185136, acc.: 95.83%] [G loss: 3.972294]\n",
      "27-11000 [D loss: 0.323898, acc.: 88.54%] [G loss: 3.766936]\n",
      "27-12000 [D loss: 0.297936, acc.: 90.62%] [G loss: 3.169785]\n",
      "27-13000 [D loss: 0.225490, acc.: 92.71%] [G loss: 3.203372]\n",
      "27-14000 [D loss: 0.265053, acc.: 89.58%] [G loss: 3.327079]\n",
      "27-15000 [D loss: 0.166028, acc.: 95.83%] [G loss: 3.988756]\n",
      "27-16000 [D loss: 0.207219, acc.: 93.75%] [G loss: 3.773907]\n",
      "27-17000 [D loss: 0.167805, acc.: 91.67%] [G loss: 4.073081]\n",
      "27-18000 [D loss: 0.143568, acc.: 92.71%] [G loss: 4.140694]\n",
      "27-19000 [D loss: 0.266280, acc.: 89.58%] [G loss: 4.181191]\n",
      "27-20000 [D loss: 0.177236, acc.: 94.79%] [G loss: 3.424593]\n",
      "27-21000 [D loss: 0.232273, acc.: 91.67%] [G loss: 3.862115]\n",
      "27-22000 [D loss: 0.243906, acc.: 90.62%] [G loss: 3.191777]\n",
      "27-23000 [D loss: 0.223884, acc.: 87.50%] [G loss: 3.090897]\n",
      "27-24000 [D loss: 0.084800, acc.: 97.92%] [G loss: 3.748001]\n",
      "27-25000 [D loss: 0.207901, acc.: 89.58%] [G loss: 3.433743]\n",
      "27-26000 [D loss: 0.141348, acc.: 95.83%] [G loss: 3.497976]\n",
      "27-27000 [D loss: 0.210104, acc.: 90.62%] [G loss: 3.937610]\n",
      "27-28000 [D loss: 0.094107, acc.: 96.88%] [G loss: 4.789553]\n",
      "27-29000 [D loss: 0.132259, acc.: 93.75%] [G loss: 4.244658]\n",
      "27-30000 [D loss: 0.192928, acc.: 90.62%] [G loss: 3.563001]\n",
      "-----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "import os\n",
    "import scipy\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class GAN():\n",
    "    def __init__(self):\n",
    "        self.img_rows = 28\n",
    "        self.img_cols = 28\n",
    "        self.channels = 1\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "        self.latent_dim = 100\n",
    "\n",
    "        optimizer = Adam(0.0002, 0.5)\n",
    "\n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(loss='binary_crossentropy',\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "        # Build the generator\n",
    "        self.generator = self.build_generator()\n",
    "\n",
    "        # The generator takes noise as input and generates imgs\n",
    "        z = Input(shape=(self.latent_dim,))\n",
    "        img = self.generator(z)\n",
    "\n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "\n",
    "        # The discriminator takes generated images as input and determines validity\n",
    "        validity = self.discriminator(img)\n",
    "\n",
    "        # The combined model  (stacked generator and discriminator)\n",
    "        # Trains the generator to fool the discriminator\n",
    "        self.combined = Model(z, validity)\n",
    "        self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "\n",
    "    def build_generator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Dense(256, input_dim=self.latent_dim))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(512))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(1024))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(np.prod(self.img_shape), activation='tanh'))\n",
    "        model.add(Reshape(self.img_shape))\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        noise = Input(shape=(self.latent_dim,))\n",
    "        img = model(noise)\n",
    "\n",
    "        return Model(noise, img)\n",
    "\n",
    "    def build_discriminator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Flatten(input_shape=self.img_shape))\n",
    "        model.add(Dense(512))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dense(256))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        model.summary()\n",
    "\n",
    "        img = Input(shape=self.img_shape)\n",
    "        validity = model(img)\n",
    "\n",
    "        return Model(img, validity)\n",
    "\n",
    "    def train(self, aksara, aksara_num, epochs, batch_size=128, sample_interval=50):\n",
    "\n",
    "        # Load the dataset\n",
    "#         preprocess()\n",
    "        X_train, sumTrain= get_sunda(aksara)\n",
    "        X_train = X_train[:,:,:,0]\n",
    "#         (X_train, _), (_, _) = mnist.load_data()\n",
    "\n",
    "        # Rescale -1 to 1\n",
    "        X_train = X_train / 127.5 - 1.\n",
    "        X_train = np.expand_dims(X_train, axis=3)\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = np.ones((batch_size, 1))\n",
    "        fake = np.zeros((batch_size, 1))\n",
    "        \n",
    "        min_loss = 100\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "\n",
    "            # Select a random batch of images\n",
    "            idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "            imgs = X_train[idx]\n",
    "\n",
    "            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "\n",
    "            # Generate a batch of new images\n",
    "            gen_imgs = self.generator.predict(noise)\n",
    "\n",
    "            # Train the discriminator\n",
    "            d_loss_real = self.discriminator.train_on_batch(imgs, valid)\n",
    "            d_loss_fake = self.discriminator.train_on_batch(gen_imgs, fake)\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Generator\n",
    "            # ---------------------\n",
    "\n",
    "            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "\n",
    "            # Train the generator (to have the discriminator label samples as valid)\n",
    "            g_loss = self.combined.train_on_batch(noise, valid)\n",
    "\n",
    "            # Plot the progress\n",
    "#             print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
    "\n",
    "            # If at save interval => save generated image samples\n",
    "            if epoch % sample_interval == 0:\n",
    "                print (\"%d-%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (aksara, epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
    "                self.sample_images(epoch, aksara, sumTrain, aksara_num)\n",
    "            \n",
    "#             if d_loss[0] + g_loss < min_loss and epoch > 10000:\n",
    "#                 min_loss = d_loss[0] + g_loss\n",
    "#                 self.sample_images(epoch, aksara, sumTrain, aksara_num)\n",
    "\n",
    "    def sample_images(self, epoch, aksara, sumTrain, aksara_num):\n",
    "        aksaraList=[\"A\",\"BA\",\"CA\",\"DA\",\"GA\",\"HA\",\"I\",\"JA\",\"KA\",\"LA\",\"MA\",\"NA\",\"NGA\",\"NYA\",\"PA\",\"PANELENG\",\"PANEULEUNG\",\"PANGHULU\",\"PANGLAYAR\",\"PANOLONG\",\"PANYUKU\",\"PATEN\",\"RA\",\"SA\",\"TA\",\"U\",\"WA\",\"YA\"]\n",
    "        r, c = 10, 10\n",
    "        noise = np.random.normal(0, 1, (r * c, self.latent_dim))\n",
    "        gen_imgs = self.generator.predict(noise)\n",
    "\n",
    "        # Rescale images 0 - 1\n",
    "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "        for i in range(0,aksara_num):\n",
    "            img = gen_imgs[i,:,:,0]\n",
    "            normalizedImg = np.zeros((28,28))\n",
    "            normalizedImg = cv.normalize(img, normalizedImg, 0, 255, cv.NORM_MINMAX)\n",
    "            cv.imwrite(\"GAN_generated_images/%s_%s.png\" % (aksaraList[aksara],str(sumTrain+i+1)), normalizedImg)\n",
    "        for i in range(0,4):\n",
    "            img = gen_imgs[i,:,:,0]\n",
    "            normalizedImg = np.zeros((28,28))\n",
    "            normalizedImg = cv.normalize(img, normalizedImg, 0, 255, cv.NORM_MINMAX)\n",
    "            cv.imwrite(\"GAN_generated_images/%s_Cadangan_%s.png\" % (aksaraList[aksara],str(i+1)), normalizedImg)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "#     %%time\n",
    "    aksara=[26,27]\n",
    "    aksara_num=[22,24]\n",
    "    for i in range(0,2):\n",
    "        gan = GAN()\n",
    "        gan.train(aksara[i], aksara_num[i], epochs=30001, batch_size=48, sample_interval=1000)\n",
    "        print(\"-----------------------------------------------------------------\")\n",
    "#     a = [0,2,4,5,6,7,12,13,18,19,20,25,26,27]\n",
    "#     for i in a:\n",
    "#         gan.train(aksara=i, epochs=30001, batch_size=32, sample_interval=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\adjie\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\adjie\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\adjie\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\adjie\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\adjie\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\adjie\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\adjie\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 533,505\n",
      "Trainable params: 533,505\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 256)               25856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 784)               803600    \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 1,493,520\n",
      "Trainable params: 1,489,936\n",
      "Non-trainable params: 3,584\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From C:\\Users\\adjie\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adjie\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18-0 [D loss: 0.639359, acc.: 51.04%] [G loss: 0.500560]\n",
      "18-1000 [D loss: 0.636255, acc.: 55.21%] [G loss: 1.093661]\n",
      "18-2000 [D loss: 0.293562, acc.: 78.12%] [G loss: 4.345566]\n",
      "18-3000 [D loss: 0.296327, acc.: 85.42%] [G loss: 2.882236]\n",
      "18-4000 [D loss: 0.334307, acc.: 84.38%] [G loss: 2.349632]\n",
      "18-5000 [D loss: 0.295570, acc.: 85.42%] [G loss: 2.278278]\n",
      "18-6000 [D loss: 0.423810, acc.: 78.12%] [G loss: 2.168582]\n",
      "18-7000 [D loss: 0.324320, acc.: 83.33%] [G loss: 2.087028]\n",
      "18-8000 [D loss: 0.280282, acc.: 88.54%] [G loss: 2.602711]\n",
      "18-9000 [D loss: 0.372309, acc.: 83.33%] [G loss: 2.500665]\n",
      "18-10000 [D loss: 0.558994, acc.: 70.83%] [G loss: 2.409971]\n",
      "18-11000 [D loss: 0.378187, acc.: 81.25%] [G loss: 2.509325]\n",
      "18-12000 [D loss: 0.294023, acc.: 85.42%] [G loss: 2.347069]\n",
      "18-13000 [D loss: 0.297227, acc.: 83.33%] [G loss: 2.964416]\n",
      "18-14000 [D loss: 0.295527, acc.: 86.46%] [G loss: 2.514031]\n",
      "18-15000 [D loss: 0.317625, acc.: 85.42%] [G loss: 2.954273]\n",
      "18-16000 [D loss: 0.498343, acc.: 80.21%] [G loss: 2.611619]\n",
      "18-17000 [D loss: 0.369464, acc.: 84.38%] [G loss: 3.742787]\n",
      "18-18000 [D loss: 0.262603, acc.: 87.50%] [G loss: 2.902113]\n",
      "18-19000 [D loss: 0.266652, acc.: 87.50%] [G loss: 3.298340]\n",
      "18-20000 [D loss: 0.188675, acc.: 92.71%] [G loss: 3.171499]\n",
      "18-21000 [D loss: 0.195823, acc.: 93.75%] [G loss: 3.629100]\n",
      "18-22000 [D loss: 0.269034, acc.: 90.62%] [G loss: 2.972099]\n",
      "18-23000 [D loss: 0.201405, acc.: 91.67%] [G loss: 2.453277]\n",
      "18-24000 [D loss: 0.232887, acc.: 90.62%] [G loss: 3.670958]\n",
      "18-25000 [D loss: 0.598401, acc.: 68.75%] [G loss: 4.082939]\n",
      "18-26000 [D loss: 0.297375, acc.: 85.42%] [G loss: 3.478458]\n",
      "18-27000 [D loss: 0.275904, acc.: 88.54%] [G loss: 3.702632]\n",
      "18-28000 [D loss: 0.244341, acc.: 87.50%] [G loss: 3.169678]\n",
      "18-29000 [D loss: 0.185379, acc.: 93.75%] [G loss: 3.681056]\n",
      "18-30000 [D loss: 0.190132, acc.: 91.67%] [G loss: 3.380460]\n",
      "-----------------------------------------------------------------\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_2 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 533,505\n",
      "Trainable params: 533,505\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_11 (Dense)             (None, 256)               25856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)   (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 784)               803600    \n",
      "_________________________________________________________________\n",
      "reshape_2 (Reshape)          (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 1,493,520\n",
      "Trainable params: 1,489,936\n",
      "Non-trainable params: 3,584\n",
      "_________________________________________________________________\n",
      "25-0 [D loss: 0.727717, acc.: 40.62%] [G loss: 0.731468]\n",
      "25-1000 [D loss: 0.541144, acc.: 67.71%] [G loss: 3.228024]\n",
      "25-2000 [D loss: 0.243838, acc.: 90.62%] [G loss: 2.783282]\n",
      "25-3000 [D loss: 0.219696, acc.: 91.67%] [G loss: 3.452844]\n",
      "25-4000 [D loss: 0.216577, acc.: 91.67%] [G loss: 2.560809]\n",
      "25-5000 [D loss: 0.170053, acc.: 92.71%] [G loss: 3.506697]\n",
      "25-6000 [D loss: 0.268739, acc.: 90.62%] [G loss: 3.343962]\n",
      "25-7000 [D loss: 0.273943, acc.: 89.58%] [G loss: 2.766605]\n",
      "25-8000 [D loss: 0.167609, acc.: 92.71%] [G loss: 3.500865]\n",
      "25-9000 [D loss: 0.243211, acc.: 91.67%] [G loss: 2.959268]\n",
      "25-10000 [D loss: 0.134358, acc.: 95.83%] [G loss: 4.556616]\n",
      "25-11000 [D loss: 0.262092, acc.: 89.58%] [G loss: 2.990833]\n",
      "25-12000 [D loss: 0.191166, acc.: 93.75%] [G loss: 3.388435]\n",
      "25-13000 [D loss: 0.168191, acc.: 92.71%] [G loss: 3.396008]\n",
      "25-14000 [D loss: 0.097259, acc.: 96.88%] [G loss: 3.771910]\n",
      "25-15000 [D loss: 0.294126, acc.: 90.62%] [G loss: 3.290607]\n",
      "25-16000 [D loss: 0.061432, acc.: 96.88%] [G loss: 4.717910]\n",
      "25-17000 [D loss: 0.168524, acc.: 93.75%] [G loss: 3.767532]\n",
      "25-18000 [D loss: 0.128721, acc.: 96.88%] [G loss: 3.425722]\n",
      "25-19000 [D loss: 0.229354, acc.: 91.67%] [G loss: 3.819567]\n",
      "25-20000 [D loss: 0.322840, acc.: 86.46%] [G loss: 2.742655]\n",
      "25-21000 [D loss: 0.169270, acc.: 93.75%] [G loss: 3.996880]\n",
      "25-22000 [D loss: 0.192884, acc.: 93.75%] [G loss: 3.606396]\n",
      "25-23000 [D loss: 0.171011, acc.: 92.71%] [G loss: 3.429312]\n",
      "25-24000 [D loss: 0.187857, acc.: 90.62%] [G loss: 4.389406]\n",
      "25-25000 [D loss: 0.239665, acc.: 91.67%] [G loss: 2.882783]\n",
      "25-26000 [D loss: 0.090458, acc.: 96.88%] [G loss: 4.796629]\n",
      "25-27000 [D loss: 0.143969, acc.: 95.83%] [G loss: 7.204267]\n",
      "25-28000 [D loss: 0.149316, acc.: 93.75%] [G loss: 3.800558]\n",
      "25-29000 [D loss: 0.146387, acc.: 94.79%] [G loss: 5.695396]\n",
      "25-30000 [D loss: 0.229515, acc.: 91.67%] [G loss: 2.964620]\n",
      "-----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == '__main__':\n",
    "#     %%time\n",
    "    aksara=[18,25]\n",
    "    aksara_num=[22,24]\n",
    "    for i in range(0,2):\n",
    "        gan = GAN()\n",
    "        gan.train(aksara[i], aksara_num[i], epochs=30001, batch_size=48, sample_interval=1000)\n",
    "        print(\"-----------------------------------------------------------------\")\n",
    "#     a = [0,2,4,5,6,7,12,13,18,19,20,25,26,27]\n",
    "#     for i in a:\n",
    "#         gan.train(aksara=i, epochs=30001, batch_size=32, sample_interval=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\adjie\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\adjie\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\adjie\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\adjie\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\adjie\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\adjie\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\adjie\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 533,505\n",
      "Trainable params: 533,505\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 256)               25856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 784)               803600    \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 1,493,520\n",
      "Trainable params: 1,489,936\n",
      "Non-trainable params: 3,584\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From C:\\Users\\adjie\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adjie\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7-0 [D loss: 0.706975, acc.: 46.88%] [G loss: 1.057230]\n",
      "7-1000 [D loss: 0.528538, acc.: 64.58%] [G loss: 2.225678]\n",
      "7-2000 [D loss: 0.333550, acc.: 92.71%] [G loss: 3.261159]\n",
      "7-3000 [D loss: 0.145060, acc.: 96.88%] [G loss: 6.615382]\n",
      "7-4000 [D loss: 0.246441, acc.: 85.42%] [G loss: 3.124181]\n",
      "7-5000 [D loss: 0.205465, acc.: 94.79%] [G loss: 3.377589]\n",
      "7-6000 [D loss: 0.100408, acc.: 97.92%] [G loss: 4.695069]\n",
      "7-7000 [D loss: 0.310659, acc.: 90.62%] [G loss: 3.413656]\n",
      "7-8000 [D loss: 0.172079, acc.: 92.71%] [G loss: 3.605320]\n",
      "7-9000 [D loss: 0.317906, acc.: 89.58%] [G loss: 3.798021]\n",
      "7-10000 [D loss: 0.157956, acc.: 93.75%] [G loss: 5.252641]\n",
      "7-11000 [D loss: 0.272350, acc.: 90.62%] [G loss: 3.552936]\n",
      "7-12000 [D loss: 0.307636, acc.: 84.38%] [G loss: 3.139872]\n",
      "7-13000 [D loss: 0.220902, acc.: 92.71%] [G loss: 4.544974]\n",
      "7-14000 [D loss: 0.337134, acc.: 85.42%] [G loss: 2.883662]\n",
      "7-15000 [D loss: 0.265410, acc.: 91.67%] [G loss: 2.349604]\n",
      "7-16000 [D loss: 0.220954, acc.: 94.79%] [G loss: 3.160787]\n",
      "7-17000 [D loss: 0.286745, acc.: 90.62%] [G loss: 3.614515]\n",
      "7-18000 [D loss: 0.124029, acc.: 97.92%] [G loss: 4.685393]\n",
      "7-19000 [D loss: 0.185018, acc.: 93.75%] [G loss: 2.944445]\n",
      "7-20000 [D loss: 0.116266, acc.: 96.88%] [G loss: 3.801891]\n",
      "7-21000 [D loss: 0.199204, acc.: 93.75%] [G loss: 3.721190]\n",
      "7-22000 [D loss: 0.229379, acc.: 89.58%] [G loss: 4.183589]\n",
      "7-23000 [D loss: 0.215769, acc.: 92.71%] [G loss: 3.306223]\n",
      "7-24000 [D loss: 0.211689, acc.: 93.75%] [G loss: 3.853604]\n",
      "7-25000 [D loss: 0.182878, acc.: 92.71%] [G loss: 3.682680]\n",
      "7-26000 [D loss: 0.151952, acc.: 96.88%] [G loss: 3.953261]\n",
      "7-27000 [D loss: 0.183397, acc.: 93.75%] [G loss: 4.096759]\n",
      "7-28000 [D loss: 0.319466, acc.: 89.58%] [G loss: 3.679753]\n",
      "7-29000 [D loss: 0.211111, acc.: 92.71%] [G loss: 3.756815]\n",
      "7-30000 [D loss: 0.178623, acc.: 93.75%] [G loss: 2.523011]\n",
      "-----------------------------------------------------------------\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_2 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 533,505\n",
      "Trainable params: 533,505\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_11 (Dense)             (None, 256)               25856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)   (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 784)               803600    \n",
      "_________________________________________________________________\n",
      "reshape_2 (Reshape)          (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 1,493,520\n",
      "Trainable params: 1,489,936\n",
      "Non-trainable params: 3,584\n",
      "_________________________________________________________________\n",
      "12-0 [D loss: 0.625377, acc.: 50.00%] [G loss: 0.417286]\n",
      "12-1000 [D loss: 0.652649, acc.: 55.21%] [G loss: 2.163742]\n",
      "12-2000 [D loss: 0.173321, acc.: 100.00%] [G loss: 5.549768]\n",
      "12-3000 [D loss: 0.190157, acc.: 98.96%] [G loss: 4.198505]\n",
      "12-4000 [D loss: 0.231054, acc.: 92.71%] [G loss: 3.372258]\n",
      "12-5000 [D loss: 0.411414, acc.: 78.12%] [G loss: 3.771175]\n",
      "12-6000 [D loss: 0.203845, acc.: 91.67%] [G loss: 3.756977]\n",
      "12-7000 [D loss: 0.235337, acc.: 88.54%] [G loss: 4.193568]\n",
      "12-8000 [D loss: 0.205584, acc.: 93.75%] [G loss: 4.311748]\n",
      "12-9000 [D loss: 0.221262, acc.: 93.75%] [G loss: 3.281307]\n",
      "12-10000 [D loss: 0.314793, acc.: 87.50%] [G loss: 3.446492]\n",
      "12-11000 [D loss: 0.112814, acc.: 96.88%] [G loss: 3.720446]\n",
      "12-12000 [D loss: 0.168420, acc.: 94.79%] [G loss: 3.692300]\n",
      "12-13000 [D loss: 0.144045, acc.: 95.83%] [G loss: 3.927657]\n",
      "12-14000 [D loss: 0.250243, acc.: 89.58%] [G loss: 4.921643]\n",
      "12-15000 [D loss: 0.141040, acc.: 93.75%] [G loss: 3.930587]\n",
      "12-16000 [D loss: 0.242829, acc.: 90.62%] [G loss: 4.009651]\n",
      "12-17000 [D loss: 0.192812, acc.: 90.62%] [G loss: 4.177927]\n",
      "12-18000 [D loss: 0.216767, acc.: 91.67%] [G loss: 4.189549]\n",
      "12-19000 [D loss: 0.166892, acc.: 94.79%] [G loss: 3.825790]\n",
      "12-20000 [D loss: 0.093339, acc.: 97.92%] [G loss: 4.904594]\n",
      "12-21000 [D loss: 0.088608, acc.: 96.88%] [G loss: 4.231031]\n",
      "12-22000 [D loss: 0.082618, acc.: 96.88%] [G loss: 4.488005]\n",
      "12-23000 [D loss: 0.185350, acc.: 93.75%] [G loss: 4.400279]\n",
      "12-24000 [D loss: 0.148616, acc.: 95.83%] [G loss: 3.297498]\n",
      "12-25000 [D loss: 0.262549, acc.: 91.67%] [G loss: 3.503657]\n",
      "12-26000 [D loss: 0.157209, acc.: 95.83%] [G loss: 3.406594]\n",
      "12-27000 [D loss: 0.184001, acc.: 93.75%] [G loss: 3.995794]\n",
      "12-28000 [D loss: 0.113864, acc.: 95.83%] [G loss: 4.030309]\n",
      "12-29000 [D loss: 0.203094, acc.: 95.83%] [G loss: 3.809233]\n",
      "12-30000 [D loss: 0.209937, acc.: 93.75%] [G loss: 4.391947]\n",
      "-----------------------------------------------------------------\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_3 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_12 (LeakyReLU)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 533,505\n",
      "Trainable params: 533,505\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_18 (Dense)             (None, 256)               25856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_13 (LeakyReLU)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_14 (LeakyReLU)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_15 (LeakyReLU)   (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 784)               803600    \n",
      "_________________________________________________________________\n",
      "reshape_3 (Reshape)          (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 1,493,520\n",
      "Trainable params: 1,489,936\n",
      "Non-trainable params: 3,584\n",
      "_________________________________________________________________\n",
      "13-0 [D loss: 0.814317, acc.: 17.71%] [G loss: 0.624552]\n",
      "13-1000 [D loss: 0.428457, acc.: 78.12%] [G loss: 2.893559]\n",
      "13-2000 [D loss: 0.270173, acc.: 91.67%] [G loss: 2.925992]\n",
      "13-3000 [D loss: 0.463450, acc.: 83.33%] [G loss: 2.061092]\n",
      "13-4000 [D loss: 0.216987, acc.: 94.79%] [G loss: 2.913143]\n",
      "13-5000 [D loss: 0.334331, acc.: 80.21%] [G loss: 2.700923]\n",
      "13-6000 [D loss: 0.376315, acc.: 86.46%] [G loss: 1.997861]\n",
      "13-7000 [D loss: 0.173800, acc.: 96.88%] [G loss: 2.278853]\n",
      "13-8000 [D loss: 0.417286, acc.: 83.33%] [G loss: 1.953086]\n",
      "13-9000 [D loss: 0.302334, acc.: 85.42%] [G loss: 2.391074]\n",
      "13-10000 [D loss: 0.289660, acc.: 86.46%] [G loss: 2.485182]\n",
      "13-11000 [D loss: 0.171010, acc.: 93.75%] [G loss: 2.864830]\n",
      "13-12000 [D loss: 0.199244, acc.: 89.58%] [G loss: 2.929904]\n",
      "13-13000 [D loss: 0.302457, acc.: 89.58%] [G loss: 1.989580]\n",
      "13-14000 [D loss: 0.166155, acc.: 93.75%] [G loss: 3.575913]\n",
      "13-15000 [D loss: 0.314393, acc.: 88.54%] [G loss: 2.620723]\n",
      "13-16000 [D loss: 0.264886, acc.: 87.50%] [G loss: 2.615389]\n",
      "13-17000 [D loss: 0.245489, acc.: 86.46%] [G loss: 3.766193]\n",
      "13-18000 [D loss: 0.301552, acc.: 88.54%] [G loss: 3.185889]\n",
      "13-19000 [D loss: 0.190913, acc.: 92.71%] [G loss: 2.830721]\n",
      "13-20000 [D loss: 0.063767, acc.: 97.92%] [G loss: 3.084959]\n",
      "13-21000 [D loss: 0.176971, acc.: 93.75%] [G loss: 3.197001]\n",
      "13-22000 [D loss: 0.233491, acc.: 86.46%] [G loss: 3.809130]\n",
      "13-23000 [D loss: 0.241094, acc.: 92.71%] [G loss: 3.083904]\n",
      "13-24000 [D loss: 0.109104, acc.: 96.88%] [G loss: 3.786152]\n",
      "13-25000 [D loss: 0.378268, acc.: 86.46%] [G loss: 2.830794]\n",
      "13-26000 [D loss: 0.162706, acc.: 95.83%] [G loss: 3.275784]\n",
      "13-27000 [D loss: 0.191107, acc.: 92.71%] [G loss: 2.974924]\n",
      "13-28000 [D loss: 0.107740, acc.: 97.92%] [G loss: 3.868544]\n",
      "13-29000 [D loss: 0.207940, acc.: 91.67%] [G loss: 3.350112]\n",
      "13-30000 [D loss: 0.219896, acc.: 93.75%] [G loss: 3.591219]\n",
      "-----------------------------------------------------------------\n",
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_4 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_16 (LeakyReLU)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_17 (LeakyReLU)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 533,505\n",
      "Trainable params: 533,505\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_25 (Dense)             (None, 256)               25856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_18 (LeakyReLU)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_19 (LeakyReLU)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_20 (LeakyReLU)   (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 784)               803600    \n",
      "_________________________________________________________________\n",
      "reshape_4 (Reshape)          (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 1,493,520\n",
      "Trainable params: 1,489,936\n",
      "Non-trainable params: 3,584\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-ff86a55a6aed>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    181\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m         \u001b[0mgan\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGAN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 183\u001b[1;33m         \u001b[0mgan\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maksara\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maksara_num\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30001\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m48\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_interval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    184\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"-----------------------------------------------------------------\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    185\u001b[0m \u001b[1;31m#     a = [0,2,4,5,6,7,12,13,18,19,20,25,26,27]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == '__main__':\n",
    "#     %%time\n",
    "    aksara=[7,12,13]\n",
    "    aksara_num=[24,21,24]\n",
    "    for i in range(0,4):\n",
    "        gan = GAN()\n",
    "        gan.train(aksara[i], aksara_num[i], epochs=30001, batch_size=48, sample_interval=1000)\n",
    "        print(\"-----------------------------------------------------------------\")\n",
    "#     a = [0,2,4,5,6,7,12,13,18,19,20,25,26,27]\n",
    "#     for i in a:\n",
    "#         gan.train(aksara=i, epochs=30001, batch_size=32, sample_interval=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\adjie\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 533,505\n",
      "Trainable params: 533,505\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 256)               25856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 784)               803600    \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 1,493,520\n",
      "Trainable params: 1,489,936\n",
      "Non-trainable params: 3,584\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From C:\\Users\\adjie\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adjie\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2-0 [D loss: 0.709409, acc.: 56.25%] [G loss: 0.949301]\n",
      "2-500 [D loss: 0.215756, acc.: 95.83%] [G loss: 4.677984]\n",
      "2-1000 [D loss: 0.509335, acc.: 67.71%] [G loss: 2.536737]\n",
      "2-1500 [D loss: 0.377938, acc.: 88.54%] [G loss: 4.425233]\n",
      "2-2000 [D loss: 0.256330, acc.: 92.71%] [G loss: 3.002691]\n",
      "2-2500 [D loss: 0.342475, acc.: 91.67%] [G loss: 2.451769]\n",
      "2-3000 [D loss: 0.192805, acc.: 94.79%] [G loss: 3.565622]\n",
      "2-3500 [D loss: 0.483268, acc.: 80.21%] [G loss: 3.228659]\n",
      "2-4000 [D loss: 0.323413, acc.: 88.54%] [G loss: 2.721824]\n",
      "2-4500 [D loss: 0.099963, acc.: 98.96%] [G loss: 3.551860]\n",
      "2-5000 [D loss: 0.150127, acc.: 94.79%] [G loss: 2.973430]\n",
      "2-5500 [D loss: 0.280885, acc.: 88.54%] [G loss: 3.068622]\n",
      "2-6000 [D loss: 0.226558, acc.: 90.62%] [G loss: 2.634293]\n",
      "2-6500 [D loss: 0.246897, acc.: 88.54%] [G loss: 3.175595]\n",
      "2-7000 [D loss: 0.318792, acc.: 85.42%] [G loss: 2.419063]\n",
      "2-7500 [D loss: 0.300048, acc.: 88.54%] [G loss: 2.658532]\n",
      "2-8000 [D loss: 0.298076, acc.: 84.38%] [G loss: 3.140390]\n",
      "2-8500 [D loss: 0.217151, acc.: 90.62%] [G loss: 2.900365]\n",
      "2-9000 [D loss: 0.282392, acc.: 84.38%] [G loss: 3.164806]\n",
      "2-9500 [D loss: 0.210474, acc.: 93.75%] [G loss: 3.165008]\n",
      "2-10000 [D loss: 0.211643, acc.: 90.62%] [G loss: 3.165854]\n",
      "2-10500 [D loss: 0.196183, acc.: 90.62%] [G loss: 3.000533]\n",
      "2-11000 [D loss: 0.198357, acc.: 95.83%] [G loss: 3.055608]\n",
      "2-11500 [D loss: 0.276704, acc.: 86.46%] [G loss: 3.447360]\n",
      "2-12000 [D loss: 0.242921, acc.: 91.67%] [G loss: 4.356338]\n",
      "2-12500 [D loss: 0.192568, acc.: 93.75%] [G loss: 3.524703]\n",
      "2-13000 [D loss: 0.399438, acc.: 83.33%] [G loss: 4.514412]\n",
      "2-13500 [D loss: 0.324087, acc.: 86.46%] [G loss: 2.839113]\n",
      "2-14000 [D loss: 0.330330, acc.: 78.12%] [G loss: 4.314804]\n",
      "2-14500 [D loss: 0.157399, acc.: 90.62%] [G loss: 3.852750]\n",
      "2-15000 [D loss: 0.115863, acc.: 95.83%] [G loss: 3.750518]\n",
      "2-15500 [D loss: 0.161502, acc.: 91.67%] [G loss: 4.376306]\n",
      "2-16000 [D loss: 0.104312, acc.: 97.92%] [G loss: 3.572767]\n",
      "2-16500 [D loss: 0.288971, acc.: 87.50%] [G loss: 4.310664]\n",
      "2-17000 [D loss: 0.261631, acc.: 87.50%] [G loss: 3.265880]\n",
      "2-17500 [D loss: 0.247088, acc.: 87.50%] [G loss: 4.760852]\n",
      "2-18000 [D loss: 0.232527, acc.: 85.42%] [G loss: 4.156862]\n",
      "2-18500 [D loss: 0.332972, acc.: 76.04%] [G loss: 3.395306]\n",
      "2-19000 [D loss: 0.256689, acc.: 93.75%] [G loss: 4.164172]\n",
      "2-19500 [D loss: 0.097090, acc.: 97.92%] [G loss: 4.562622]\n",
      "2-20000 [D loss: 0.183595, acc.: 89.58%] [G loss: 4.794284]\n",
      "-----------------------------------------------------------------\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_2 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 533,505\n",
      "Trainable params: 533,505\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_11 (Dense)             (None, 256)               25856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)   (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 784)               803600    \n",
      "_________________________________________________________________\n",
      "reshape_2 (Reshape)          (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 1,493,520\n",
      "Trainable params: 1,489,936\n",
      "Non-trainable params: 3,584\n",
      "_________________________________________________________________\n",
      "4-0 [D loss: 0.877671, acc.: 5.21%] [G loss: 0.747325]\n",
      "4-500 [D loss: 0.254574, acc.: 87.50%] [G loss: 7.674582]\n",
      "4-1000 [D loss: 0.223897, acc.: 100.00%] [G loss: 4.285599]\n",
      "4-1500 [D loss: 0.484932, acc.: 76.04%] [G loss: 2.236989]\n",
      "4-2000 [D loss: 0.258796, acc.: 96.88%] [G loss: 3.452235]\n",
      "4-2500 [D loss: 0.286162, acc.: 89.58%] [G loss: 2.678693]\n",
      "4-3000 [D loss: 0.170081, acc.: 96.88%] [G loss: 3.627504]\n",
      "4-3500 [D loss: 0.374677, acc.: 90.62%] [G loss: 2.238644]\n",
      "4-4000 [D loss: 0.339351, acc.: 80.21%] [G loss: 3.641980]\n",
      "4-4500 [D loss: 0.309996, acc.: 87.50%] [G loss: 2.557737]\n",
      "4-5000 [D loss: 0.275251, acc.: 92.71%] [G loss: 3.418974]\n",
      "4-5500 [D loss: 0.435773, acc.: 78.12%] [G loss: 2.830631]\n",
      "4-6000 [D loss: 0.112395, acc.: 96.88%] [G loss: 4.350589]\n",
      "4-6500 [D loss: 0.141502, acc.: 95.83%] [G loss: 3.972187]\n",
      "4-7000 [D loss: 0.160317, acc.: 96.88%] [G loss: 5.271272]\n",
      "4-7500 [D loss: 0.308721, acc.: 90.62%] [G loss: 4.223567]\n",
      "4-8000 [D loss: 0.129801, acc.: 96.88%] [G loss: 3.999969]\n",
      "4-8500 [D loss: 0.130237, acc.: 94.79%] [G loss: 3.431036]\n",
      "4-9000 [D loss: 0.274175, acc.: 86.46%] [G loss: 3.499289]\n",
      "4-9500 [D loss: 0.106248, acc.: 97.92%] [G loss: 3.835930]\n",
      "4-10000 [D loss: 0.183746, acc.: 94.79%] [G loss: 3.519622]\n",
      "4-10500 [D loss: 0.060360, acc.: 98.96%] [G loss: 4.593396]\n",
      "4-11000 [D loss: 0.195877, acc.: 92.71%] [G loss: 3.193195]\n",
      "4-11500 [D loss: 0.112677, acc.: 95.83%] [G loss: 5.391258]\n",
      "4-12000 [D loss: 0.202946, acc.: 91.67%] [G loss: 3.693645]\n",
      "4-12500 [D loss: 0.291704, acc.: 92.71%] [G loss: 3.528963]\n",
      "4-13000 [D loss: 0.188187, acc.: 93.75%] [G loss: 3.646805]\n",
      "4-13500 [D loss: 0.176344, acc.: 93.75%] [G loss: 3.572626]\n",
      "4-14000 [D loss: 0.125295, acc.: 95.83%] [G loss: 3.328440]\n",
      "4-14500 [D loss: 0.272894, acc.: 86.46%] [G loss: 3.361851]\n",
      "4-15000 [D loss: 0.299009, acc.: 89.58%] [G loss: 2.935214]\n",
      "4-15500 [D loss: 0.131858, acc.: 96.88%] [G loss: 4.257495]\n",
      "4-16000 [D loss: 0.222513, acc.: 92.71%] [G loss: 4.492248]\n",
      "4-16500 [D loss: 0.199464, acc.: 93.75%] [G loss: 3.866958]\n",
      "4-17000 [D loss: 0.229306, acc.: 91.67%] [G loss: 3.468334]\n",
      "4-17500 [D loss: 0.224934, acc.: 91.67%] [G loss: 3.357421]\n",
      "4-18000 [D loss: 0.115985, acc.: 96.88%] [G loss: 5.582680]\n",
      "4-18500 [D loss: 0.224883, acc.: 89.58%] [G loss: 4.673522]\n",
      "4-19000 [D loss: 0.256070, acc.: 87.50%] [G loss: 4.281636]\n",
      "4-19500 [D loss: 0.101788, acc.: 97.92%] [G loss: 4.242873]\n",
      "4-20000 [D loss: 0.158338, acc.: 95.83%] [G loss: 4.100681]\n",
      "-----------------------------------------------------------------\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_3 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_12 (LeakyReLU)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 533,505\n",
      "Trainable params: 533,505\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_18 (Dense)             (None, 256)               25856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_13 (LeakyReLU)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_14 (LeakyReLU)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_15 (LeakyReLU)   (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 784)               803600    \n",
      "_________________________________________________________________\n",
      "reshape_3 (Reshape)          (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 1,493,520\n",
      "Trainable params: 1,489,936\n",
      "Non-trainable params: 3,584\n",
      "_________________________________________________________________\n",
      "5-0 [D loss: 0.631504, acc.: 64.58%] [G loss: 0.519460]\n",
      "5-500 [D loss: 0.244970, acc.: 90.62%] [G loss: 9.110124]\n",
      "5-1000 [D loss: 0.573987, acc.: 59.38%] [G loss: 3.705923]\n",
      "5-1500 [D loss: 0.561963, acc.: 63.54%] [G loss: 1.695387]\n",
      "5-2000 [D loss: 0.482974, acc.: 76.04%] [G loss: 1.688059]\n",
      "5-2500 [D loss: 0.578895, acc.: 73.96%] [G loss: 2.515985]\n",
      "5-3000 [D loss: 0.363445, acc.: 84.38%] [G loss: 3.652734]\n",
      "5-3500 [D loss: 0.190707, acc.: 93.75%] [G loss: 3.352417]\n",
      "5-4000 [D loss: 0.315894, acc.: 90.62%] [G loss: 3.211989]\n",
      "5-4500 [D loss: 0.258847, acc.: 90.62%] [G loss: 4.144951]\n",
      "5-5000 [D loss: 0.170584, acc.: 94.79%] [G loss: 5.605952]\n",
      "5-5500 [D loss: 0.166461, acc.: 97.92%] [G loss: 4.596508]\n",
      "5-6000 [D loss: 0.168388, acc.: 94.79%] [G loss: 4.473694]\n",
      "5-6500 [D loss: 0.200502, acc.: 93.75%] [G loss: 4.385603]\n",
      "5-7000 [D loss: 0.974850, acc.: 77.08%] [G loss: 3.521154]\n",
      "5-7500 [D loss: 0.234269, acc.: 89.58%] [G loss: 3.426919]\n",
      "5-8000 [D loss: 0.140849, acc.: 96.88%] [G loss: 4.939515]\n",
      "5-8500 [D loss: 0.227925, acc.: 89.58%] [G loss: 3.867920]\n",
      "5-9000 [D loss: 0.140203, acc.: 96.88%] [G loss: 3.725284]\n",
      "5-9500 [D loss: 0.245653, acc.: 92.71%] [G loss: 3.986286]\n",
      "5-10000 [D loss: 0.191869, acc.: 94.79%] [G loss: 4.155670]\n",
      "5-10500 [D loss: 0.122904, acc.: 93.75%] [G loss: 5.631522]\n",
      "5-11000 [D loss: 0.228343, acc.: 89.58%] [G loss: 4.807909]\n",
      "5-11500 [D loss: 0.106361, acc.: 95.83%] [G loss: 3.998555]\n",
      "5-12000 [D loss: 0.119053, acc.: 96.88%] [G loss: 4.041099]\n",
      "5-12500 [D loss: 0.063805, acc.: 98.96%] [G loss: 3.653746]\n",
      "5-13000 [D loss: 0.171792, acc.: 91.67%] [G loss: 4.419324]\n",
      "5-13500 [D loss: 0.102042, acc.: 96.88%] [G loss: 4.392728]\n",
      "5-14000 [D loss: 0.247172, acc.: 91.67%] [G loss: 3.058980]\n",
      "5-14500 [D loss: 0.133318, acc.: 94.79%] [G loss: 4.227814]\n",
      "5-15000 [D loss: 0.240392, acc.: 92.71%] [G loss: 3.344191]\n",
      "5-15500 [D loss: 0.206741, acc.: 93.75%] [G loss: 3.321685]\n",
      "5-16000 [D loss: 0.074257, acc.: 95.83%] [G loss: 4.587860]\n",
      "5-16500 [D loss: 0.145864, acc.: 94.79%] [G loss: 4.397085]\n",
      "5-17000 [D loss: 0.229368, acc.: 90.62%] [G loss: 2.711945]\n",
      "5-17500 [D loss: 0.106933, acc.: 98.96%] [G loss: 4.163724]\n",
      "5-18000 [D loss: 0.211254, acc.: 92.71%] [G loss: 2.750491]\n",
      "5-18500 [D loss: 0.110372, acc.: 96.88%] [G loss: 4.190903]\n",
      "5-19000 [D loss: 0.117334, acc.: 95.83%] [G loss: 3.604196]\n",
      "5-19500 [D loss: 0.118139, acc.: 96.88%] [G loss: 4.392365]\n",
      "5-20000 [D loss: 0.212010, acc.: 89.58%] [G loss: 4.289959]\n",
      "-----------------------------------------------------------------\n",
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_4 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_16 (LeakyReLU)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_17 (LeakyReLU)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 533,505\n",
      "Trainable params: 533,505\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_25 (Dense)             (None, 256)               25856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_18 (LeakyReLU)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_19 (LeakyReLU)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_20 (LeakyReLU)   (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 784)               803600    \n",
      "_________________________________________________________________\n",
      "reshape_4 (Reshape)          (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 1,493,520\n",
      "Trainable params: 1,489,936\n",
      "Non-trainable params: 3,584\n",
      "_________________________________________________________________\n",
      "6-0 [D loss: 0.700018, acc.: 58.33%] [G loss: 0.596262]\n",
      "6-500 [D loss: 0.253951, acc.: 90.62%] [G loss: 5.712351]\n",
      "6-1000 [D loss: 0.373907, acc.: 87.50%] [G loss: 3.182943]\n",
      "6-1500 [D loss: 0.244964, acc.: 94.79%] [G loss: 3.671803]\n",
      "6-2000 [D loss: 0.436090, acc.: 69.79%] [G loss: 3.404960]\n",
      "6-2500 [D loss: 0.137717, acc.: 98.96%] [G loss: 3.931037]\n",
      "6-3000 [D loss: 0.315757, acc.: 90.62%] [G loss: 3.727412]\n",
      "6-3500 [D loss: 0.193808, acc.: 94.79%] [G loss: 3.501079]\n",
      "6-4000 [D loss: 0.261317, acc.: 90.62%] [G loss: 3.221184]\n",
      "6-4500 [D loss: 0.354968, acc.: 85.42%] [G loss: 2.334225]\n",
      "6-5000 [D loss: 0.355699, acc.: 85.42%] [G loss: 2.684989]\n",
      "6-5500 [D loss: 0.304438, acc.: 88.54%] [G loss: 2.747623]\n",
      "6-6000 [D loss: 0.253973, acc.: 87.50%] [G loss: 2.940639]\n",
      "6-6500 [D loss: 0.204728, acc.: 94.79%] [G loss: 3.381097]\n",
      "6-7000 [D loss: 0.192470, acc.: 93.75%] [G loss: 3.241573]\n",
      "6-7500 [D loss: 0.157772, acc.: 93.75%] [G loss: 3.757294]\n",
      "6-8000 [D loss: 0.248251, acc.: 87.50%] [G loss: 3.731177]\n",
      "6-8500 [D loss: 0.355197, acc.: 81.25%] [G loss: 3.242331]\n",
      "6-9000 [D loss: 0.249226, acc.: 90.62%] [G loss: 2.958632]\n",
      "6-9500 [D loss: 0.269734, acc.: 88.54%] [G loss: 2.511791]\n",
      "6-10000 [D loss: 0.157971, acc.: 94.79%] [G loss: 3.768198]\n",
      "6-10500 [D loss: 0.164048, acc.: 94.79%] [G loss: 4.331674]\n",
      "6-11000 [D loss: 0.217212, acc.: 93.75%] [G loss: 3.138102]\n",
      "6-11500 [D loss: 0.305055, acc.: 87.50%] [G loss: 3.733655]\n",
      "6-12000 [D loss: 0.184503, acc.: 93.75%] [G loss: 4.612410]\n",
      "6-12500 [D loss: 0.266627, acc.: 90.62%] [G loss: 2.812876]\n",
      "6-13000 [D loss: 0.150975, acc.: 96.88%] [G loss: 3.832851]\n",
      "6-13500 [D loss: 0.190798, acc.: 94.79%] [G loss: 4.143119]\n",
      "6-14000 [D loss: 0.168982, acc.: 91.67%] [G loss: 3.741477]\n",
      "6-14500 [D loss: 0.189378, acc.: 93.75%] [G loss: 3.246381]\n",
      "6-15000 [D loss: 0.277400, acc.: 84.38%] [G loss: 3.069271]\n",
      "6-15500 [D loss: 0.191557, acc.: 93.75%] [G loss: 3.039819]\n",
      "6-16000 [D loss: 0.186365, acc.: 90.62%] [G loss: 3.293277]\n",
      "6-16500 [D loss: 0.483894, acc.: 79.17%] [G loss: 3.389085]\n",
      "6-17000 [D loss: 0.136911, acc.: 95.83%] [G loss: 3.845231]\n",
      "6-17500 [D loss: 0.316094, acc.: 87.50%] [G loss: 2.534030]\n",
      "6-18000 [D loss: 0.321235, acc.: 87.50%] [G loss: 3.050556]\n",
      "6-18500 [D loss: 0.259496, acc.: 87.50%] [G loss: 2.611927]\n",
      "6-19000 [D loss: 0.121334, acc.: 94.79%] [G loss: 4.021949]\n",
      "6-19500 [D loss: 0.212871, acc.: 93.75%] [G loss: 3.941919]\n",
      "6-20000 [D loss: 0.297012, acc.: 88.54%] [G loss: 3.293138]\n",
      "-----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "#     %%time\n",
    "    aksara=[2,4,5,6]\n",
    "    aksara_num=[24,8,19,24]\n",
    "    for i in range(0,4):\n",
    "        gan = GAN()\n",
    "        gan.train(aksara[i], aksara_num[i], epochs=30001, batch_size=48, sample_interval=1000)\n",
    "        print(\"-----------------------------------------------------------------\")\n",
    "#     a = [0,2,4,5,6,7,12,13,18,19,20,25,26,27]\n",
    "#     for i in a:\n",
    "#         gan.train(aksara=i, epochs=30001, batch_size=32, sample_interval=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "aksara=2\n",
    "aksara_num=27\n",
    "gan = GAN()\n",
    "gan.train(aksara, aksara_num, epochs=20001, batch_size=48, sample_interval=1000)\n",
    "print(\"-----------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
