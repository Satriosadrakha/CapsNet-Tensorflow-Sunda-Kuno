{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from PIL import Image\n",
    "# from numpy import asarray\n",
    "# import numpy as np\n",
    "\n",
    "# # load the image\n",
    "# image = Image.open('HA_3.png')\n",
    "# # convert image to numpy array\n",
    "# data = asarray(image)\n",
    "# print(type(data))\n",
    "# # summarize shape\n",
    "# print(data.shape)\n",
    "\n",
    "# # create Pillow image\n",
    "# image2 = Image.fromarray(data)\n",
    "# print(type(image2))\n",
    "\n",
    "# # summarize image details\n",
    "# print(image2.mode)\n",
    "# print(image2.size)\n",
    "\n",
    "# im = np.array(Image.open('HA_3.png').convert('L').resize((30,30))) #you can pass multiple arguments in single line\n",
    "# print(type(im))\n",
    "\n",
    "# Image.fromarray(im).save('HA_3_transformed.png')\n",
    "\n",
    "# print(im)\n",
    "\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "from PIL import Image, ImageFilter, ImageEnhance\n",
    "import PIL.ImageOps\n",
    "import os\n",
    "import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def resize_only(file_name):\n",
    "\timg = Image.open(file_name).convert('LA')\n",
    "\t# img = img.filter(ImageFilter.SMOOTH)\n",
    "\t# img = img.filter(ImageFilter.SHARPEN)\n",
    "\timg = img.resize((28, 28))\n",
    "\t# img = PIL.ImageOps.invert(img)\n",
    "\timg.save(\"temp.png\")\n",
    "\timg = cv.imread('temp.png', 0)\n",
    "\timg = cv.bitwise_not(img)\n",
    "\timg =  Image.fromarray(img)\n",
    "\t\n",
    "\treturn img\n",
    "\n",
    "def resize(file_name, invert):\n",
    "\t# image = cv.imread('%s' % file_name,0)\n",
    "\timg = Image.open(file_name).convert('LA')\n",
    "# \tif invert==True:\n",
    "# \t\timg = PIL.ImageOps.invert(img)\n",
    "\tenhancer = ImageEnhance.Contrast(img)\n",
    "\t\n",
    "\tfactor = 2.0\n",
    "\timg = enhancer.enhance(factor)\n",
    "\timg = img.filter(ImageFilter.SMOOTH)\n",
    "\timg = img.filter(ImageFilter.SHARPEN)\n",
    "\timg = img.resize((28, 28))\n",
    "\t# print(type(img))    \n",
    "    \n",
    "\timg.save(\"temp.png\")\n",
    "    \n",
    "\timg = cv.imread('temp.png', 0)\n",
    "    \n",
    "\tif invert==True:\n",
    "\t\timg = cv.bitwise_not(img)\n",
    "\tret,th = cv.threshold(img,0,255,cv.THRESH_BINARY+cv.THRESH_OTSU)\n",
    "    \n",
    "\tblur = cv.GaussianBlur(img,(5,5),0)\n",
    "\tret2,th2 = cv.threshold(blur,0,255,cv.THRESH_BINARY+cv.THRESH_OTSU)\n",
    "    \n",
    "\tmask = np.where(th ==  0, th2, 255)\n",
    "    \n",
    "\timg = np.where(mask == 0, img, 255)\n",
    "\n",
    "#     create a CLAHE object (Arguments are optional)\n",
    "\tclahe = cv.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "\timg = clahe.apply(img)\n",
    "\n",
    "\t_,img_remove = cv.threshold(img,95,255,cv.THRESH_TOZERO)\n",
    "\timg_remove = cv.bitwise_not(img_remove)\n",
    "\t_,img = cv.threshold(img_remove,95,255,cv.THRESH_TOZERO)\n",
    "\n",
    "# \timg = cv.bitwise_not(img)\n",
    "    \n",
    "\timg =  Image.fromarray(img)\n",
    "  \n",
    "\treturn img\n",
    "\t# img.save(\"resized_\"+file_name)\n",
    "\n",
    "def preprocess():\n",
    "\t# crop(str(sys.argv))\n",
    "\tscript_dir = os.path.abspath('')\n",
    "\ti=[\"A\",\"BA\",\"CA\",\"DA\",\"GA\",\"HA\",\"I\",\"JA\",\"KA\",\"LA\",\"MA\",\"NA\",\"NGA\",\"NYA\",\"PA\",\"PANELENG\",\"PANEULEUNG\",\"PANGHULU\",\"PANGLAYAR\",\"PANOLONG\",\"PANYUKU\",\"PATEN\",\"RA\",\"SA\",\"TA\",\"U\",\"WA\",\"YA\"]\n",
    "\tj=[30,47,19,67,37,27,16,21,60,60,56,120,25,14,61,42,63,60,23,35,36,84,56,90,78,18,24,22]\n",
    "\tk=[12,30,7,45,16,11,7,9,40,25,24,80,10,6,39,28,42,40,10,24,24,36,36,60,52,7,10,9]\n",
    "\tganed = [0,2,4,5,6,7,12,13,18,19,20,25,26,27]\n",
    "\n",
    "\tfinal_np = np.array([])\n",
    "\tfinal_label = np.array([])\n",
    "\n",
    "    #   train_image to ready_to_train\n",
    "\tfor x in range(0, len(i), 1):\n",
    "\t\tsumImg = 0\n",
    "\t\ttrain_directory_path = os.path.join(script_dir,\"train-test_image\",\"dataset2\")\n",
    "\t\ttrain_count_image = len(glob.glob1(train_directory_path,\"%s_*.png\" % (i[x])))\n",
    "\n",
    "\t\tprint(\"Aksara = \" + str(i[x]))\n",
    "\t\tprint(\"Count_image training = \" + str(train_count_image))\n",
    "\n",
    "\t\tfor y in range(1, train_count_image+1):\n",
    "\t\t\tabs_file_path = os.path.join(directory_path, \"%s_%s.png\" % (i[aksara],str(y)))\n",
    "\t\t\timg = Image.open(abs_file_path)\n",
    "\t\t\timg = np.array(img)\n",
    "\t\t\timg = img[:, :]\n",
    "\t\t\tfinal_np = np.append(final_np,img)\n",
    "\t\t\tfinal_label = np.append(final_label,aksara)\n",
    "\n",
    "# \t\t\trel_path = \"train_image/%s_%s.png\" % (i[x],str(y))\n",
    "# \t\t\tabs_file_path = os.path.join(script_dir, rel_path)\n",
    "            \n",
    "# \t\t\timg = resize(abs_file_path, invert=False)\n",
    "# \t\t\timg.save(\"train-test_image/%s_%s.png\" % (i[x],str(y)))\n",
    "# \t\t\t# img = resize(abs_file_path, invert=False)\n",
    "# \t\t\t# img.save(\"ready_to_train_3/%s_%s.png\" % (i[x],str(y)))\n",
    "\t\tsumImg = sumImg + train_count_image\n",
    "\t\tnp.random.seed(42)\n",
    "\n",
    "\t\ttest_directory_path = os.path.join(script_dir,\"test_image/%s\" % i[x])\n",
    "\t\ttest_count_image = len(glob.glob1(test_directory_path,\"%s_*.png\" % (i[x])))\n",
    "\n",
    "\t\tfor y in range(1, test_count_image+1):\n",
    "\t\t\trel_path = os.path.join(test_directory_path,\"%s_%s.png\" % (i[x],str(y)))\n",
    "\t\t\tabs_file_path = os.path.join(script_dir, rel_path)\n",
    "            \n",
    "\t\t\timg = resize(abs_file_path, invert=False)\n",
    "\t\t\timg.save(\"train-test_image/%s_%s.png\" % (i[x],str(y+sumImg)))\n",
    "\t\tsumImg = sumImg + test_count_image\n",
    "\n",
    "# \t\tif x == 0:\n",
    "# \t\t\tfor y in range(1, sumImg+1):\n",
    "# \t\t\t\trel_path = os.path.join(\"train-test_image\",\"%s_%s.png\" % (i[x],str(y)))\n",
    "# \t\t\t\tabs_file_path = os.path.join(script_dir, rel_path)\n",
    "\n",
    "# \t\t\t\timg = Image.open(abs_file_path)\n",
    "# \t\t\t\timg = np.array(img)\n",
    "# \t\t\t\timg = img[:, :]\n",
    "# \t\t\t\tfinal_np = np.append(final_np,img)\n",
    "# \t\t\t\tfinal_label = np.append(final_label,x)\n",
    "# \t\t\tfinal_np = final_np.reshape((sumImg, 28, 28, 1)).astype(np.float32)\n",
    "# \t\t\tfinal_label = final_label.reshape((sumImg)).astype(np.int32)\n",
    "# \t\t\tdata_train, data_test, labels_train, labels_test = train_test_split(final_np, final_label, test_size=0.30, random_state=42)\n",
    "# \t\t\tprint(final_label)\n",
    "# \t\t\tprint(data_train.shape)\n",
    "# \t\t\tprint(data_test.shape)\n",
    "# \t\t\tprint(labels_train)\n",
    "# \t\t\tprint(labels_test)\n",
    "            \n",
    "# #   GAN_image to ready_to_train\n",
    "# \tfor x in ganed:\n",
    "# \t\tfor y in range(1, 26):\n",
    "# \t\t\trel_path = \"GAN_generated_images/%s_%s.png\" % (i[x],str(y+j[x]))\n",
    "# # \t\t\trel_path = \"ready_to_train/%s_%s.png\" % (i[x],str(y))\n",
    "# \t\t\tabs_file_path = os.path.join(script_dir, rel_path)\n",
    "            \n",
    "# \t\t\timg = resize(abs_file_path, invert=True)\n",
    "# \t\t\timg.save(\"ready_to_train_3/%s_%s.png\" % (i[x],str(y+j[x])))\n",
    "\n",
    "# #   test_image to ready_to_test (validation)\n",
    "# \tsumImg = 0\n",
    "# \tfinal_np = np.array([])\n",
    "# \tfinal_label = np.array([])\n",
    "# \tfor x in range(0, len(i), 1):\n",
    "# \t\tsumImg = sumImg + k[x]\n",
    "# \t\tfor y in range(1, int((k[x])/2)+1):\n",
    "# \t\t\trel_path = \"test_image/%s/%s_%s.png\" % (i[x],i[x],str(y))\n",
    "# \t\t\t# rel_path = \"ready_to_train/%s_%s.png\" % (i[x],str(y))\n",
    "# \t\t\tabs_file_path = os.path.join(script_dir, rel_path)\n",
    "\n",
    "# \t\t\t# img = resize(abs_file_path, invert=False)\n",
    "# \t\t\t# img.save(\"ready_to_test_2/%s_%s.png\" % (i[x],str(y)))\n",
    "# \t\t\timg = resize_only(abs_file_path)\n",
    "# \t\t\timg.save(\"ready_to_test/%s_%s.png\" % (i[x],str(y)))\n",
    "\n",
    "# # \ttest_image to ready_to_test (test)\n",
    "# \tsumImg = 0\n",
    "# \tfor x in range(0, len(i), 1):\n",
    "# \t\tsumImg = sumImg - (-k[x]//2)\n",
    "# \t\tfor y in range((-(-k[x])//2)+1, k[x]+1):\n",
    "# #             rel_path = \"test_image/%s/%s_%s.png\" % (i[x],i[x],str(y))\n",
    "# \t\t\trel_path = \"test_image/%s/%s_%s.png\" % (i[x],i[x],str(y))\n",
    "# \t\t\t# rel_path = \"ready_to_train/%s_%s.png\" % (i[x],str(y))\n",
    "# \t\t\tabs_file_path = os.path.join(script_dir, rel_path)\n",
    "            \n",
    "# \t\t\timg = Image.open(abs_file_path)\n",
    "# \t\t\timg = np.array(img)\n",
    "# \t\t\timg = img[:, :, 0]\n",
    "# \t\t\timg = resize_only(abs_file_path)\n",
    "# \t\t\timg.save(\"ready_to_test/%s_%s.png\" % (i[x],str(y)))\n",
    "# \t\t\t# img = resize(abs_file_path, invert=False)\n",
    "# \t\t\t# img.save(\"ready_to_test_2/%s_%s.png\" % (i[x],str(y)))\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "# \tmain()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sunda(aksara):\n",
    "    script_dir = os.path.abspath('')\n",
    "    i=[\"A\",\"BA\",\"CA\",\"DA\",\"GA\",\"HA\",\"I\",\"JA\",\"KA\",\"LA\",\"MA\",\"NA\",\"NGA\",\"NYA\",\"PA\",\"PANELENG\",\"PANEULEUNG\",\"PANGHULU\",\"PANGLAYAR\",\"PANOLONG\",\"PANYUKU\",\"PATEN\",\"RA\",\"SA\",\"TA\",\"U\",\"WA\",\"YA\"]\n",
    "    j=[30,47,19,67,37,27,16,21,60,60,56,120,25,14,61,42,63,60,23,35,36,84,56,90,78,18,24,22]\n",
    "    k=[12,30,7,45,16,11,7,9,40,25,24,80,10,6,39,28,42,40,10,24,24,36,36,60,52,7,10,9]\n",
    "    ganed = [0,2,4,5,6,7,12,13,18,19,20,25,26,27]\n",
    "\n",
    "    final_np = np.array([])\n",
    "    final_label = np.array([])\n",
    "\n",
    "    #   train_image to ready_to_train\n",
    "#for x in range(0, len(i), 1):\n",
    "    sumImg = 0\n",
    "    directory_path = os.path.join(script_dir,\"train-test_image\",\"dataset2\")\n",
    "    count_image = len(glob.glob1(directory_path,\"%s_*.png\" % (i[aksara])))\n",
    "\n",
    "    for y in range(1, count_image+1):\n",
    "        abs_file_path = os.path.join(directory_path, \"%s_%s.png\" % (i[aksara],str(y)))\n",
    "        img = Image.open(abs_file_path)\n",
    "        img = np.array(img)\n",
    "        img = img[:, :]\n",
    "        final_np = np.append(final_np,img)\n",
    "        final_label = np.append(final_label,aksara)\n",
    "    sumImg = sumImg + count_image\n",
    "    \n",
    "    directory_path = os.path.join(script_dir,\"train-test_image\",\"koropak_28\")\n",
    "    count_image = len(glob.glob1(directory_path,\"%s_*.png\" % (i[aksara])))\n",
    "    print(\"Count_image koropak = \" + str(count_image))\n",
    "\n",
    "    for y in range(0, count_image):\n",
    "        abs_file_path = os.path.join(directory_path, \"%s_%s.png\" % (i[aksara],str(y)))\n",
    "        img = Image.open(abs_file_path)\n",
    "        img = np.array(img)\n",
    "        img = img[:, :]\n",
    "        final_np = np.append(final_np,img)\n",
    "        final_label = np.append(final_label,aksara)\n",
    "    sumImg = sumImg + count_image\n",
    "\n",
    "    directory_path = os.path.join(script_dir,\"train-test_image\",\"kawih_p\")\n",
    "    count_image = len(glob.glob1(directory_path,\"%s_*.png\" % (i[aksara])))\n",
    "    print(\"Count_image kawih = \" + str(count_image))\n",
    "\n",
    "    for y in range(0, count_image):\n",
    "        abs_file_path = os.path.join(directory_path, \"%s_%s.png\" % (i[aksara],str(y)))\n",
    "        img = Image.open(abs_file_path)\n",
    "        img = np.array(img)\n",
    "        img = img[:, :]\n",
    "        final_np = np.append(final_np,img)\n",
    "        final_label = np.append(final_label,aksara)\n",
    "    sumImg = sumImg + count_image\n",
    "\n",
    "    print(\"SumImg = \" + str(sumImg))\n",
    "\n",
    "    final_np = final_np.reshape((sumImg, 28, 28, 1)).astype(np.float32)\n",
    "    final_label = final_label.reshape((sumImg)).astype(np.int32)\n",
    "\n",
    "#     final_np = final_np.reshape((sumImg, 28, 28, 1)).astype(np.float32)\n",
    "#     final_label = final_label.reshape((sumImg)).astype(np.int32)\n",
    "    data_train, data_test, labels_train, labels_test = train_test_split(final_np, final_label, test_size=0.30, random_state=42)\n",
    "    return(data_train,sumImg)\n",
    "\n",
    "\n",
    "#     print(final_label)\n",
    "#     print(data_train.shape)\n",
    "#     print(data_test.shape)\n",
    "#     print(labels_train)\n",
    "#     print(labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\Iyo\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\Iyo\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\Iyo\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\Iyo\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\Iyo\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\Iyo\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Iyo\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 533,505\n",
      "Trainable params: 533,505\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 256)               25856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 784)               803600    \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 1,493,520\n",
      "Trainable params: 1,489,936\n",
      "Non-trainable params: 3,584\n",
      "_________________________________________________________________\n",
      "Count_image koropak = 4\n",
      "Count_image kawih = 10\n",
      "SumImg = 34\n",
      "WARNING:tensorflow:From C:\\Users\\Iyo\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Iyo\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13-0 [D loss: 0.938999, acc.: 27.08%] [G loss: 0.683352]\n",
      "13-1000 [D loss: 0.266018, acc.: 91.67%] [G loss: 2.932680]\n",
      "13-2000 [D loss: 0.263276, acc.: 89.58%] [G loss: 3.256689]\n",
      "13-3000 [D loss: 0.189221, acc.: 91.67%] [G loss: 2.358646]\n",
      "13-4000 [D loss: 0.233752, acc.: 91.67%] [G loss: 2.876971]\n",
      "13-5000 [D loss: 0.188877, acc.: 93.75%] [G loss: 2.793161]\n",
      "13-6000 [D loss: 0.208905, acc.: 91.67%] [G loss: 2.608845]\n",
      "13-7000 [D loss: 0.485863, acc.: 79.17%] [G loss: 3.060521]\n",
      "13-8000 [D loss: 0.270975, acc.: 87.50%] [G loss: 2.298813]\n",
      "13-9000 [D loss: 0.233233, acc.: 91.67%] [G loss: 2.227478]\n",
      "13-10000 [D loss: 0.198722, acc.: 93.75%] [G loss: 2.763816]\n",
      "-----------------------------------------------------------------\n",
      "--- 736.6054253578186 seconds ---\n",
      "-----------------------------------------------------------------\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_2 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 533,505\n",
      "Trainable params: 533,505\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_11 (Dense)             (None, 256)               25856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)   (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 784)               803600    \n",
      "_________________________________________________________________\n",
      "reshape_2 (Reshape)          (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 1,493,520\n",
      "Trainable params: 1,489,936\n",
      "Non-trainable params: 3,584\n",
      "_________________________________________________________________\n",
      "Count_image koropak = 10\n",
      "Count_image kawih = 15\n",
      "SumImg = 58\n",
      "18-0 [D loss: 0.683651, acc.: 33.33%] [G loss: 0.400223]\n",
      "18-1000 [D loss: 0.147106, acc.: 95.83%] [G loss: 3.505952]\n",
      "18-2000 [D loss: 0.212081, acc.: 93.75%] [G loss: 3.644306]\n",
      "18-3000 [D loss: 0.263396, acc.: 89.58%] [G loss: 2.925836]\n",
      "18-4000 [D loss: 0.155591, acc.: 97.92%] [G loss: 3.348889]\n",
      "18-5000 [D loss: 0.078198, acc.: 100.00%] [G loss: 3.838778]\n",
      "18-6000 [D loss: 0.124993, acc.: 97.92%] [G loss: 4.183437]\n",
      "18-7000 [D loss: 0.131618, acc.: 95.83%] [G loss: 4.263930]\n",
      "18-8000 [D loss: 0.184853, acc.: 91.67%] [G loss: 4.678257]\n",
      "18-9000 [D loss: 0.175209, acc.: 93.75%] [G loss: 5.319467]\n",
      "18-10000 [D loss: 0.121757, acc.: 97.92%] [G loss: 4.133110]\n",
      "-----------------------------------------------------------------\n",
      "--- 1479.7590653896332 seconds ---\n",
      "-----------------------------------------------------------------\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_3 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_12 (LeakyReLU)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 533,505\n",
      "Trainable params: 533,505\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_18 (Dense)             (None, 256)               25856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_13 (LeakyReLU)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_14 (LeakyReLU)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_15 (LeakyReLU)   (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 784)               803600    \n",
      "_________________________________________________________________\n",
      "reshape_3 (Reshape)          (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 1,493,520\n",
      "Trainable params: 1,489,936\n",
      "Non-trainable params: 3,584\n",
      "_________________________________________________________________\n",
      "Count_image koropak = 6\n",
      "Count_image kawih = 4\n",
      "SumImg = 35\n",
      "25-0 [D loss: 0.826147, acc.: 22.92%] [G loss: 0.586052]\n",
      "25-1000 [D loss: 0.080562, acc.: 100.00%] [G loss: 4.258167]\n",
      "25-2000 [D loss: 0.500169, acc.: 83.33%] [G loss: 2.208656]\n",
      "25-3000 [D loss: 0.266235, acc.: 89.58%] [G loss: 2.862052]\n",
      "25-4000 [D loss: 0.222419, acc.: 91.67%] [G loss: 3.381445]\n",
      "25-5000 [D loss: 0.284433, acc.: 81.25%] [G loss: 2.643731]\n",
      "25-6000 [D loss: 0.051253, acc.: 100.00%] [G loss: 3.286692]\n",
      "25-7000 [D loss: 0.167163, acc.: 93.75%] [G loss: 3.674870]\n",
      "25-8000 [D loss: 0.107556, acc.: 95.83%] [G loss: 3.593035]\n",
      "25-9000 [D loss: 0.313309, acc.: 89.58%] [G loss: 2.243208]\n",
      "25-10000 [D loss: 0.169340, acc.: 93.75%] [G loss: 3.620768]\n",
      "-----------------------------------------------------------------\n",
      "--- 2231.7154886722565 seconds ---\n",
      "-----------------------------------------------------------------\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_4 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_16 (LeakyReLU)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_17 (LeakyReLU)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 533,505\n",
      "Trainable params: 533,505\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_25 (Dense)             (None, 256)               25856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_18 (LeakyReLU)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_19 (LeakyReLU)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_20 (LeakyReLU)   (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 784)               803600    \n",
      "_________________________________________________________________\n",
      "reshape_4 (Reshape)          (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 1,493,520\n",
      "Trainable params: 1,489,936\n",
      "Non-trainable params: 3,584\n",
      "_________________________________________________________________\n",
      "Count_image koropak = 5\n",
      "Count_image kawih = 19\n",
      "SumImg = 55\n",
      "27-0 [D loss: 0.602971, acc.: 52.08%] [G loss: 0.524347]\n",
      "27-1000 [D loss: 0.094888, acc.: 97.92%] [G loss: 3.354272]\n",
      "27-2000 [D loss: 0.065938, acc.: 100.00%] [G loss: 4.491134]\n",
      "27-3000 [D loss: 0.107469, acc.: 97.92%] [G loss: 2.979260]\n",
      "27-4000 [D loss: 0.258699, acc.: 87.50%] [G loss: 3.460899]\n",
      "27-5000 [D loss: 0.261113, acc.: 87.50%] [G loss: 2.887000]\n",
      "27-6000 [D loss: 0.149028, acc.: 95.83%] [G loss: 3.528750]\n",
      "27-7000 [D loss: 0.225116, acc.: 85.42%] [G loss: 2.651403]\n",
      "27-8000 [D loss: 0.071245, acc.: 97.92%] [G loss: 3.434508]\n",
      "27-9000 [D loss: 0.190960, acc.: 93.75%] [G loss: 3.078413]\n",
      "27-10000 [D loss: 0.089341, acc.: 97.92%] [G loss: 3.160455]\n",
      "-----------------------------------------------------------------\n",
      "--- 2982.777976036072 seconds ---\n",
      "-----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "import os\n",
    "import scipy\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "\n",
    "class GAN():\n",
    "    def __init__(self):\n",
    "        self.img_rows = 28\n",
    "        self.img_cols = 28\n",
    "        self.channels = 1\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "        self.latent_dim = 100\n",
    "\n",
    "        optimizer = Adam(0.0002, 0.5)\n",
    "\n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(loss='binary_crossentropy',\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "        # Build the generator\n",
    "        self.generator = self.build_generator()\n",
    "\n",
    "        # The generator takes noise as input and generates imgs\n",
    "        z = Input(shape=(self.latent_dim,))\n",
    "        img = self.generator(z)\n",
    "\n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "\n",
    "        # The discriminator takes generated images as input and determines validity\n",
    "        validity = self.discriminator(img)\n",
    "\n",
    "        # The combined model  (stacked generator and discriminator)\n",
    "        # Trains the generator to fool the discriminator\n",
    "        self.combined = Model(z, validity)\n",
    "        self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "\n",
    "    def build_generator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Dense(256, input_dim=self.latent_dim))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(512))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(1024))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(np.prod(self.img_shape), activation='tanh'))\n",
    "        model.add(Reshape(self.img_shape))\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        noise = Input(shape=(self.latent_dim,))\n",
    "        img = model(noise)\n",
    "\n",
    "        return Model(noise, img)\n",
    "\n",
    "    def build_discriminator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Flatten(input_shape=self.img_shape))\n",
    "        model.add(Dense(512))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dense(256))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        model.summary()\n",
    "\n",
    "        img = Input(shape=self.img_shape)\n",
    "        validity = model(img)\n",
    "\n",
    "        return Model(img, validity)\n",
    "\n",
    "    def train(self, aksara, aksara_num, epochs, batch_size=128, sample_interval=50):\n",
    "\n",
    "        # Load the dataset\n",
    "#         preprocess()\n",
    "        X_train, sumTrain= get_sunda(aksara)\n",
    "        X_train = X_train[:,:,:,0]\n",
    "#         (X_train, _), (_, _) = mnist.load_data()\n",
    "\n",
    "        # Rescale -1 to 1\n",
    "        X_train = X_train / 127.5 - 1.\n",
    "        X_train = np.expand_dims(X_train, axis=3)\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = np.ones((batch_size, 1))\n",
    "        fake = np.zeros((batch_size, 1))\n",
    "        \n",
    "        min_loss = 100\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "\n",
    "            # Select a random batch of images\n",
    "            idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "            imgs = X_train[idx]\n",
    "\n",
    "            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "\n",
    "            # Generate a batch of new images\n",
    "            gen_imgs = self.generator.predict(noise)\n",
    "\n",
    "            # Train the discriminator\n",
    "            d_loss_real = self.discriminator.train_on_batch(imgs, valid)\n",
    "            d_loss_fake = self.discriminator.train_on_batch(gen_imgs, fake)\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Generator\n",
    "            # ---------------------\n",
    "\n",
    "            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "\n",
    "            # Train the generator (to have the discriminator label samples as valid)\n",
    "            g_loss = self.combined.train_on_batch(noise, valid)\n",
    "\n",
    "            # Plot the progress\n",
    "#             print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
    "\n",
    "            # If at save interval => save generated image samples\n",
    "            if epoch % sample_interval == 0:\n",
    "                print (\"%d-%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (aksara, epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
    "                self.sample_images(epoch, aksara, sumTrain, aksara_num)\n",
    "            \n",
    "#             if d_loss[0] + g_loss < min_loss and epoch > 10000:\n",
    "#                 min_loss = d_loss[0] + g_loss\n",
    "#                 self.sample_images(epoch, aksara, sumTrain, aksara_num)\n",
    "\n",
    "    def sample_images(self, epoch, aksara, sumTrain, aksara_num):\n",
    "        aksaraList=[\"A\",\"BA\",\"CA\",\"DA\",\"GA\",\"HA\",\"I\",\"JA\",\"KA\",\"LA\",\"MA\",\"NA\",\"NGA\",\"NYA\",\"PA\",\"PANELENG\",\"PANEULEUNG\",\"PANGHULU\",\"PANGLAYAR\",\"PANOLONG\",\"PANYUKU\",\"PATEN\",\"RA\",\"SA\",\"TA\",\"U\",\"WA\",\"YA\"]\n",
    "        r, c = 10, 10\n",
    "        noise = np.random.normal(0, 1, (r * c, self.latent_dim))\n",
    "        gen_imgs = self.generator.predict(noise)\n",
    "\n",
    "        # Rescale images 0 - 1\n",
    "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "        for i in range(0,aksara_num):\n",
    "            img = gen_imgs[i,:,:,0]\n",
    "            normalizedImg = np.zeros((28,28))\n",
    "            normalizedImg = cv.normalize(img, normalizedImg, 0, 255, cv.NORM_MINMAX)\n",
    "            cv.imwrite(\"GAN_generated_images/GAN_training_only/%s_%s.png\" % (aksaraList[aksara],str(sumTrain+i+1)), normalizedImg)\n",
    "        for i in range(aksara_num,aksara_num+10):\n",
    "            img = gen_imgs[i,:,:,0]\n",
    "            normalizedImg = np.zeros((28,28))\n",
    "            normalizedImg = cv.normalize(img, normalizedImg, 0, 255, cv.NORM_MINMAX)\n",
    "            cv.imwrite(\"GAN_generated_images/GAN_training_only/%s_Cadangan_%s.png\" % (aksaraList[aksara],str(i+1)), normalizedImg)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    start_time = time.time()\n",
    "    aksara=[13,18,25,27]\n",
    "    aksara_num=[38,23,37,25]\n",
    "    for i in range(0,4):\n",
    "        gan = GAN()\n",
    "        gan.train(aksara[i], aksara_num[i], epochs=10001, batch_size=24, sample_interval=1000)\n",
    "        print(\"-----------------------------------------------------------------\")\n",
    "        print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "        print(\"-----------------------------------------------------------------\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_2 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 533,505\n",
      "Trainable params: 533,505\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_11 (Dense)             (None, 256)               25856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)   (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 784)               803600    \n",
      "_________________________________________________________________\n",
      "reshape_2 (Reshape)          (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 1,493,520\n",
      "Trainable params: 1,489,936\n",
      "Non-trainable params: 3,584\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From C:\\Users\\Sadrakh\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sadrakh\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0-0 [D loss: 0.798397, acc.: 51.04%] [G loss: 0.718805]\n",
      "0-1000 [D loss: 0.134721, acc.: 100.00%] [G loss: 3.232478]\n",
      "0-2000 [D loss: 0.233812, acc.: 93.75%] [G loss: 2.447292]\n",
      "0-3000 [D loss: 0.245962, acc.: 90.62%] [G loss: 2.709725]\n",
      "0-4000 [D loss: 0.197391, acc.: 92.71%] [G loss: 2.442304]\n",
      "0-5000 [D loss: 0.267694, acc.: 88.54%] [G loss: 2.841992]\n",
      "0-6000 [D loss: 0.170154, acc.: 93.75%] [G loss: 2.776485]\n",
      "0-7000 [D loss: 0.233312, acc.: 87.50%] [G loss: 2.457732]\n",
      "0-8000 [D loss: 0.164447, acc.: 94.79%] [G loss: 2.848845]\n",
      "0-9000 [D loss: 0.098829, acc.: 97.92%] [G loss: 3.764603]\n",
      "0-10000 [D loss: 0.224366, acc.: 92.71%] [G loss: 3.156589]\n",
      "0-11000 [D loss: 0.166608, acc.: 95.83%] [G loss: 3.200519]\n",
      "0-12000 [D loss: 0.139632, acc.: 96.88%] [G loss: 3.521345]\n",
      "0-13000 [D loss: 0.172081, acc.: 93.75%] [G loss: 3.368053]\n",
      "0-14000 [D loss: 0.081057, acc.: 97.92%] [G loss: 3.755254]\n",
      "0-15000 [D loss: 0.190834, acc.: 94.79%] [G loss: 3.514821]\n",
      "0-16000 [D loss: 0.154563, acc.: 94.79%] [G loss: 3.068565]\n",
      "0-17000 [D loss: 0.097606, acc.: 97.92%] [G loss: 3.285231]\n",
      "0-18000 [D loss: 0.161008, acc.: 95.83%] [G loss: 3.136484]\n",
      "0-19000 [D loss: 0.184011, acc.: 94.79%] [G loss: 3.225016]\n",
      "0-20000 [D loss: 0.173802, acc.: 93.75%] [G loss: 2.634283]\n",
      "0-21000 [D loss: 0.119664, acc.: 96.88%] [G loss: 3.669947]\n",
      "0-22000 [D loss: 0.120291, acc.: 96.88%] [G loss: 3.261364]\n",
      "0-23000 [D loss: 0.109106, acc.: 96.88%] [G loss: 3.428701]\n",
      "0-24000 [D loss: 0.161163, acc.: 95.83%] [G loss: 2.929056]\n",
      "0-25000 [D loss: 0.166304, acc.: 94.79%] [G loss: 3.026939]\n",
      "0-26000 [D loss: 0.195600, acc.: 94.79%] [G loss: 3.068057]\n",
      "0-27000 [D loss: 0.210920, acc.: 93.75%] [G loss: 2.621295]\n",
      "0-28000 [D loss: 0.151126, acc.: 95.83%] [G loss: 3.194458]\n",
      "0-29000 [D loss: 0.162121, acc.: 95.83%] [G loss: 3.255035]\n",
      "0-30000 [D loss: 0.142719, acc.: 95.83%] [G loss: 3.102381]\n",
      "-----------------------------------------------------------------\n",
      "--- 5160.591403961182 seconds ---\n",
      "-----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == '__main__':\n",
    "    start_time = time.time()\n",
    "    aksara=[0]\n",
    "    aksara_num=[16]\n",
    "    for i in range(0,1):\n",
    "        gan = GAN()\n",
    "        gan.train(aksara[i], aksara_num[i], epochs=30001, batch_size=48, sample_interval=1000)\n",
    "        print(\"-----------------------------------------------------------------\")\n",
    "        print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "        print(\"-----------------------------------------------------------------\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Sadrakh\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 533,505\n",
      "Trainable params: 533,505\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 256)               25856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 784)               803600    \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 1,493,520\n",
      "Trainable params: 1,489,936\n",
      "Non-trainable params: 3,584\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From C:\\Users\\Sadrakh\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sadrakh\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-0 [D loss: 0.626050, acc.: 45.83%] [G loss: 0.485649]\n",
      "1-1000 [D loss: 0.128476, acc.: 100.00%] [G loss: 4.069482]\n",
      "1-2000 [D loss: 0.237716, acc.: 91.67%] [G loss: 3.169679]\n",
      "1-3000 [D loss: 0.054870, acc.: 100.00%] [G loss: 3.807199]\n",
      "1-4000 [D loss: 0.132430, acc.: 94.79%] [G loss: 3.183053]\n",
      "1-5000 [D loss: 0.107382, acc.: 96.88%] [G loss: 3.201871]\n",
      "1-6000 [D loss: 0.260991, acc.: 89.58%] [G loss: 2.767683]\n",
      "1-7000 [D loss: 0.275476, acc.: 91.67%] [G loss: 2.770325]\n",
      "1-8000 [D loss: 0.168208, acc.: 94.79%] [G loss: 3.542899]\n",
      "1-9000 [D loss: 0.142599, acc.: 93.75%] [G loss: 3.440157]\n",
      "1-10000 [D loss: 0.186412, acc.: 93.75%] [G loss: 3.363034]\n",
      "1-11000 [D loss: 0.142329, acc.: 94.79%] [G loss: 3.309966]\n",
      "1-12000 [D loss: 0.118339, acc.: 95.83%] [G loss: 3.870631]\n",
      "1-13000 [D loss: 0.173666, acc.: 93.75%] [G loss: 3.689058]\n",
      "1-14000 [D loss: 0.156913, acc.: 93.75%] [G loss: 3.553060]\n",
      "1-15000 [D loss: 0.186420, acc.: 92.71%] [G loss: 3.478856]\n",
      "1-16000 [D loss: 0.190914, acc.: 89.58%] [G loss: 3.778640]\n",
      "1-17000 [D loss: 0.119717, acc.: 92.71%] [G loss: 4.400187]\n",
      "1-18000 [D loss: 0.169243, acc.: 92.71%] [G loss: 4.545187]\n",
      "1-19000 [D loss: 0.181279, acc.: 92.71%] [G loss: 4.440076]\n",
      "1-20000 [D loss: 0.146512, acc.: 94.79%] [G loss: 3.923618]\n",
      "1-21000 [D loss: 0.149220, acc.: 94.79%] [G loss: 4.237690]\n",
      "1-22000 [D loss: 0.146923, acc.: 91.67%] [G loss: 4.899380]\n",
      "1-23000 [D loss: 0.124279, acc.: 94.79%] [G loss: 5.309513]\n",
      "1-24000 [D loss: 0.122386, acc.: 95.83%] [G loss: 3.583663]\n",
      "1-25000 [D loss: 0.172826, acc.: 91.67%] [G loss: 4.336934]\n",
      "1-26000 [D loss: 0.102150, acc.: 96.88%] [G loss: 5.374907]\n",
      "1-27000 [D loss: 0.155393, acc.: 91.67%] [G loss: 4.835453]\n",
      "1-28000 [D loss: 0.110407, acc.: 95.83%] [G loss: 5.051018]\n",
      "1-29000 [D loss: 0.109314, acc.: 95.83%] [G loss: 4.403612]\n",
      "1-30000 [D loss: 0.130219, acc.: 95.83%] [G loss: 3.972158]\n",
      "-----------------------------------------------------------------\n",
      "--- 6374.8666479587555 seconds ---\n",
      "-----------------------------------------------------------------\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_2 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 533,505\n",
      "Trainable params: 533,505\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_11 (Dense)             (None, 256)               25856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)   (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 784)               803600    \n",
      "_________________________________________________________________\n",
      "reshape_2 (Reshape)          (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 1,493,520\n",
      "Trainable params: 1,489,936\n",
      "Non-trainable params: 3,584\n",
      "_________________________________________________________________\n",
      "2-0 [D loss: 0.717623, acc.: 61.46%] [G loss: 0.704438]\n",
      "2-1000 [D loss: 0.480582, acc.: 78.12%] [G loss: 2.556268]\n",
      "2-2000 [D loss: 0.226376, acc.: 92.71%] [G loss: 2.635207]\n",
      "2-3000 [D loss: 0.248672, acc.: 88.54%] [G loss: 2.066908]\n",
      "2-4000 [D loss: 0.330566, acc.: 85.42%] [G loss: 2.509400]\n",
      "2-5000 [D loss: 0.285482, acc.: 85.42%] [G loss: 2.172141]\n",
      "2-6000 [D loss: 0.224120, acc.: 89.58%] [G loss: 2.910487]\n",
      "2-7000 [D loss: 0.321565, acc.: 83.33%] [G loss: 2.566722]\n",
      "2-8000 [D loss: 0.187772, acc.: 91.67%] [G loss: 2.853340]\n",
      "2-9000 [D loss: 0.183241, acc.: 94.79%] [G loss: 3.068192]\n",
      "2-10000 [D loss: 0.202248, acc.: 93.75%] [G loss: 3.751029]\n",
      "2-11000 [D loss: 0.284083, acc.: 83.33%] [G loss: 3.617696]\n",
      "2-12000 [D loss: 0.212331, acc.: 89.58%] [G loss: 3.290065]\n",
      "2-13000 [D loss: 0.181396, acc.: 91.67%] [G loss: 3.638538]\n",
      "2-14000 [D loss: 0.190253, acc.: 93.75%] [G loss: 3.253791]\n",
      "2-15000 [D loss: 0.156779, acc.: 94.79%] [G loss: 4.579545]\n",
      "2-16000 [D loss: 0.171820, acc.: 94.79%] [G loss: 4.970859]\n",
      "2-17000 [D loss: 0.158635, acc.: 94.79%] [G loss: 4.036012]\n",
      "2-18000 [D loss: 0.164555, acc.: 94.79%] [G loss: 5.181774]\n",
      "2-19000 [D loss: 0.254107, acc.: 87.50%] [G loss: 4.870401]\n",
      "2-20000 [D loss: 0.186953, acc.: 91.67%] [G loss: 5.303628]\n",
      "2-21000 [D loss: 0.181255, acc.: 95.83%] [G loss: 4.779900]\n",
      "2-22000 [D loss: 0.179628, acc.: 90.62%] [G loss: 5.527614]\n",
      "2-23000 [D loss: 0.478779, acc.: 82.29%] [G loss: 4.533854]\n",
      "2-24000 [D loss: 0.168269, acc.: 95.83%] [G loss: 7.223768]\n",
      "2-25000 [D loss: 0.091978, acc.: 97.92%] [G loss: 4.858709]\n",
      "2-26000 [D loss: 0.137517, acc.: 93.75%] [G loss: 6.871427]\n",
      "2-27000 [D loss: 0.182962, acc.: 95.83%] [G loss: 3.821806]\n",
      "2-28000 [D loss: 0.191719, acc.: 91.67%] [G loss: 4.834835]\n",
      "2-29000 [D loss: 0.229278, acc.: 93.75%] [G loss: 3.910230]\n",
      "2-30000 [D loss: 0.209300, acc.: 90.62%] [G loss: 4.239624]\n",
      "-----------------------------------------------------------------\n",
      "--- 12511.356689691544 seconds ---\n",
      "-----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == '__main__':\n",
    "    start_time = time.time()\n",
    "    aksara=[1,2]\n",
    "    aksara_num=[24,24]\n",
    "    for i in range(0,2):\n",
    "        gan = GAN()\n",
    "        gan.train(aksara[i], aksara_num[i], epochs=30001, batch_size=48, sample_interval=1000)\n",
    "        print(\"-----------------------------------------------------------------\")\n",
    "        print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "        print(\"-----------------------------------------------------------------\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "aksara=2\n",
    "aksara_num=27\n",
    "gan = GAN()\n",
    "gan.train(aksara, aksara_num, epochs=20001, batch_size=48, sample_interval=1000)\n",
    "print(\"-----------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Sadrakh\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 533,505\n",
      "Trainable params: 533,505\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 256)               25856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 784)               803600    \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 1,493,520\n",
      "Trainable params: 1,489,936\n",
      "Non-trainable params: 3,584\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From C:\\Users\\Sadrakh\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sadrakh\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4-0 [D loss: 0.610326, acc.: 72.92%] [G loss: 0.830040]\n",
      "4-1000 [D loss: 0.029783, acc.: 100.00%] [G loss: 4.822430]\n",
      "4-2000 [D loss: 0.149972, acc.: 94.79%] [G loss: 3.693342]\n",
      "4-3000 [D loss: 0.127729, acc.: 94.79%] [G loss: 3.944894]\n",
      "4-4000 [D loss: 0.325468, acc.: 88.54%] [G loss: 2.532356]\n",
      "4-5000 [D loss: 0.327141, acc.: 87.50%] [G loss: 2.639507]\n",
      "4-6000 [D loss: 0.196871, acc.: 92.71%] [G loss: 2.968440]\n",
      "4-7000 [D loss: 0.122801, acc.: 95.83%] [G loss: 3.039479]\n",
      "4-8000 [D loss: 0.189742, acc.: 93.75%] [G loss: 2.867384]\n",
      "4-9000 [D loss: 0.205211, acc.: 93.75%] [G loss: 3.190418]\n",
      "4-10000 [D loss: 0.232591, acc.: 89.58%] [G loss: 3.439430]\n",
      "4-11000 [D loss: 0.140849, acc.: 96.88%] [G loss: 3.305743]\n",
      "4-12000 [D loss: 0.115760, acc.: 95.83%] [G loss: 4.155413]\n",
      "4-13000 [D loss: 0.280250, acc.: 89.58%] [G loss: 3.961958]\n",
      "4-14000 [D loss: 0.110069, acc.: 97.92%] [G loss: 3.959473]\n",
      "4-15000 [D loss: 0.199278, acc.: 92.71%] [G loss: 3.896241]\n",
      "4-16000 [D loss: 0.194762, acc.: 90.62%] [G loss: 3.853235]\n",
      "4-17000 [D loss: 0.220706, acc.: 87.50%] [G loss: 3.764877]\n",
      "4-18000 [D loss: 0.155133, acc.: 93.75%] [G loss: 3.751161]\n",
      "4-19000 [D loss: 0.119018, acc.: 95.83%] [G loss: 5.098902]\n",
      "4-20000 [D loss: 0.227020, acc.: 86.46%] [G loss: 3.703166]\n",
      "4-21000 [D loss: 0.139621, acc.: 92.71%] [G loss: 4.513026]\n",
      "4-22000 [D loss: 0.111968, acc.: 95.83%] [G loss: 4.715862]\n",
      "4-23000 [D loss: 0.107297, acc.: 95.83%] [G loss: 5.253189]\n",
      "4-24000 [D loss: 0.185493, acc.: 93.75%] [G loss: 4.416460]\n",
      "4-25000 [D loss: 0.089787, acc.: 96.88%] [G loss: 5.348834]\n",
      "4-26000 [D loss: 0.097424, acc.: 96.88%] [G loss: 4.656038]\n",
      "4-27000 [D loss: 0.188264, acc.: 90.62%] [G loss: 3.705264]\n",
      "4-28000 [D loss: 0.108110, acc.: 96.88%] [G loss: 6.321670]\n",
      "4-29000 [D loss: 0.116179, acc.: 95.83%] [G loss: 4.565444]\n",
      "4-30000 [D loss: 0.082574, acc.: 96.88%] [G loss: 5.432487]\n",
      "-----------------------------------------------------------------\n",
      "--- 6781.829559326172 seconds ---\n",
      "-----------------------------------------------------------------\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_2 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 533,505\n",
      "Trainable params: 533,505\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_11 (Dense)             (None, 256)               25856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)   (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 784)               803600    \n",
      "_________________________________________________________________\n",
      "reshape_2 (Reshape)          (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 1,493,520\n",
      "Trainable params: 1,489,936\n",
      "Non-trainable params: 3,584\n",
      "_________________________________________________________________\n",
      "5-0 [D loss: 0.736071, acc.: 45.83%] [G loss: 0.799251]\n",
      "5-1000 [D loss: 0.312509, acc.: 88.54%] [G loss: 2.583249]\n",
      "5-2000 [D loss: 0.106081, acc.: 96.88%] [G loss: 3.498859]\n",
      "5-3000 [D loss: 0.175857, acc.: 94.79%] [G loss: 2.991071]\n",
      "5-4000 [D loss: 0.150472, acc.: 93.75%] [G loss: 3.365704]\n",
      "5-5000 [D loss: 0.136929, acc.: 94.79%] [G loss: 3.077794]\n",
      "5-6000 [D loss: 0.310466, acc.: 89.58%] [G loss: 2.891051]\n",
      "5-7000 [D loss: 0.377139, acc.: 87.50%] [G loss: 4.567174]\n",
      "5-8000 [D loss: 0.180110, acc.: 91.67%] [G loss: 3.039613]\n",
      "5-9000 [D loss: 0.077911, acc.: 95.83%] [G loss: 4.087974]\n",
      "5-10000 [D loss: 0.109325, acc.: 95.83%] [G loss: 3.174046]\n",
      "5-11000 [D loss: 0.209332, acc.: 91.67%] [G loss: 4.319756]\n",
      "5-12000 [D loss: 0.220429, acc.: 90.62%] [G loss: 5.467718]\n",
      "5-13000 [D loss: 0.170257, acc.: 90.62%] [G loss: 3.333102]\n",
      "5-14000 [D loss: 0.218215, acc.: 93.75%] [G loss: 4.912817]\n",
      "5-15000 [D loss: 0.132981, acc.: 91.67%] [G loss: 4.431324]\n",
      "5-16000 [D loss: 0.045440, acc.: 98.96%] [G loss: 5.478094]\n",
      "5-17000 [D loss: 0.157932, acc.: 95.83%] [G loss: 4.825691]\n",
      "5-18000 [D loss: 0.248411, acc.: 85.42%] [G loss: 5.473078]\n",
      "5-19000 [D loss: 0.145886, acc.: 95.83%] [G loss: 5.857008]\n",
      "5-20000 [D loss: 0.123716, acc.: 92.71%] [G loss: 5.679050]\n",
      "5-21000 [D loss: 0.132716, acc.: 91.67%] [G loss: 5.810831]\n",
      "5-22000 [D loss: 0.095503, acc.: 95.83%] [G loss: 5.396683]\n",
      "5-23000 [D loss: 0.195670, acc.: 93.75%] [G loss: 4.827258]\n",
      "5-24000 [D loss: 0.075789, acc.: 97.92%] [G loss: 7.430071]\n",
      "5-25000 [D loss: 0.187326, acc.: 90.62%] [G loss: 5.546353]\n",
      "5-26000 [D loss: 0.140008, acc.: 93.75%] [G loss: 5.355085]\n",
      "5-27000 [D loss: 0.118455, acc.: 98.96%] [G loss: 4.594232]\n",
      "5-28000 [D loss: 0.100515, acc.: 96.88%] [G loss: 5.414136]\n",
      "5-29000 [D loss: 0.136649, acc.: 94.79%] [G loss: 4.567452]\n",
      "5-30000 [D loss: 0.126651, acc.: 96.88%] [G loss: 6.768841]\n",
      "-----------------------------------------------------------------\n",
      "--- 13253.23496723175 seconds ---\n",
      "-----------------------------------------------------------------\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_3 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_12 (LeakyReLU)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 533,505\n",
      "Trainable params: 533,505\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_18 (Dense)             (None, 256)               25856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_13 (LeakyReLU)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_14 (LeakyReLU)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_15 (LeakyReLU)   (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 784)               803600    \n",
      "_________________________________________________________________\n",
      "reshape_3 (Reshape)          (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 1,493,520\n",
      "Trainable params: 1,489,936\n",
      "Non-trainable params: 3,584\n",
      "_________________________________________________________________\n",
      "6-0 [D loss: 1.014422, acc.: 44.79%] [G loss: 1.060038]\n",
      "6-1000 [D loss: 0.237727, acc.: 91.67%] [G loss: 2.730915]\n",
      "6-2000 [D loss: 0.256987, acc.: 90.62%] [G loss: 2.255905]\n",
      "6-3000 [D loss: 0.306662, acc.: 87.50%] [G loss: 2.547477]\n",
      "6-4000 [D loss: 0.133197, acc.: 97.92%] [G loss: 2.834335]\n",
      "6-5000 [D loss: 0.205343, acc.: 91.67%] [G loss: 3.560060]\n",
      "6-6000 [D loss: 0.246898, acc.: 88.54%] [G loss: 3.748746]\n",
      "6-7000 [D loss: 0.164488, acc.: 96.88%] [G loss: 4.918743]\n",
      "6-8000 [D loss: 0.389248, acc.: 79.17%] [G loss: 4.050931]\n",
      "6-9000 [D loss: 0.191370, acc.: 94.79%] [G loss: 2.890940]\n",
      "6-10000 [D loss: 0.294445, acc.: 89.58%] [G loss: 2.780090]\n",
      "6-11000 [D loss: 0.136129, acc.: 93.75%] [G loss: 3.299245]\n",
      "6-12000 [D loss: 0.242932, acc.: 88.54%] [G loss: 3.583355]\n",
      "6-13000 [D loss: 0.134096, acc.: 96.88%] [G loss: 3.837053]\n",
      "6-14000 [D loss: 0.117395, acc.: 96.88%] [G loss: 3.493462]\n",
      "6-15000 [D loss: 0.176238, acc.: 93.75%] [G loss: 3.908272]\n",
      "6-16000 [D loss: 0.227869, acc.: 93.75%] [G loss: 4.013836]\n",
      "6-17000 [D loss: 0.115060, acc.: 96.88%] [G loss: 3.548701]\n",
      "6-18000 [D loss: 0.113879, acc.: 96.88%] [G loss: 3.758530]\n",
      "6-19000 [D loss: 0.148378, acc.: 95.83%] [G loss: 3.481395]\n",
      "6-20000 [D loss: 0.217355, acc.: 93.75%] [G loss: 3.414450]\n",
      "6-21000 [D loss: 0.112257, acc.: 96.88%] [G loss: 3.335026]\n",
      "6-22000 [D loss: 0.121859, acc.: 96.88%] [G loss: 4.171698]\n",
      "6-23000 [D loss: 0.151042, acc.: 95.83%] [G loss: 3.441578]\n",
      "6-24000 [D loss: 0.197600, acc.: 94.79%] [G loss: 3.476716]\n",
      "6-25000 [D loss: 0.124305, acc.: 96.88%] [G loss: 3.699499]\n",
      "6-26000 [D loss: 0.056496, acc.: 98.96%] [G loss: 3.402211]\n",
      "6-27000 [D loss: 0.167675, acc.: 95.83%] [G loss: 3.412085]\n",
      "6-28000 [D loss: 0.200432, acc.: 94.79%] [G loss: 2.965119]\n",
      "6-29000 [D loss: 0.168721, acc.: 94.79%] [G loss: 3.201602]\n",
      "6-30000 [D loss: 0.224053, acc.: 93.75%] [G loss: 3.243094]\n",
      "-----------------------------------------------------------------\n",
      "--- 19426.384951114655 seconds ---\n",
      "-----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == '__main__':\n",
    "    start_time = time.time()\n",
    "    aksara=[4,5,6]\n",
    "    aksara_num=[8,19,24]\n",
    "    for i in range(0,3):\n",
    "        gan = GAN()\n",
    "        gan.train(aksara[i], aksara_num[i], epochs=30001, batch_size=48, sample_interval=1000)\n",
    "        print(\"-----------------------------------------------------------------\")\n",
    "        print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "        print(\"-----------------------------------------------------------------\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Sadrakh\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 533,505\n",
      "Trainable params: 533,505\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 256)               25856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 784)               803600    \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 1,493,520\n",
      "Trainable params: 1,489,936\n",
      "Non-trainable params: 3,584\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From C:\\Users\\Sadrakh\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sadrakh\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7-0 [D loss: 0.596523, acc.: 50.00%] [G loss: 0.392304]\n",
      "7-1000 [D loss: 0.396122, acc.: 84.38%] [G loss: 2.487470]\n",
      "7-2000 [D loss: 0.342031, acc.: 85.42%] [G loss: 1.963295]\n",
      "7-3000 [D loss: 0.235858, acc.: 91.67%] [G loss: 1.995665]\n",
      "7-4000 [D loss: 0.290153, acc.: 88.54%] [G loss: 2.245050]\n",
      "7-5000 [D loss: 0.247665, acc.: 90.62%] [G loss: 1.985743]\n",
      "7-6000 [D loss: 0.303459, acc.: 87.50%] [G loss: 2.142445]\n",
      "7-7000 [D loss: 0.307098, acc.: 87.50%] [G loss: 2.406739]\n",
      "7-8000 [D loss: 0.268866, acc.: 89.58%] [G loss: 2.649334]\n",
      "7-9000 [D loss: 0.260654, acc.: 92.71%] [G loss: 3.226983]\n",
      "7-10000 [D loss: 0.290214, acc.: 80.21%] [G loss: 2.393304]\n",
      "7-11000 [D loss: 0.306444, acc.: 85.42%] [G loss: 3.395325]\n",
      "7-12000 [D loss: 0.198541, acc.: 91.67%] [G loss: 3.258292]\n",
      "7-13000 [D loss: 0.291292, acc.: 88.54%] [G loss: 2.990735]\n",
      "7-14000 [D loss: 0.200564, acc.: 91.67%] [G loss: 3.006142]\n",
      "7-15000 [D loss: 0.206361, acc.: 91.67%] [G loss: 4.148321]\n",
      "7-16000 [D loss: 0.157281, acc.: 92.71%] [G loss: 5.851728]\n",
      "7-17000 [D loss: 0.226849, acc.: 89.58%] [G loss: 4.674045]\n",
      "7-18000 [D loss: 0.160919, acc.: 94.79%] [G loss: 3.765035]\n",
      "7-19000 [D loss: 0.100420, acc.: 96.88%] [G loss: 4.569805]\n",
      "7-20000 [D loss: 0.167460, acc.: 93.75%] [G loss: 3.540794]\n",
      "7-21000 [D loss: 0.242595, acc.: 91.67%] [G loss: 3.771322]\n",
      "7-22000 [D loss: 0.090117, acc.: 96.88%] [G loss: 5.816506]\n",
      "7-23000 [D loss: 0.136004, acc.: 95.83%] [G loss: 3.637981]\n",
      "7-24000 [D loss: 0.136480, acc.: 95.83%] [G loss: 3.019635]\n",
      "7-25000 [D loss: 0.192177, acc.: 93.75%] [G loss: 3.215976]\n",
      "7-26000 [D loss: 0.180528, acc.: 94.79%] [G loss: 3.451712]\n",
      "7-27000 [D loss: 0.121382, acc.: 96.88%] [G loss: 3.185024]\n",
      "7-28000 [D loss: 0.274019, acc.: 91.67%] [G loss: 3.096548]\n",
      "7-29000 [D loss: 0.213208, acc.: 93.75%] [G loss: 3.429198]\n",
      "7-30000 [D loss: 0.178949, acc.: 93.75%] [G loss: 3.545627]\n",
      "-----------------------------------------------------------------\n",
      "--- 5917.3602206707 seconds ---\n",
      "-----------------------------------------------------------------\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_2 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 533,505\n",
      "Trainable params: 533,505\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_11 (Dense)             (None, 256)               25856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)   (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 784)               803600    \n",
      "_________________________________________________________________\n",
      "reshape_2 (Reshape)          (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 1,493,520\n",
      "Trainable params: 1,489,936\n",
      "Non-trainable params: 3,584\n",
      "_________________________________________________________________\n",
      "12-0 [D loss: 0.519923, acc.: 50.00%] [G loss: 0.431669]\n",
      "12-1000 [D loss: 0.229477, acc.: 90.62%] [G loss: 3.222585]\n",
      "12-2000 [D loss: 0.304589, acc.: 89.58%] [G loss: 2.236382]\n",
      "12-3000 [D loss: 0.204226, acc.: 92.71%] [G loss: 2.572457]\n",
      "12-4000 [D loss: 0.258036, acc.: 89.58%] [G loss: 2.645949]\n",
      "12-5000 [D loss: 0.270115, acc.: 86.46%] [G loss: 2.393155]\n",
      "12-6000 [D loss: 0.282033, acc.: 82.29%] [G loss: 2.427171]\n",
      "12-7000 [D loss: 0.169932, acc.: 93.75%] [G loss: 3.276139]\n",
      "12-8000 [D loss: 0.279548, acc.: 84.38%] [G loss: 3.205157]\n",
      "12-9000 [D loss: 0.220811, acc.: 89.58%] [G loss: 3.150583]\n",
      "12-10000 [D loss: 0.146164, acc.: 94.79%] [G loss: 3.419858]\n",
      "12-11000 [D loss: 0.216205, acc.: 93.75%] [G loss: 4.034259]\n",
      "12-12000 [D loss: 0.245044, acc.: 88.54%] [G loss: 5.071719]\n",
      "12-13000 [D loss: 0.152827, acc.: 93.75%] [G loss: 4.433492]\n",
      "12-14000 [D loss: 0.180442, acc.: 94.79%] [G loss: 3.637860]\n",
      "12-15000 [D loss: 0.141777, acc.: 91.67%] [G loss: 5.205332]\n",
      "12-16000 [D loss: 0.177962, acc.: 94.79%] [G loss: 3.497769]\n",
      "12-17000 [D loss: 0.161197, acc.: 92.71%] [G loss: 3.721757]\n",
      "12-18000 [D loss: 0.098765, acc.: 96.88%] [G loss: 3.977482]\n",
      "12-19000 [D loss: 0.125306, acc.: 94.79%] [G loss: 5.671667]\n",
      "12-20000 [D loss: 0.104180, acc.: 95.83%] [G loss: 5.107955]\n",
      "12-21000 [D loss: 0.075716, acc.: 96.88%] [G loss: 5.681529]\n",
      "12-22000 [D loss: 0.128647, acc.: 95.83%] [G loss: 5.209872]\n",
      "12-23000 [D loss: 0.190096, acc.: 89.58%] [G loss: 4.771292]\n",
      "12-24000 [D loss: 0.130805, acc.: 94.79%] [G loss: 6.161842]\n",
      "12-25000 [D loss: 0.105598, acc.: 96.88%] [G loss: 5.742215]\n",
      "12-26000 [D loss: 0.153208, acc.: 94.79%] [G loss: 6.813516]\n",
      "12-27000 [D loss: 0.148502, acc.: 91.67%] [G loss: 4.991410]\n",
      "12-28000 [D loss: 0.124939, acc.: 92.71%] [G loss: 5.678396]\n",
      "12-29000 [D loss: 0.173062, acc.: 91.67%] [G loss: 4.546992]\n",
      "12-30000 [D loss: 0.122739, acc.: 95.83%] [G loss: 6.841950]\n",
      "-----------------------------------------------------------------\n",
      "--- 12675.46747636795 seconds ---\n",
      "-----------------------------------------------------------------\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_3 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_12 (LeakyReLU)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 533,505\n",
      "Trainable params: 533,505\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_18 (Dense)             (None, 256)               25856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_13 (LeakyReLU)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_14 (LeakyReLU)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_15 (LeakyReLU)   (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 784)               803600    \n",
      "_________________________________________________________________\n",
      "reshape_3 (Reshape)          (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 1,493,520\n",
      "Trainable params: 1,489,936\n",
      "Non-trainable params: 3,584\n",
      "_________________________________________________________________\n",
      "13-0 [D loss: 0.562412, acc.: 63.54%] [G loss: 0.519471]\n",
      "13-1000 [D loss: 0.236260, acc.: 90.62%] [G loss: 2.518946]\n",
      "13-2000 [D loss: 0.302446, acc.: 86.46%] [G loss: 1.991547]\n",
      "13-3000 [D loss: 0.284470, acc.: 88.54%] [G loss: 2.399041]\n",
      "13-4000 [D loss: 0.305573, acc.: 85.42%] [G loss: 2.167506]\n",
      "13-5000 [D loss: 0.374851, acc.: 84.38%] [G loss: 1.631770]\n",
      "13-6000 [D loss: 0.401621, acc.: 82.29%] [G loss: 1.961074]\n",
      "13-7000 [D loss: 0.241847, acc.: 87.50%] [G loss: 3.541899]\n",
      "13-8000 [D loss: 0.233948, acc.: 86.46%] [G loss: 2.511164]\n",
      "13-9000 [D loss: 0.252453, acc.: 89.58%] [G loss: 2.875370]\n",
      "13-10000 [D loss: 0.197450, acc.: 90.62%] [G loss: 3.742240]\n",
      "13-11000 [D loss: 0.262990, acc.: 83.33%] [G loss: 3.017995]\n",
      "13-12000 [D loss: 0.153916, acc.: 96.88%] [G loss: 4.822207]\n",
      "13-13000 [D loss: 0.170682, acc.: 92.71%] [G loss: 4.011044]\n",
      "13-14000 [D loss: 0.222556, acc.: 89.58%] [G loss: 3.653815]\n",
      "13-15000 [D loss: 0.195408, acc.: 90.62%] [G loss: 5.356081]\n",
      "13-16000 [D loss: 0.300671, acc.: 81.25%] [G loss: 5.184687]\n",
      "13-17000 [D loss: 0.189117, acc.: 91.67%] [G loss: 4.319262]\n",
      "13-18000 [D loss: 0.155111, acc.: 92.71%] [G loss: 3.629059]\n",
      "13-19000 [D loss: 0.187098, acc.: 87.50%] [G loss: 5.523590]\n",
      "13-20000 [D loss: 0.278585, acc.: 85.42%] [G loss: 5.931160]\n",
      "13-21000 [D loss: 0.204298, acc.: 91.67%] [G loss: 3.827109]\n",
      "13-22000 [D loss: 0.204678, acc.: 90.62%] [G loss: 4.113063]\n",
      "13-23000 [D loss: 0.215709, acc.: 85.42%] [G loss: 4.682169]\n",
      "13-24000 [D loss: 0.306445, acc.: 86.46%] [G loss: 7.565801]\n",
      "13-25000 [D loss: 0.124938, acc.: 96.88%] [G loss: 5.104444]\n",
      "13-26000 [D loss: 0.225946, acc.: 89.58%] [G loss: 4.126024]\n",
      "13-27000 [D loss: 0.211176, acc.: 90.62%] [G loss: 4.188439]\n",
      "13-28000 [D loss: 0.086858, acc.: 95.83%] [G loss: 4.271287]\n",
      "13-29000 [D loss: 0.108958, acc.: 94.79%] [G loss: 4.968701]\n",
      "13-30000 [D loss: 0.164062, acc.: 90.62%] [G loss: 6.640131]\n",
      "-----------------------------------------------------------------\n",
      "--- 19126.074605226517 seconds ---\n",
      "-----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == '__main__':\n",
    "    start_time = time.time()\n",
    "    aksara=[7,12,13]\n",
    "    aksara_num=[24,21,24]\n",
    "    for i in range(0,3):\n",
    "        gan = GAN()\n",
    "        gan.train(aksara[i], aksara_num[i], epochs=30001, batch_size=48, sample_interval=1000)\n",
    "        print(\"-----------------------------------------------------------------\")\n",
    "        print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "        print(\"-----------------------------------------------------------------\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Sadrakh\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 533,505\n",
      "Trainable params: 533,505\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 256)               25856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 784)               803600    \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 1,493,520\n",
      "Trainable params: 1,489,936\n",
      "Non-trainable params: 3,584\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From C:\\Users\\Sadrakh\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sadrakh\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18-0 [D loss: 0.504987, acc.: 69.79%] [G loss: 0.542743]\n",
      "18-1000 [D loss: 0.281308, acc.: 88.54%] [G loss: 2.455142]\n",
      "18-2000 [D loss: 0.267303, acc.: 88.54%] [G loss: 2.400737]\n",
      "18-3000 [D loss: 0.281740, acc.: 84.38%] [G loss: 2.304826]\n",
      "18-4000 [D loss: 0.361859, acc.: 85.42%] [G loss: 1.952903]\n",
      "18-5000 [D loss: 0.319918, acc.: 87.50%] [G loss: 2.132948]\n",
      "18-6000 [D loss: 0.268459, acc.: 89.58%] [G loss: 2.359825]\n",
      "18-7000 [D loss: 0.293484, acc.: 87.50%] [G loss: 2.292262]\n",
      "18-8000 [D loss: 0.427897, acc.: 78.12%] [G loss: 3.644894]\n",
      "18-9000 [D loss: 0.208218, acc.: 89.58%] [G loss: 3.089723]\n",
      "18-10000 [D loss: 0.234326, acc.: 91.67%] [G loss: 2.638286]\n",
      "18-11000 [D loss: 0.241724, acc.: 87.50%] [G loss: 3.713802]\n",
      "18-12000 [D loss: 0.241218, acc.: 88.54%] [G loss: 3.777828]\n",
      "18-13000 [D loss: 0.156676, acc.: 93.75%] [G loss: 4.592323]\n",
      "18-14000 [D loss: 0.166597, acc.: 91.67%] [G loss: 4.373890]\n",
      "18-15000 [D loss: 0.218427, acc.: 88.54%] [G loss: 3.575727]\n",
      "18-16000 [D loss: 0.185256, acc.: 90.62%] [G loss: 3.910001]\n",
      "18-17000 [D loss: 0.136769, acc.: 94.79%] [G loss: 3.562460]\n",
      "18-18000 [D loss: 0.156042, acc.: 93.75%] [G loss: 4.288858]\n",
      "18-19000 [D loss: 0.247639, acc.: 89.58%] [G loss: 4.022080]\n",
      "18-20000 [D loss: 0.180584, acc.: 91.67%] [G loss: 4.535469]\n",
      "18-21000 [D loss: 0.226270, acc.: 92.71%] [G loss: 6.752500]\n",
      "18-22000 [D loss: 0.139174, acc.: 95.83%] [G loss: 5.173078]\n",
      "18-23000 [D loss: 0.177904, acc.: 92.71%] [G loss: 5.190531]\n",
      "18-24000 [D loss: 0.191534, acc.: 95.83%] [G loss: 4.616055]\n",
      "18-25000 [D loss: 0.156251, acc.: 94.79%] [G loss: 4.321847]\n",
      "18-26000 [D loss: 0.098351, acc.: 95.83%] [G loss: 4.963858]\n",
      "18-27000 [D loss: 0.115689, acc.: 96.88%] [G loss: 5.453611]\n",
      "18-28000 [D loss: 0.202456, acc.: 89.58%] [G loss: 3.597639]\n",
      "18-29000 [D loss: 0.148805, acc.: 93.75%] [G loss: 6.690836]\n",
      "18-30000 [D loss: 0.176757, acc.: 90.62%] [G loss: 4.853045]\n",
      "-----------------------------------------------------------------\n",
      "--- 7479.786339044571 seconds ---\n",
      "-----------------------------------------------------------------\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_2 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 533,505\n",
      "Trainable params: 533,505\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_11 (Dense)             (None, 256)               25856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)   (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 784)               803600    \n",
      "_________________________________________________________________\n",
      "reshape_2 (Reshape)          (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 1,493,520\n",
      "Trainable params: 1,489,936\n",
      "Non-trainable params: 3,584\n",
      "_________________________________________________________________\n",
      "25-0 [D loss: 0.483602, acc.: 80.21%] [G loss: 0.682000]\n",
      "25-1000 [D loss: 0.261948, acc.: 90.62%] [G loss: 3.001943]\n",
      "25-2000 [D loss: 0.253042, acc.: 90.62%] [G loss: 2.191667]\n",
      "25-3000 [D loss: 0.216489, acc.: 91.67%] [G loss: 2.974129]\n",
      "25-4000 [D loss: 0.220256, acc.: 89.58%] [G loss: 3.890811]\n",
      "25-5000 [D loss: 0.179828, acc.: 94.79%] [G loss: 2.798988]\n",
      "25-6000 [D loss: 0.267159, acc.: 90.62%] [G loss: 2.846391]\n",
      "25-7000 [D loss: 0.239045, acc.: 91.67%] [G loss: 3.125029]\n",
      "25-8000 [D loss: 0.189282, acc.: 92.71%] [G loss: 2.601986]\n",
      "25-9000 [D loss: 0.292762, acc.: 88.54%] [G loss: 2.711708]\n",
      "25-10000 [D loss: 0.106139, acc.: 98.96%] [G loss: 5.239722]\n",
      "25-11000 [D loss: 0.188146, acc.: 91.67%] [G loss: 4.925107]\n",
      "25-12000 [D loss: 0.232685, acc.: 91.67%] [G loss: 4.781330]\n",
      "25-13000 [D loss: 0.112975, acc.: 94.79%] [G loss: 4.196661]\n",
      "25-14000 [D loss: 0.115937, acc.: 96.88%] [G loss: 3.606856]\n",
      "25-15000 [D loss: 0.128163, acc.: 93.75%] [G loss: 4.813507]\n",
      "25-16000 [D loss: 0.217506, acc.: 95.83%] [G loss: 4.581293]\n",
      "25-17000 [D loss: 0.190196, acc.: 90.62%] [G loss: 6.726527]\n",
      "25-18000 [D loss: 0.212858, acc.: 92.71%] [G loss: 5.069817]\n",
      "25-19000 [D loss: 0.191764, acc.: 91.67%] [G loss: 4.907168]\n",
      "25-20000 [D loss: 0.132637, acc.: 94.79%] [G loss: 6.834015]\n",
      "25-21000 [D loss: 0.104498, acc.: 95.83%] [G loss: 4.798998]\n",
      "25-22000 [D loss: 0.127848, acc.: 95.83%] [G loss: 5.389259]\n",
      "25-23000 [D loss: 0.162981, acc.: 95.83%] [G loss: 5.034925]\n",
      "25-24000 [D loss: 0.161264, acc.: 91.67%] [G loss: 8.930443]\n",
      "25-25000 [D loss: 0.492681, acc.: 83.33%] [G loss: 4.907656]\n",
      "25-26000 [D loss: 0.056077, acc.: 98.96%] [G loss: 4.764312]\n",
      "25-27000 [D loss: 0.154709, acc.: 95.83%] [G loss: 5.131355]\n",
      "25-28000 [D loss: 0.060393, acc.: 98.96%] [G loss: 3.942286]\n",
      "25-29000 [D loss: 0.086691, acc.: 97.92%] [G loss: 3.948760]\n",
      "25-30000 [D loss: 0.142188, acc.: 95.83%] [G loss: 3.857044]\n",
      "-----------------------------------------------------------------\n",
      "--- 16022.948189258575 seconds ---\n",
      "-----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == '__main__':\n",
    "    start_time = time.time()\n",
    "    aksara=[18,25]\n",
    "    aksara_num=[22,24]\n",
    "    for i in range(0,2):\n",
    "        gan = GAN()\n",
    "        gan.train(aksara[i], aksara_num[i], epochs=30001, batch_size=48, sample_interval=1000)\n",
    "        print(\"-----------------------------------------------------------------\")\n",
    "        print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "        print(\"-----------------------------------------------------------------\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Sadrakh\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 533,505\n",
      "Trainable params: 533,505\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 256)               25856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 784)               803600    \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 1,493,520\n",
      "Trainable params: 1,489,936\n",
      "Non-trainable params: 3,584\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From C:\\Users\\Sadrakh\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sadrakh\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26-0 [D loss: 1.122073, acc.: 40.83%] [G loss: 0.868423]\n",
      "26-1000 [D loss: 0.143286, acc.: 96.67%] [G loss: 3.289359]\n",
      "26-2000 [D loss: 0.216810, acc.: 93.33%] [G loss: 2.531711]\n",
      "26-3000 [D loss: 0.273464, acc.: 90.83%] [G loss: 2.181910]\n",
      "26-4000 [D loss: 0.164020, acc.: 95.00%] [G loss: 2.515854]\n",
      "26-5000 [D loss: 0.291590, acc.: 86.67%] [G loss: 2.534489]\n",
      "26-6000 [D loss: 0.287588, acc.: 89.17%] [G loss: 2.754672]\n",
      "26-7000 [D loss: 0.133445, acc.: 95.00%] [G loss: 2.677151]\n",
      "26-8000 [D loss: 0.183777, acc.: 95.00%] [G loss: 3.026753]\n",
      "26-9000 [D loss: 0.126500, acc.: 92.50%] [G loss: 4.036805]\n",
      "26-10000 [D loss: 0.203842, acc.: 90.83%] [G loss: 4.285261]\n",
      "26-11000 [D loss: 0.216017, acc.: 92.50%] [G loss: 3.144452]\n",
      "26-12000 [D loss: 0.229879, acc.: 89.17%] [G loss: 3.129029]\n",
      "26-13000 [D loss: 0.170899, acc.: 94.17%] [G loss: 4.159106]\n",
      "26-14000 [D loss: 0.168630, acc.: 93.33%] [G loss: 4.212809]\n",
      "26-15000 [D loss: 0.169313, acc.: 94.17%] [G loss: 4.976254]\n",
      "26-16000 [D loss: 0.155719, acc.: 92.50%] [G loss: 4.857841]\n",
      "26-17000 [D loss: 0.193680, acc.: 91.67%] [G loss: 4.567191]\n",
      "26-18000 [D loss: 0.189207, acc.: 90.83%] [G loss: 5.110456]\n",
      "26-19000 [D loss: 0.185831, acc.: 91.67%] [G loss: 4.753507]\n",
      "26-20000 [D loss: 0.085865, acc.: 96.67%] [G loss: 5.495763]\n",
      "26-21000 [D loss: 0.230255, acc.: 90.00%] [G loss: 4.137982]\n",
      "26-22000 [D loss: 0.134987, acc.: 95.83%] [G loss: 4.187263]\n",
      "26-23000 [D loss: 0.126677, acc.: 95.83%] [G loss: 5.658555]\n",
      "26-24000 [D loss: 0.170276, acc.: 95.00%] [G loss: 5.056938]\n",
      "26-25000 [D loss: 0.118876, acc.: 95.83%] [G loss: 5.811765]\n",
      "26-26000 [D loss: 0.085014, acc.: 95.00%] [G loss: 7.233548]\n",
      "26-27000 [D loss: 0.156195, acc.: 95.00%] [G loss: 4.621254]\n",
      "26-28000 [D loss: 0.102228, acc.: 95.00%] [G loss: 7.853928]\n",
      "26-29000 [D loss: 0.096816, acc.: 97.50%] [G loss: 5.513256]\n",
      "26-30000 [D loss: 0.122283, acc.: 93.33%] [G loss: 7.089309]\n",
      "-----------------------------------------------------------------\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_2 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 533,505\n",
      "Trainable params: 533,505\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_11 (Dense)             (None, 256)               25856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)   (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 784)               803600    \n",
      "_________________________________________________________________\n",
      "reshape_2 (Reshape)          (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 1,493,520\n",
      "Trainable params: 1,489,936\n",
      "Non-trainable params: 3,584\n",
      "_________________________________________________________________\n",
      "27-0 [D loss: 0.580746, acc.: 85.00%] [G loss: 0.785497]\n",
      "27-1000 [D loss: 0.160833, acc.: 93.33%] [G loss: 3.419156]\n",
      "27-2000 [D loss: 0.303971, acc.: 84.17%] [G loss: 2.453926]\n",
      "27-3000 [D loss: 0.304583, acc.: 87.50%] [G loss: 2.528091]\n",
      "27-4000 [D loss: 0.256374, acc.: 84.17%] [G loss: 2.408790]\n",
      "27-5000 [D loss: 0.221644, acc.: 90.83%] [G loss: 2.809222]\n",
      "27-6000 [D loss: 0.267369, acc.: 85.00%] [G loss: 3.474771]\n",
      "27-7000 [D loss: 0.165622, acc.: 93.33%] [G loss: 3.785080]\n",
      "27-8000 [D loss: 0.269782, acc.: 90.83%] [G loss: 3.403981]\n",
      "27-9000 [D loss: 0.291549, acc.: 82.50%] [G loss: 3.607215]\n",
      "27-10000 [D loss: 0.257759, acc.: 85.00%] [G loss: 4.139077]\n",
      "27-11000 [D loss: 0.206117, acc.: 91.67%] [G loss: 4.590588]\n",
      "27-12000 [D loss: 0.187636, acc.: 90.83%] [G loss: 4.849963]\n",
      "27-13000 [D loss: 0.217383, acc.: 89.17%] [G loss: 4.971386]\n",
      "27-14000 [D loss: 0.168521, acc.: 94.17%] [G loss: 3.598340]\n",
      "27-15000 [D loss: 0.180631, acc.: 92.50%] [G loss: 4.747127]\n",
      "27-16000 [D loss: 0.146489, acc.: 93.33%] [G loss: 6.447818]\n",
      "27-17000 [D loss: 0.234298, acc.: 93.33%] [G loss: 3.647815]\n",
      "27-18000 [D loss: 0.104416, acc.: 95.00%] [G loss: 6.343028]\n",
      "27-19000 [D loss: 0.154020, acc.: 92.50%] [G loss: 6.466568]\n",
      "27-20000 [D loss: 0.087027, acc.: 96.67%] [G loss: 5.939568]\n",
      "27-21000 [D loss: 0.147935, acc.: 94.17%] [G loss: 5.522057]\n",
      "27-22000 [D loss: 0.217094, acc.: 91.67%] [G loss: 5.002701]\n",
      "27-23000 [D loss: 0.119973, acc.: 95.00%] [G loss: 4.883729]\n",
      "27-24000 [D loss: 0.092005, acc.: 95.00%] [G loss: 7.775003]\n",
      "27-25000 [D loss: 0.140783, acc.: 93.33%] [G loss: 4.143484]\n",
      "27-26000 [D loss: 0.058445, acc.: 99.17%] [G loss: 5.496709]\n",
      "27-27000 [D loss: 0.083574, acc.: 97.50%] [G loss: 6.281624]\n",
      "27-28000 [D loss: 0.099371, acc.: 95.00%] [G loss: 5.951762]\n",
      "27-29000 [D loss: 0.105640, acc.: 97.50%] [G loss: 5.373983]\n",
      "27-30000 [D loss: 0.177696, acc.: 95.83%] [G loss: 6.178197]\n",
      "-----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == '__main__':\n",
    "#     %%time\n",
    "    aksara=[26,27]\n",
    "    aksara_num=[22,24]\n",
    "    for i in range(0,2):\n",
    "        gan = GAN()\n",
    "        gan.train(aksara[i], aksara_num[i], epochs=30001, batch_size=60, sample_interval=1000)\n",
    "        print(\"-----------------------------------------------------------------\")\n",
    "#     a = [0,2,4,5,6,7,12,13,18,19,20,25,26,27]\n",
    "#     for i in a:\n",
    "#         gan.train(aksara=i, epochs=30001, batch_size=32, sample_interval=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
