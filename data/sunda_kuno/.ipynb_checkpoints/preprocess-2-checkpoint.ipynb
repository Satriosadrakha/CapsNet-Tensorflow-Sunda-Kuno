{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from PIL import Image\n",
    "# from numpy import asarray\n",
    "# import numpy as np\n",
    "\n",
    "# # load the image\n",
    "# image = Image.open('HA_3.png')\n",
    "# # convert image to numpy array\n",
    "# data = asarray(image)\n",
    "# print(type(data))\n",
    "# # summarize shape\n",
    "# print(data.shape)\n",
    "\n",
    "# # create Pillow image\n",
    "# image2 = Image.fromarray(data)\n",
    "# print(type(image2))\n",
    "\n",
    "# # summarize image details\n",
    "# print(image2.mode)\n",
    "# print(image2.size)\n",
    "\n",
    "# im = np.array(Image.open('HA_3.png').convert('L').resize((30,30))) #you can pass multiple arguments in single line\n",
    "# print(type(im))\n",
    "\n",
    "# Image.fromarray(im).save('HA_3_transformed.png')\n",
    "\n",
    "# print(im)\n",
    "\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "from PIL import Image, ImageFilter, ImageEnhance\n",
    "import PIL.ImageOps\n",
    "import os\n",
    "import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def resize_only(file_name):\n",
    "\timg = Image.open(file_name).convert('LA')\n",
    "\t# img = img.filter(ImageFilter.SMOOTH)\n",
    "\t# img = img.filter(ImageFilter.SHARPEN)\n",
    "\timg = img.resize((28, 28))\n",
    "\t# img = PIL.ImageOps.invert(img)\n",
    "\timg.save(\"temp.png\")\n",
    "\timg = cv.imread('temp.png', 0)\n",
    "\timg = cv.bitwise_not(img)\n",
    "\timg =  Image.fromarray(img)\n",
    "\t\n",
    "\treturn img\n",
    "\n",
    "def resize(file_name, invert):\n",
    "\t# image = cv.imread('%s' % file_name,0)\n",
    "\timg = Image.open(file_name)\n",
    "\timg.save(\"temp1.png\")\n",
    "\timg = Image.open(file_name).convert('LA')\n",
    "# \tif invert==True:\n",
    "# \t\timg = PIL.ImageOps.invert(img)\n",
    "\tenhancer = ImageEnhance.Contrast(img)\n",
    "\t\n",
    "\tfactor = 2.0\n",
    "\timg = img.resize((28, 28))\n",
    "\timg.save(\"temp2.png\")\n",
    "\timg = enhancer.enhance(factor)\n",
    "\timg.save(\"temp3.png\")\n",
    "\timg = img.filter(ImageFilter.SMOOTH)\n",
    "\timg = img.filter(ImageFilter.SHARPEN)\n",
    "\t# print(type(img))    \n",
    "\timg.save(\"temp4.png\")\n",
    "\n",
    "\timg.save(\"temp.png\")\n",
    "    \n",
    "\timg = cv.imread('temp.png', 0)\n",
    "    \n",
    "\tif invert==True:\n",
    "\t\timg = cv.bitwise_not(img)\n",
    "\tret,th = cv.threshold(img,0,255,cv.THRESH_BINARY+cv.THRESH_OTSU)\n",
    "    \n",
    "\tblur = cv.GaussianBlur(img,(5,5),0)\n",
    "\tret2,th2 = cv.threshold(blur,0,255,cv.THRESH_BINARY+cv.THRESH_OTSU)\n",
    "    \n",
    "\tmask = np.where(th ==  0, th2, 255)\n",
    "\tmaskimg = Image.fromarray(mask)\n",
    "\tmaskimg.save(\"temp5.png\")\n",
    "\timg = np.where(mask == 0, img, 255)\n",
    "\n",
    "#     create a CLAHE object (Arguments are optional)\n",
    "\tclahe = cv.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "\timg = clahe.apply(img)\n",
    "\n",
    "\t_,img_remove = cv.threshold(img,95,255,cv.THRESH_TOZERO)\n",
    "\timg_remove = cv.bitwise_not(img_remove)\n",
    "\t_,img = cv.threshold(img_remove,95,255,cv.THRESH_TOZERO)\n",
    "\n",
    "# \timg = cv.bitwise_not(img)\n",
    "    \n",
    "\timg = Image.fromarray(img)\n",
    "  \n",
    "\treturn img\n",
    "\t# img.save(\"resized_\"+file_name)\n",
    "\n",
    "def preprocess():\n",
    "\t# crop(str(sys.argv))\n",
    "\tscript_dir = os.path.abspath('')\n",
    "\ti=[\"A\",\"BA\",\"CA\",\"DA\",\"GA\",\"HA\",\"I\",\"JA\",\"KA\",\"LA\",\"MA\",\"NA\",\"NGA\",\"NYA\",\"PA\",\"PANELENG\",\"PANEULEUNG\",\"PANGHULU\",\"PANGLAYAR\",\"PANOLONG\",\"PANYUKU\",\"PATEN\",\"RA\",\"SA\",\"TA\",\"U\",\"WA\",\"YA\"]\n",
    "\tj=[30,47,19,67,37,27,16,21,60,60,56,120,25,14,61,42,63,60,23,35,36,84,56,90,78,18,24,22]\n",
    "\tk=[12,30,7,45,16,11,7,9,40,25,24,80,10,6,39,28,42,40,10,24,24,36,36,60,52,7,10,9]\n",
    "\tganed = [0,2,4,5,6,7,12,13,18,19,20,25,26,27]\n",
    "\n",
    "\tfinal_np = np.array([])\n",
    "\tfinal_label = np.array([])\n",
    "\n",
    "    #   train_image to ready_to_train\n",
    "\tfor x in range(0, len(i), 1):\n",
    "\t\tsumImg = 0\n",
    "\t\ttrain_directory_path = os.path.join(script_dir,\"train_image\")\n",
    "\t\ttrain_count_image = len(glob.glob1(train_directory_path,\"%s_*.png\" % (i[x])))\n",
    "\n",
    "\t\tfor y in range(1, train_count_image+1):\n",
    "\t\t\trel_path = \"train_image/%s_%s.png\" % (i[x],str(y))\n",
    "\t\t\tabs_file_path = os.path.join(script_dir, rel_path)\n",
    "            \n",
    "\t\t\timg = resize(abs_file_path, invert=False)\n",
    "\t\t\timg.save(\"train-test_image/%s_%s.png\" % (i[x],str(y)))\n",
    "\t\t\t# img = resize(abs_file_path, invert=False)\n",
    "\t\t\t# img.save(\"ready_to_train_3/%s_%s.png\" % (i[x],str(y)))\n",
    "\t\tsumImg = sumImg + train_count_image\n",
    "\n",
    "\t\ttest_directory_path = os.path.join(script_dir,\"test_image/%s\" % i[x])\n",
    "\t\ttest_count_image = len(glob.glob1(test_directory_path,\"%s_*.png\" % (i[x])))\n",
    "\n",
    "\t\tfor y in range(1, test_count_image+1):\n",
    "\t\t\trel_path = os.path.join(test_directory_path,\"%s_%s.png\" % (i[x],str(y)))\n",
    "\t\t\tabs_file_path = os.path.join(script_dir, rel_path)\n",
    "            \n",
    "\t\t\timg = resize(abs_file_path, invert=False)\n",
    "\t\t\timg.save(\"train-test_image/%s_%s.png\" % (i[x],str(y+sumImg)))\n",
    "\t\tsumImg = sumImg + test_count_image\n",
    "\n",
    "# \t\tif x == 0:\n",
    "# \t\t\tfor y in range(1, sumImg+1):\n",
    "# \t\t\t\trel_path = os.path.join(\"train-test_image\",\"%s_%s.png\" % (i[x],str(y)))\n",
    "# \t\t\t\tabs_file_path = os.path.join(script_dir, rel_path)\n",
    "\n",
    "# \t\t\t\timg = Image.open(abs_file_path)\n",
    "# \t\t\t\timg = np.array(img)\n",
    "# \t\t\t\timg = img[:, :]\n",
    "# \t\t\t\tfinal_np = np.append(final_np,img)\n",
    "# \t\t\t\tfinal_label = np.append(final_label,x)\n",
    "# \t\t\tfinal_np = final_np.reshape((sumImg, 28, 28, 1)).astype(np.float32)\n",
    "# \t\t\tfinal_label = final_label.reshape((sumImg)).astype(np.int32)\n",
    "# \t\t\tdata_train, data_test, labels_train, labels_test = train_test_split(final_np, final_label, test_size=0.30, random_state=42)\n",
    "# \t\t\tprint(final_label)\n",
    "# \t\t\tprint(data_train.shape)\n",
    "# \t\t\tprint(data_test.shape)\n",
    "# \t\t\tprint(labels_train)\n",
    "# \t\t\tprint(labels_test)\n",
    "            \n",
    "# #   GAN_image to ready_to_train\n",
    "# \tfor x in ganed:\n",
    "# \t\tfor y in range(1, 26):\n",
    "# \t\t\trel_path = \"GAN_generated_images/%s_%s.png\" % (i[x],str(y+j[x]))\n",
    "# # \t\t\trel_path = \"ready_to_train/%s_%s.png\" % (i[x],str(y))\n",
    "# \t\t\tabs_file_path = os.path.join(script_dir, rel_path)\n",
    "            \n",
    "# \t\t\timg = resize(abs_file_path, invert=True)\n",
    "# \t\t\timg.save(\"ready_to_train_3/%s_%s.png\" % (i[x],str(y+j[x])))\n",
    "\n",
    "# #   test_image to ready_to_test (validation)\n",
    "# \tsumImg = 0\n",
    "# \tfinal_np = np.array([])\n",
    "# \tfinal_label = np.array([])\n",
    "# \tfor x in range(0, len(i), 1):\n",
    "# \t\tsumImg = sumImg + k[x]\n",
    "# \t\tfor y in range(1, int((k[x])/2)+1):\n",
    "# \t\t\trel_path = \"test_image/%s/%s_%s.png\" % (i[x],i[x],str(y))\n",
    "# \t\t\t# rel_path = \"ready_to_train/%s_%s.png\" % (i[x],str(y))\n",
    "# \t\t\tabs_file_path = os.path.join(script_dir, rel_path)\n",
    "\n",
    "# \t\t\t# img = resize(abs_file_path, invert=False)\n",
    "# \t\t\t# img.save(\"ready_to_test_2/%s_%s.png\" % (i[x],str(y)))\n",
    "# \t\t\timg = resize_only(abs_file_path)\n",
    "# \t\t\timg.save(\"ready_to_test/%s_%s.png\" % (i[x],str(y)))\n",
    "\n",
    "# # \ttest_image to ready_to_test (test)\n",
    "# \tsumImg = 0\n",
    "# \tfor x in range(0, len(i), 1):\n",
    "# \t\tsumImg = sumImg - (-k[x]//2)\n",
    "# \t\tfor y in range((-(-k[x])//2)+1, k[x]+1):\n",
    "# #             rel_path = \"test_image/%s/%s_%s.png\" % (i[x],i[x],str(y))\n",
    "# \t\t\trel_path = \"test_image/%s/%s_%s.png\" % (i[x],i[x],str(y))\n",
    "# \t\t\t# rel_path = \"ready_to_train/%s_%s.png\" % (i[x],str(y))\n",
    "# \t\t\tabs_file_path = os.path.join(script_dir, rel_path)\n",
    "            \n",
    "# \t\t\timg = Image.open(abs_file_path)\n",
    "# \t\t\timg = np.array(img)\n",
    "# \t\t\timg = img[:, :, 0]\n",
    "# \t\t\timg = resize_only(abs_file_path)\n",
    "# \t\t\timg.save(\"ready_to_test/%s_%s.png\" % (i[x],str(y)))\n",
    "# \t\t\t# img = resize(abs_file_path, invert=False)\n",
    "# \t\t\t# img.save(\"ready_to_test_2/%s_%s.png\" % (i[x],str(y)))\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "# \tmain()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'save'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-d47c6e61390d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"train_image/A_1.png\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-1-58939ad8883e>\u001b[0m in \u001b[0;36mresize\u001b[1;34m(file_name, invert)\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m         \u001b[0mmask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mth\u001b[0m \u001b[1;33m==\u001b[0m  \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mth2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m255\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 80\u001b[1;33m         \u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"temp5.png\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     81\u001b[0m         \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmask\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m255\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'save'"
     ]
    }
   ],
   "source": [
    "resize(\"train_image/A_1.png\", False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sunda(aksara):\n",
    "    script_dir = os.path.abspath('')\n",
    "    i=[\"A\",\"BA\",\"CA\",\"DA\",\"GA\",\"HA\",\"I\",\"JA\",\"KA\",\"LA\",\"MA\",\"NA\",\"NGA\",\"NYA\",\"PA\",\"PANELENG\",\"PANEULEUNG\",\"PANGHULU\",\"PANGLAYAR\",\"PANOLONG\",\"PANYUKU\",\"PATEN\",\"RA\",\"SA\",\"TA\",\"U\",\"WA\",\"YA\"]\n",
    "    j=[30,47,19,67,37,27,16,21,60,60,56,120,25,14,61,42,63,60,23,35,36,84,56,90,78,18,24,22]\n",
    "    k=[12,30,7,45,16,11,7,9,40,25,24,80,10,6,39,28,42,40,10,24,24,36,36,60,52,7,10,9]\n",
    "    ganed = [0,2,4,5,6,7,12,13,18,19,20,25,26,27]\n",
    "\n",
    "    final_np = np.array([])\n",
    "    final_label = np.array([])\n",
    "\n",
    "    #   train_image to ready_to_train\n",
    "#for x in range(0, len(i), 1):\n",
    "    sumImg = 0\n",
    "    directory_path = os.path.join(script_dir,\"train-test_image\")\n",
    "    count_image = len(glob.glob1(directory_path,\"%s_*.png\" % (i[aksara])))\n",
    "\n",
    "    for y in range(1, count_image+1):\n",
    "        abs_file_path = os.path.join(directory_path, \"%s_%s.png\" % (i[aksara],str(y)))\n",
    "        img = Image.open(abs_file_path)\n",
    "        img = np.array(img)\n",
    "        img = img[:, :]\n",
    "        final_np = np.append(final_np,img)\n",
    "        final_label = np.append(final_label,aksara)\n",
    "    sumImg = sumImg + count_image\n",
    "    final_np = final_np.reshape((sumImg, 28, 28, 1)).astype(np.float32)\n",
    "    final_label = final_label.reshape((sumImg)).astype(np.int32)\n",
    "    data_train, data_test, labels_train, labels_test = train_test_split(final_np, final_label, test_size=0.30, random_state=42)\n",
    "    return(data_train,sumImg)\n",
    "#     print(final_label)\n",
    "#     print(data_train.shape)\n",
    "#     print(data_test.shape)\n",
    "#     print(labels_train)\n",
    "#     print(labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Sadrakh\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 533,505\n",
      "Trainable params: 533,505\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 256)               25856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 784)               803600    \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 1,493,520\n",
      "Trainable params: 1,489,936\n",
      "Non-trainable params: 3,584\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From C:\\Users\\Sadrakh\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sadrakh\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-0 [D loss: 0.852320, acc.: 11.46%] [G loss: 0.616074]\n",
      "1-1000 [D loss: 0.089144, acc.: 100.00%] [G loss: 4.656466]\n",
      "1-2000 [D loss: 0.089971, acc.: 96.88%] [G loss: 5.121223]\n",
      "1-3000 [D loss: 0.068486, acc.: 97.92%] [G loss: 4.272728]\n",
      "1-4000 [D loss: 0.083238, acc.: 97.92%] [G loss: 4.106057]\n",
      "1-5000 [D loss: 0.108003, acc.: 96.88%] [G loss: 3.564067]\n",
      "1-6000 [D loss: 0.217159, acc.: 92.71%] [G loss: 2.951691]\n",
      "1-7000 [D loss: 0.153278, acc.: 94.79%] [G loss: 3.467499]\n",
      "1-8000 [D loss: 0.217697, acc.: 93.75%] [G loss: 3.454765]\n",
      "1-9000 [D loss: 0.087497, acc.: 97.92%] [G loss: 3.424032]\n",
      "1-10000 [D loss: 0.112284, acc.: 95.83%] [G loss: 3.094842]\n",
      "1-11000 [D loss: 0.114176, acc.: 96.88%] [G loss: 3.552041]\n",
      "1-12000 [D loss: 0.132235, acc.: 95.83%] [G loss: 4.350391]\n",
      "1-13000 [D loss: 0.064859, acc.: 98.96%] [G loss: 3.912926]\n",
      "1-14000 [D loss: 0.056827, acc.: 97.92%] [G loss: 4.604816]\n",
      "1-15000 [D loss: 0.125841, acc.: 95.83%] [G loss: 3.832892]\n",
      "1-16000 [D loss: 0.128565, acc.: 95.83%] [G loss: 4.362437]\n",
      "1-17000 [D loss: 0.080756, acc.: 96.88%] [G loss: 5.078261]\n",
      "1-18000 [D loss: 0.105200, acc.: 96.88%] [G loss: 4.934318]\n",
      "1-19000 [D loss: 0.098331, acc.: 96.88%] [G loss: 4.317240]\n",
      "1-20000 [D loss: 0.148848, acc.: 94.79%] [G loss: 4.179473]\n",
      "1-21000 [D loss: 0.189162, acc.: 93.75%] [G loss: 4.190558]\n",
      "1-22000 [D loss: 0.206161, acc.: 88.54%] [G loss: 5.049426]\n",
      "1-23000 [D loss: 0.221900, acc.: 91.67%] [G loss: 5.682508]\n",
      "1-24000 [D loss: 0.047659, acc.: 98.96%] [G loss: 4.855824]\n",
      "1-25000 [D loss: 0.084335, acc.: 96.88%] [G loss: 5.167835]\n",
      "1-26000 [D loss: 0.218340, acc.: 90.62%] [G loss: 5.069399]\n",
      "1-27000 [D loss: 0.139911, acc.: 95.83%] [G loss: 5.350786]\n",
      "1-28000 [D loss: 0.017363, acc.: 98.96%] [G loss: 5.169977]\n",
      "1-29000 [D loss: 0.115139, acc.: 93.75%] [G loss: 7.642596]\n",
      "1-30000 [D loss: 0.097992, acc.: 97.92%] [G loss: 5.554778]\n",
      "-----------------------------------------------------------------\n",
      "--- 6261.347868204117 seconds ---\n",
      "-----------------------------------------------------------------\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_2 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 533,505\n",
      "Trainable params: 533,505\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_11 (Dense)             (None, 256)               25856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)   (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 784)               803600    \n",
      "_________________________________________________________________\n",
      "reshape_2 (Reshape)          (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 1,493,520\n",
      "Trainable params: 1,489,936\n",
      "Non-trainable params: 3,584\n",
      "_________________________________________________________________\n",
      "2-0 [D loss: 0.785934, acc.: 47.92%] [G loss: 0.741225]\n",
      "2-1000 [D loss: 0.375729, acc.: 82.29%] [G loss: 2.188640]\n",
      "2-2000 [D loss: 0.242191, acc.: 89.58%] [G loss: 2.532154]\n",
      "2-3000 [D loss: 0.295144, acc.: 88.54%] [G loss: 3.979763]\n",
      "2-4000 [D loss: 0.123548, acc.: 93.75%] [G loss: 3.830590]\n",
      "2-5000 [D loss: 0.189678, acc.: 97.92%] [G loss: 3.852854]\n",
      "2-6000 [D loss: 0.210643, acc.: 91.67%] [G loss: 4.573496]\n",
      "2-7000 [D loss: 0.189588, acc.: 94.79%] [G loss: 4.785294]\n",
      "2-8000 [D loss: 0.319661, acc.: 89.58%] [G loss: 4.702647]\n",
      "2-9000 [D loss: 0.086989, acc.: 95.83%] [G loss: 8.123241]\n",
      "2-10000 [D loss: 0.067687, acc.: 97.92%] [G loss: 5.751356]\n",
      "2-11000 [D loss: 0.175053, acc.: 92.71%] [G loss: 5.304033]\n",
      "2-12000 [D loss: 0.341364, acc.: 85.42%] [G loss: 4.266679]\n",
      "2-13000 [D loss: 0.254722, acc.: 86.46%] [G loss: 6.016054]\n",
      "2-14000 [D loss: 0.328824, acc.: 90.62%] [G loss: 4.878512]\n",
      "2-15000 [D loss: 0.109081, acc.: 97.92%] [G loss: 6.650963]\n",
      "2-16000 [D loss: 0.050295, acc.: 98.96%] [G loss: 7.693747]\n",
      "2-17000 [D loss: 0.089588, acc.: 96.88%] [G loss: 5.736901]\n",
      "2-18000 [D loss: 0.124191, acc.: 96.88%] [G loss: 5.640205]\n",
      "2-19000 [D loss: 0.253758, acc.: 92.71%] [G loss: 4.695673]\n",
      "2-20000 [D loss: 0.103556, acc.: 98.96%] [G loss: 4.664003]\n",
      "2-21000 [D loss: 0.203959, acc.: 92.71%] [G loss: 6.596350]\n",
      "2-22000 [D loss: 0.092719, acc.: 96.88%] [G loss: 4.540832]\n",
      "2-23000 [D loss: 0.183729, acc.: 93.75%] [G loss: 4.534224]\n",
      "2-24000 [D loss: 0.074347, acc.: 98.96%] [G loss: 4.860000]\n",
      "2-25000 [D loss: 0.187259, acc.: 95.83%] [G loss: 6.445976]\n",
      "2-26000 [D loss: 0.188000, acc.: 94.79%] [G loss: 4.441884]\n",
      "2-27000 [D loss: 0.170469, acc.: 94.79%] [G loss: 4.998801]\n",
      "2-28000 [D loss: 0.084362, acc.: 97.92%] [G loss: 5.516206]\n",
      "2-29000 [D loss: 0.197016, acc.: 94.79%] [G loss: 3.346545]\n",
      "2-30000 [D loss: 0.192477, acc.: 94.79%] [G loss: 3.704939]\n",
      "-----------------------------------------------------------------\n",
      "--- 12852.59457540512 seconds ---\n",
      "-----------------------------------------------------------------\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_3 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_12 (LeakyReLU)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 533,505\n",
      "Trainable params: 533,505\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_18 (Dense)             (None, 256)               25856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_13 (LeakyReLU)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_14 (LeakyReLU)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_15 (LeakyReLU)   (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 784)               803600    \n",
      "_________________________________________________________________\n",
      "reshape_3 (Reshape)          (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 1,493,520\n",
      "Trainable params: 1,489,936\n",
      "Non-trainable params: 3,584\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-5c1b26ac2690>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    182\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    183\u001b[0m         \u001b[0mgan\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGAN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 184\u001b[1;33m         \u001b[0mgan\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maksara\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maksara_num\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30001\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m48\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_interval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    185\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"-----------------------------------------------------------------\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"--- %s seconds ---\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "import os\n",
    "import scipy\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "\n",
    "class GAN():\n",
    "    def __init__(self):\n",
    "        self.img_rows = 28\n",
    "        self.img_cols = 28\n",
    "        self.channels = 1\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "        self.latent_dim = 100\n",
    "\n",
    "        optimizer = Adam(0.0002, 0.5)\n",
    "\n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(loss='binary_crossentropy',\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "        # Build the generator\n",
    "        self.generator = self.build_generator()\n",
    "\n",
    "        # The generator takes noise as input and generates imgs\n",
    "        z = Input(shape=(self.latent_dim,))\n",
    "        img = self.generator(z)\n",
    "\n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "\n",
    "        # The discriminator takes generated images as input and determines validity\n",
    "        validity = self.discriminator(img)\n",
    "\n",
    "        # The combined model  (stacked generator and discriminator)\n",
    "        # Trains the generator to fool the discriminator\n",
    "        self.combined = Model(z, validity)\n",
    "        self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "\n",
    "    def build_generator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Dense(256, input_dim=self.latent_dim))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(512))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(1024))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(np.prod(self.img_shape), activation='tanh'))\n",
    "        model.add(Reshape(self.img_shape))\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        noise = Input(shape=(self.latent_dim,))\n",
    "        img = model(noise)\n",
    "\n",
    "        return Model(noise, img)\n",
    "\n",
    "    def build_discriminator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Flatten(input_shape=self.img_shape))\n",
    "        model.add(Dense(512))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dense(256))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        model.summary()\n",
    "\n",
    "        img = Input(shape=self.img_shape)\n",
    "        validity = model(img)\n",
    "\n",
    "        return Model(img, validity)\n",
    "\n",
    "    def train(self, aksara, aksara_num, epochs, batch_size=128, sample_interval=50):\n",
    "\n",
    "        # Load the dataset\n",
    "#         preprocess()\n",
    "        X_train, sumTrain= get_sunda(aksara)\n",
    "        X_train = X_train[:,:,:,0]\n",
    "#         (X_train, _), (_, _) = mnist.load_data()\n",
    "\n",
    "        # Rescale -1 to 1\n",
    "        X_train = X_train / 127.5 - 1.\n",
    "        X_train = np.expand_dims(X_train, axis=3)\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = np.ones((batch_size, 1))\n",
    "        fake = np.zeros((batch_size, 1))\n",
    "        \n",
    "        min_loss = 100\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "\n",
    "            # Select a random batch of images\n",
    "            idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "            imgs = X_train[idx]\n",
    "\n",
    "            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "\n",
    "            # Generate a batch of new images\n",
    "            gen_imgs = self.generator.predict(noise)\n",
    "\n",
    "            # Train the discriminator\n",
    "            d_loss_real = self.discriminator.train_on_batch(imgs, valid)\n",
    "            d_loss_fake = self.discriminator.train_on_batch(gen_imgs, fake)\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Generator\n",
    "            # ---------------------\n",
    "\n",
    "            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "\n",
    "            # Train the generator (to have the discriminator label samples as valid)\n",
    "            g_loss = self.combined.train_on_batch(noise, valid)\n",
    "\n",
    "            # Plot the progress\n",
    "#             print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
    "\n",
    "            # If at save interval => save generated image samples\n",
    "            if epoch % sample_interval == 0:\n",
    "                print (\"%d-%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (aksara, epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
    "                self.sample_images(epoch, aksara, sumTrain, aksara_num)\n",
    "            \n",
    "#             if d_loss[0] + g_loss < min_loss and epoch > 10000:\n",
    "#                 min_loss = d_loss[0] + g_loss\n",
    "#                 self.sample_images(epoch, aksara, sumTrain, aksara_num)\n",
    "\n",
    "    def sample_images(self, epoch, aksara, sumTrain, aksara_num):\n",
    "        aksaraList=[\"A\",\"BA\",\"CA\",\"DA\",\"GA\",\"HA\",\"I\",\"JA\",\"KA\",\"LA\",\"MA\",\"NA\",\"NGA\",\"NYA\",\"PA\",\"PANELENG\",\"PANEULEUNG\",\"PANGHULU\",\"PANGLAYAR\",\"PANOLONG\",\"PANYUKU\",\"PATEN\",\"RA\",\"SA\",\"TA\",\"U\",\"WA\",\"YA\"]\n",
    "        r, c = 10, 10\n",
    "        noise = np.random.normal(0, 1, (r * c, self.latent_dim))\n",
    "        gen_imgs = self.generator.predict(noise)\n",
    "\n",
    "        # Rescale images 0 - 1\n",
    "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "        for i in range(0,aksara_num):\n",
    "            img = gen_imgs[i,:,:,0]\n",
    "            normalizedImg = np.zeros((28,28))\n",
    "            normalizedImg = cv.normalize(img, normalizedImg, 0, 255, cv.NORM_MINMAX)\n",
    "            cv.imwrite(\"GAN_generated_images/%s_%s.png\" % (aksaraList[aksara],str(sumTrain+i+1)), normalizedImg)\n",
    "        for i in range(0,10):\n",
    "            img = gen_imgs[i,:,:,0]\n",
    "            normalizedImg = np.zeros((28,28))\n",
    "            normalizedImg = cv.normalize(img, normalizedImg, 0, 255, cv.NORM_MINMAX)\n",
    "            cv.imwrite(\"GAN_generated_images/%s_Cadangan_%s.png\" % (aksaraList[aksara],str(i+1)), normalizedImg)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    start_time = time.time()\n",
    "    aksara=[1,2]\n",
    "    aksara_num=[24,24]\n",
    "    for i in range(0,3):\n",
    "        gan = GAN()\n",
    "        gan.train(aksara[i], aksara_num[i], epochs=30001, batch_size=48, sample_interval=1000)\n",
    "        print(\"-----------------------------------------------------------------\")\n",
    "        print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "        print(\"-----------------------------------------------------------------\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\adjie\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 533,505\n",
      "Trainable params: 533,505\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 256)               25856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 784)               803600    \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 1,493,520\n",
      "Trainable params: 1,489,936\n",
      "Non-trainable params: 3,584\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From C:\\Users\\adjie\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adjie\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2-0 [D loss: 0.709409, acc.: 56.25%] [G loss: 0.949301]\n",
      "2-500 [D loss: 0.215756, acc.: 95.83%] [G loss: 4.677984]\n",
      "2-1000 [D loss: 0.509335, acc.: 67.71%] [G loss: 2.536737]\n",
      "2-1500 [D loss: 0.377938, acc.: 88.54%] [G loss: 4.425233]\n",
      "2-2000 [D loss: 0.256330, acc.: 92.71%] [G loss: 3.002691]\n",
      "2-2500 [D loss: 0.342475, acc.: 91.67%] [G loss: 2.451769]\n",
      "2-3000 [D loss: 0.192805, acc.: 94.79%] [G loss: 3.565622]\n",
      "2-3500 [D loss: 0.483268, acc.: 80.21%] [G loss: 3.228659]\n",
      "2-4000 [D loss: 0.323413, acc.: 88.54%] [G loss: 2.721824]\n",
      "2-4500 [D loss: 0.099963, acc.: 98.96%] [G loss: 3.551860]\n",
      "2-5000 [D loss: 0.150127, acc.: 94.79%] [G loss: 2.973430]\n",
      "2-5500 [D loss: 0.280885, acc.: 88.54%] [G loss: 3.068622]\n",
      "2-6000 [D loss: 0.226558, acc.: 90.62%] [G loss: 2.634293]\n",
      "2-6500 [D loss: 0.246897, acc.: 88.54%] [G loss: 3.175595]\n",
      "2-7000 [D loss: 0.318792, acc.: 85.42%] [G loss: 2.419063]\n",
      "2-7500 [D loss: 0.300048, acc.: 88.54%] [G loss: 2.658532]\n",
      "2-8000 [D loss: 0.298076, acc.: 84.38%] [G loss: 3.140390]\n",
      "2-8500 [D loss: 0.217151, acc.: 90.62%] [G loss: 2.900365]\n",
      "2-9000 [D loss: 0.282392, acc.: 84.38%] [G loss: 3.164806]\n",
      "2-9500 [D loss: 0.210474, acc.: 93.75%] [G loss: 3.165008]\n",
      "2-10000 [D loss: 0.211643, acc.: 90.62%] [G loss: 3.165854]\n",
      "2-10500 [D loss: 0.196183, acc.: 90.62%] [G loss: 3.000533]\n",
      "2-11000 [D loss: 0.198357, acc.: 95.83%] [G loss: 3.055608]\n",
      "2-11500 [D loss: 0.276704, acc.: 86.46%] [G loss: 3.447360]\n",
      "2-12000 [D loss: 0.242921, acc.: 91.67%] [G loss: 4.356338]\n",
      "2-12500 [D loss: 0.192568, acc.: 93.75%] [G loss: 3.524703]\n",
      "2-13000 [D loss: 0.399438, acc.: 83.33%] [G loss: 4.514412]\n",
      "2-13500 [D loss: 0.324087, acc.: 86.46%] [G loss: 2.839113]\n",
      "2-14000 [D loss: 0.330330, acc.: 78.12%] [G loss: 4.314804]\n",
      "2-14500 [D loss: 0.157399, acc.: 90.62%] [G loss: 3.852750]\n",
      "2-15000 [D loss: 0.115863, acc.: 95.83%] [G loss: 3.750518]\n",
      "2-15500 [D loss: 0.161502, acc.: 91.67%] [G loss: 4.376306]\n",
      "2-16000 [D loss: 0.104312, acc.: 97.92%] [G loss: 3.572767]\n",
      "2-16500 [D loss: 0.288971, acc.: 87.50%] [G loss: 4.310664]\n",
      "2-17000 [D loss: 0.261631, acc.: 87.50%] [G loss: 3.265880]\n",
      "2-17500 [D loss: 0.247088, acc.: 87.50%] [G loss: 4.760852]\n",
      "2-18000 [D loss: 0.232527, acc.: 85.42%] [G loss: 4.156862]\n",
      "2-18500 [D loss: 0.332972, acc.: 76.04%] [G loss: 3.395306]\n",
      "2-19000 [D loss: 0.256689, acc.: 93.75%] [G loss: 4.164172]\n",
      "2-19500 [D loss: 0.097090, acc.: 97.92%] [G loss: 4.562622]\n",
      "2-20000 [D loss: 0.183595, acc.: 89.58%] [G loss: 4.794284]\n",
      "-----------------------------------------------------------------\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_2 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 533,505\n",
      "Trainable params: 533,505\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_11 (Dense)             (None, 256)               25856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)   (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 784)               803600    \n",
      "_________________________________________________________________\n",
      "reshape_2 (Reshape)          (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 1,493,520\n",
      "Trainable params: 1,489,936\n",
      "Non-trainable params: 3,584\n",
      "_________________________________________________________________\n",
      "4-0 [D loss: 0.877671, acc.: 5.21%] [G loss: 0.747325]\n",
      "4-500 [D loss: 0.254574, acc.: 87.50%] [G loss: 7.674582]\n",
      "4-1000 [D loss: 0.223897, acc.: 100.00%] [G loss: 4.285599]\n",
      "4-1500 [D loss: 0.484932, acc.: 76.04%] [G loss: 2.236989]\n",
      "4-2000 [D loss: 0.258796, acc.: 96.88%] [G loss: 3.452235]\n",
      "4-2500 [D loss: 0.286162, acc.: 89.58%] [G loss: 2.678693]\n",
      "4-3000 [D loss: 0.170081, acc.: 96.88%] [G loss: 3.627504]\n",
      "4-3500 [D loss: 0.374677, acc.: 90.62%] [G loss: 2.238644]\n",
      "4-4000 [D loss: 0.339351, acc.: 80.21%] [G loss: 3.641980]\n",
      "4-4500 [D loss: 0.309996, acc.: 87.50%] [G loss: 2.557737]\n",
      "4-5000 [D loss: 0.275251, acc.: 92.71%] [G loss: 3.418974]\n",
      "4-5500 [D loss: 0.435773, acc.: 78.12%] [G loss: 2.830631]\n",
      "4-6000 [D loss: 0.112395, acc.: 96.88%] [G loss: 4.350589]\n",
      "4-6500 [D loss: 0.141502, acc.: 95.83%] [G loss: 3.972187]\n",
      "4-7000 [D loss: 0.160317, acc.: 96.88%] [G loss: 5.271272]\n",
      "4-7500 [D loss: 0.308721, acc.: 90.62%] [G loss: 4.223567]\n",
      "4-8000 [D loss: 0.129801, acc.: 96.88%] [G loss: 3.999969]\n",
      "4-8500 [D loss: 0.130237, acc.: 94.79%] [G loss: 3.431036]\n",
      "4-9000 [D loss: 0.274175, acc.: 86.46%] [G loss: 3.499289]\n",
      "4-9500 [D loss: 0.106248, acc.: 97.92%] [G loss: 3.835930]\n",
      "4-10000 [D loss: 0.183746, acc.: 94.79%] [G loss: 3.519622]\n",
      "4-10500 [D loss: 0.060360, acc.: 98.96%] [G loss: 4.593396]\n",
      "4-11000 [D loss: 0.195877, acc.: 92.71%] [G loss: 3.193195]\n",
      "4-11500 [D loss: 0.112677, acc.: 95.83%] [G loss: 5.391258]\n",
      "4-12000 [D loss: 0.202946, acc.: 91.67%] [G loss: 3.693645]\n",
      "4-12500 [D loss: 0.291704, acc.: 92.71%] [G loss: 3.528963]\n",
      "4-13000 [D loss: 0.188187, acc.: 93.75%] [G loss: 3.646805]\n",
      "4-13500 [D loss: 0.176344, acc.: 93.75%] [G loss: 3.572626]\n",
      "4-14000 [D loss: 0.125295, acc.: 95.83%] [G loss: 3.328440]\n",
      "4-14500 [D loss: 0.272894, acc.: 86.46%] [G loss: 3.361851]\n",
      "4-15000 [D loss: 0.299009, acc.: 89.58%] [G loss: 2.935214]\n",
      "4-15500 [D loss: 0.131858, acc.: 96.88%] [G loss: 4.257495]\n",
      "4-16000 [D loss: 0.222513, acc.: 92.71%] [G loss: 4.492248]\n",
      "4-16500 [D loss: 0.199464, acc.: 93.75%] [G loss: 3.866958]\n",
      "4-17000 [D loss: 0.229306, acc.: 91.67%] [G loss: 3.468334]\n",
      "4-17500 [D loss: 0.224934, acc.: 91.67%] [G loss: 3.357421]\n",
      "4-18000 [D loss: 0.115985, acc.: 96.88%] [G loss: 5.582680]\n",
      "4-18500 [D loss: 0.224883, acc.: 89.58%] [G loss: 4.673522]\n",
      "4-19000 [D loss: 0.256070, acc.: 87.50%] [G loss: 4.281636]\n",
      "4-19500 [D loss: 0.101788, acc.: 97.92%] [G loss: 4.242873]\n",
      "4-20000 [D loss: 0.158338, acc.: 95.83%] [G loss: 4.100681]\n",
      "-----------------------------------------------------------------\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_3 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_12 (LeakyReLU)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 533,505\n",
      "Trainable params: 533,505\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_18 (Dense)             (None, 256)               25856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_13 (LeakyReLU)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_14 (LeakyReLU)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_15 (LeakyReLU)   (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 784)               803600    \n",
      "_________________________________________________________________\n",
      "reshape_3 (Reshape)          (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 1,493,520\n",
      "Trainable params: 1,489,936\n",
      "Non-trainable params: 3,584\n",
      "_________________________________________________________________\n",
      "5-0 [D loss: 0.631504, acc.: 64.58%] [G loss: 0.519460]\n",
      "5-500 [D loss: 0.244970, acc.: 90.62%] [G loss: 9.110124]\n",
      "5-1000 [D loss: 0.573987, acc.: 59.38%] [G loss: 3.705923]\n",
      "5-1500 [D loss: 0.561963, acc.: 63.54%] [G loss: 1.695387]\n",
      "5-2000 [D loss: 0.482974, acc.: 76.04%] [G loss: 1.688059]\n",
      "5-2500 [D loss: 0.578895, acc.: 73.96%] [G loss: 2.515985]\n",
      "5-3000 [D loss: 0.363445, acc.: 84.38%] [G loss: 3.652734]\n",
      "5-3500 [D loss: 0.190707, acc.: 93.75%] [G loss: 3.352417]\n",
      "5-4000 [D loss: 0.315894, acc.: 90.62%] [G loss: 3.211989]\n",
      "5-4500 [D loss: 0.258847, acc.: 90.62%] [G loss: 4.144951]\n",
      "5-5000 [D loss: 0.170584, acc.: 94.79%] [G loss: 5.605952]\n",
      "5-5500 [D loss: 0.166461, acc.: 97.92%] [G loss: 4.596508]\n",
      "5-6000 [D loss: 0.168388, acc.: 94.79%] [G loss: 4.473694]\n",
      "5-6500 [D loss: 0.200502, acc.: 93.75%] [G loss: 4.385603]\n",
      "5-7000 [D loss: 0.974850, acc.: 77.08%] [G loss: 3.521154]\n",
      "5-7500 [D loss: 0.234269, acc.: 89.58%] [G loss: 3.426919]\n",
      "5-8000 [D loss: 0.140849, acc.: 96.88%] [G loss: 4.939515]\n",
      "5-8500 [D loss: 0.227925, acc.: 89.58%] [G loss: 3.867920]\n",
      "5-9000 [D loss: 0.140203, acc.: 96.88%] [G loss: 3.725284]\n",
      "5-9500 [D loss: 0.245653, acc.: 92.71%] [G loss: 3.986286]\n",
      "5-10000 [D loss: 0.191869, acc.: 94.79%] [G loss: 4.155670]\n",
      "5-10500 [D loss: 0.122904, acc.: 93.75%] [G loss: 5.631522]\n",
      "5-11000 [D loss: 0.228343, acc.: 89.58%] [G loss: 4.807909]\n",
      "5-11500 [D loss: 0.106361, acc.: 95.83%] [G loss: 3.998555]\n",
      "5-12000 [D loss: 0.119053, acc.: 96.88%] [G loss: 4.041099]\n",
      "5-12500 [D loss: 0.063805, acc.: 98.96%] [G loss: 3.653746]\n",
      "5-13000 [D loss: 0.171792, acc.: 91.67%] [G loss: 4.419324]\n",
      "5-13500 [D loss: 0.102042, acc.: 96.88%] [G loss: 4.392728]\n",
      "5-14000 [D loss: 0.247172, acc.: 91.67%] [G loss: 3.058980]\n",
      "5-14500 [D loss: 0.133318, acc.: 94.79%] [G loss: 4.227814]\n",
      "5-15000 [D loss: 0.240392, acc.: 92.71%] [G loss: 3.344191]\n",
      "5-15500 [D loss: 0.206741, acc.: 93.75%] [G loss: 3.321685]\n",
      "5-16000 [D loss: 0.074257, acc.: 95.83%] [G loss: 4.587860]\n",
      "5-16500 [D loss: 0.145864, acc.: 94.79%] [G loss: 4.397085]\n",
      "5-17000 [D loss: 0.229368, acc.: 90.62%] [G loss: 2.711945]\n",
      "5-17500 [D loss: 0.106933, acc.: 98.96%] [G loss: 4.163724]\n",
      "5-18000 [D loss: 0.211254, acc.: 92.71%] [G loss: 2.750491]\n",
      "5-18500 [D loss: 0.110372, acc.: 96.88%] [G loss: 4.190903]\n",
      "5-19000 [D loss: 0.117334, acc.: 95.83%] [G loss: 3.604196]\n",
      "5-19500 [D loss: 0.118139, acc.: 96.88%] [G loss: 4.392365]\n",
      "5-20000 [D loss: 0.212010, acc.: 89.58%] [G loss: 4.289959]\n",
      "-----------------------------------------------------------------\n",
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_4 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_16 (LeakyReLU)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_17 (LeakyReLU)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 533,505\n",
      "Trainable params: 533,505\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_25 (Dense)             (None, 256)               25856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_18 (LeakyReLU)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_19 (LeakyReLU)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_20 (LeakyReLU)   (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 784)               803600    \n",
      "_________________________________________________________________\n",
      "reshape_4 (Reshape)          (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 1,493,520\n",
      "Trainable params: 1,489,936\n",
      "Non-trainable params: 3,584\n",
      "_________________________________________________________________\n",
      "6-0 [D loss: 0.700018, acc.: 58.33%] [G loss: 0.596262]\n",
      "6-500 [D loss: 0.253951, acc.: 90.62%] [G loss: 5.712351]\n",
      "6-1000 [D loss: 0.373907, acc.: 87.50%] [G loss: 3.182943]\n",
      "6-1500 [D loss: 0.244964, acc.: 94.79%] [G loss: 3.671803]\n",
      "6-2000 [D loss: 0.436090, acc.: 69.79%] [G loss: 3.404960]\n",
      "6-2500 [D loss: 0.137717, acc.: 98.96%] [G loss: 3.931037]\n",
      "6-3000 [D loss: 0.315757, acc.: 90.62%] [G loss: 3.727412]\n",
      "6-3500 [D loss: 0.193808, acc.: 94.79%] [G loss: 3.501079]\n",
      "6-4000 [D loss: 0.261317, acc.: 90.62%] [G loss: 3.221184]\n",
      "6-4500 [D loss: 0.354968, acc.: 85.42%] [G loss: 2.334225]\n",
      "6-5000 [D loss: 0.355699, acc.: 85.42%] [G loss: 2.684989]\n",
      "6-5500 [D loss: 0.304438, acc.: 88.54%] [G loss: 2.747623]\n",
      "6-6000 [D loss: 0.253973, acc.: 87.50%] [G loss: 2.940639]\n",
      "6-6500 [D loss: 0.204728, acc.: 94.79%] [G loss: 3.381097]\n",
      "6-7000 [D loss: 0.192470, acc.: 93.75%] [G loss: 3.241573]\n",
      "6-7500 [D loss: 0.157772, acc.: 93.75%] [G loss: 3.757294]\n",
      "6-8000 [D loss: 0.248251, acc.: 87.50%] [G loss: 3.731177]\n",
      "6-8500 [D loss: 0.355197, acc.: 81.25%] [G loss: 3.242331]\n",
      "6-9000 [D loss: 0.249226, acc.: 90.62%] [G loss: 2.958632]\n",
      "6-9500 [D loss: 0.269734, acc.: 88.54%] [G loss: 2.511791]\n",
      "6-10000 [D loss: 0.157971, acc.: 94.79%] [G loss: 3.768198]\n",
      "6-10500 [D loss: 0.164048, acc.: 94.79%] [G loss: 4.331674]\n",
      "6-11000 [D loss: 0.217212, acc.: 93.75%] [G loss: 3.138102]\n",
      "6-11500 [D loss: 0.305055, acc.: 87.50%] [G loss: 3.733655]\n",
      "6-12000 [D loss: 0.184503, acc.: 93.75%] [G loss: 4.612410]\n",
      "6-12500 [D loss: 0.266627, acc.: 90.62%] [G loss: 2.812876]\n",
      "6-13000 [D loss: 0.150975, acc.: 96.88%] [G loss: 3.832851]\n",
      "6-13500 [D loss: 0.190798, acc.: 94.79%] [G loss: 4.143119]\n",
      "6-14000 [D loss: 0.168982, acc.: 91.67%] [G loss: 3.741477]\n",
      "6-14500 [D loss: 0.189378, acc.: 93.75%] [G loss: 3.246381]\n",
      "6-15000 [D loss: 0.277400, acc.: 84.38%] [G loss: 3.069271]\n",
      "6-15500 [D loss: 0.191557, acc.: 93.75%] [G loss: 3.039819]\n",
      "6-16000 [D loss: 0.186365, acc.: 90.62%] [G loss: 3.293277]\n",
      "6-16500 [D loss: 0.483894, acc.: 79.17%] [G loss: 3.389085]\n",
      "6-17000 [D loss: 0.136911, acc.: 95.83%] [G loss: 3.845231]\n",
      "6-17500 [D loss: 0.316094, acc.: 87.50%] [G loss: 2.534030]\n",
      "6-18000 [D loss: 0.321235, acc.: 87.50%] [G loss: 3.050556]\n",
      "6-18500 [D loss: 0.259496, acc.: 87.50%] [G loss: 2.611927]\n",
      "6-19000 [D loss: 0.121334, acc.: 94.79%] [G loss: 4.021949]\n",
      "6-19500 [D loss: 0.212871, acc.: 93.75%] [G loss: 3.941919]\n",
      "6-20000 [D loss: 0.297012, acc.: 88.54%] [G loss: 3.293138]\n",
      "-----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "#     %%time\n",
    "    aksara=[2,4,5,6]\n",
    "    aksara_num=[24,8,19,24]\n",
    "    for i in range(0,4):\n",
    "        gan = GAN()\n",
    "        gan.train(aksara[i], aksara_num[i], epochs=30001, batch_size=48, sample_interval=1000)\n",
    "        print(\"-----------------------------------------------------------------\")\n",
    "#     a = [0,2,4,5,6,7,12,13,18,19,20,25,26,27]\n",
    "#     for i in a:\n",
    "#         gan.train(aksara=i, epochs=30001, batch_size=32, sample_interval=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "aksara=2\n",
    "aksara_num=27\n",
    "gan = GAN()\n",
    "gan.train(aksara, aksara_num, epochs=20001, batch_size=48, sample_interval=1000)\n",
    "print(\"-----------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Sadrakh\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 533,505\n",
      "Trainable params: 533,505\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 256)               25856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 784)               803600    \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 1,493,520\n",
      "Trainable params: 1,489,936\n",
      "Non-trainable params: 3,584\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From C:\\Users\\Sadrakh\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sadrakh\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4-0 [D loss: 0.656645, acc.: 55.21%] [G loss: 0.573448]\n",
      "4-1000 [D loss: 0.062383, acc.: 100.00%] [G loss: 4.644412]\n",
      "4-2000 [D loss: 0.178460, acc.: 92.71%] [G loss: 3.899518]\n",
      "4-3000 [D loss: 0.268067, acc.: 89.58%] [G loss: 2.960062]\n",
      "4-4000 [D loss: 0.184979, acc.: 91.67%] [G loss: 3.417551]\n",
      "4-5000 [D loss: 0.195814, acc.: 91.67%] [G loss: 3.114289]\n",
      "4-6000 [D loss: 0.166421, acc.: 91.67%] [G loss: 3.649276]\n",
      "4-7000 [D loss: 0.188624, acc.: 93.75%] [G loss: 3.563302]\n",
      "4-8000 [D loss: 0.171223, acc.: 90.62%] [G loss: 4.374502]\n",
      "4-9000 [D loss: 0.240093, acc.: 88.54%] [G loss: 3.300213]\n",
      "4-10000 [D loss: 0.299209, acc.: 84.38%] [G loss: 4.110332]\n",
      "4-11000 [D loss: 0.213784, acc.: 90.62%] [G loss: 3.510488]\n",
      "4-12000 [D loss: 0.155748, acc.: 94.79%] [G loss: 6.178015]\n",
      "4-13000 [D loss: 0.171447, acc.: 91.67%] [G loss: 4.502308]\n",
      "4-14000 [D loss: 0.149584, acc.: 92.71%] [G loss: 5.515707]\n",
      "4-15000 [D loss: 0.116943, acc.: 95.83%] [G loss: 4.856581]\n",
      "4-16000 [D loss: 0.150021, acc.: 95.83%] [G loss: 4.427292]\n",
      "4-17000 [D loss: 0.278335, acc.: 89.58%] [G loss: 5.066295]\n",
      "4-18000 [D loss: 0.170889, acc.: 95.83%] [G loss: 4.956745]\n",
      "4-19000 [D loss: 0.124432, acc.: 94.79%] [G loss: 5.287670]\n",
      "4-20000 [D loss: 0.127217, acc.: 92.71%] [G loss: 4.864821]\n",
      "4-21000 [D loss: 0.111181, acc.: 96.88%] [G loss: 5.492794]\n",
      "4-22000 [D loss: 0.039243, acc.: 100.00%] [G loss: 5.736171]\n",
      "4-23000 [D loss: 0.061033, acc.: 97.92%] [G loss: 5.647256]\n",
      "4-24000 [D loss: 0.152387, acc.: 93.75%] [G loss: 7.235479]\n",
      "4-25000 [D loss: 0.172595, acc.: 91.67%] [G loss: 4.672633]\n",
      "4-26000 [D loss: 0.076653, acc.: 95.83%] [G loss: 6.138446]\n",
      "4-27000 [D loss: 0.168017, acc.: 92.71%] [G loss: 4.871282]\n",
      "4-28000 [D loss: 0.078838, acc.: 94.79%] [G loss: 6.758213]\n",
      "4-29000 [D loss: 0.137852, acc.: 94.79%] [G loss: 6.669613]\n",
      "4-30000 [D loss: 0.067132, acc.: 98.96%] [G loss: 8.605628]\n",
      "-----------------------------------------------------------------\n",
      "--- 6173.006311416626 seconds ---\n",
      "-----------------------------------------------------------------\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_2 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 533,505\n",
      "Trainable params: 533,505\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_11 (Dense)             (None, 256)               25856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)   (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 784)               803600    \n",
      "_________________________________________________________________\n",
      "reshape_2 (Reshape)          (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 1,493,520\n",
      "Trainable params: 1,489,936\n",
      "Non-trainable params: 3,584\n",
      "_________________________________________________________________\n",
      "5-0 [D loss: 0.798500, acc.: 27.08%] [G loss: 0.663940]\n",
      "5-1000 [D loss: 0.160102, acc.: 96.88%] [G loss: 4.151351]\n",
      "5-2000 [D loss: 0.183167, acc.: 93.75%] [G loss: 3.279059]\n",
      "5-3000 [D loss: 0.155190, acc.: 92.71%] [G loss: 3.891845]\n",
      "5-4000 [D loss: 0.120477, acc.: 96.88%] [G loss: 3.768231]\n",
      "5-5000 [D loss: 0.283901, acc.: 88.54%] [G loss: 3.074628]\n",
      "5-6000 [D loss: 0.100130, acc.: 96.88%] [G loss: 4.547815]\n",
      "5-7000 [D loss: 0.142169, acc.: 96.88%] [G loss: 3.521765]\n",
      "5-8000 [D loss: 0.147364, acc.: 89.58%] [G loss: 4.669350]\n",
      "5-9000 [D loss: 0.199307, acc.: 90.62%] [G loss: 3.784778]\n",
      "5-10000 [D loss: 0.086323, acc.: 95.83%] [G loss: 6.604368]\n",
      "5-11000 [D loss: 0.185970, acc.: 93.75%] [G loss: 5.427149]\n",
      "5-12000 [D loss: 0.133445, acc.: 92.71%] [G loss: 5.080995]\n",
      "5-13000 [D loss: 0.136547, acc.: 93.75%] [G loss: 6.545124]\n",
      "5-14000 [D loss: 0.178993, acc.: 94.79%] [G loss: 3.183501]\n",
      "5-15000 [D loss: 0.187963, acc.: 91.67%] [G loss: 5.162758]\n",
      "5-16000 [D loss: 0.104767, acc.: 95.83%] [G loss: 5.392315]\n",
      "5-17000 [D loss: 0.127543, acc.: 94.79%] [G loss: 6.266728]\n",
      "5-18000 [D loss: 0.051037, acc.: 97.92%] [G loss: 5.490716]\n",
      "5-19000 [D loss: 0.119919, acc.: 91.67%] [G loss: 5.477774]\n",
      "5-20000 [D loss: 0.167058, acc.: 94.79%] [G loss: 5.354958]\n",
      "5-21000 [D loss: 0.176938, acc.: 95.83%] [G loss: 4.738417]\n",
      "5-22000 [D loss: 0.160760, acc.: 93.75%] [G loss: 7.212061]\n",
      "5-23000 [D loss: 0.117503, acc.: 97.92%] [G loss: 7.144920]\n",
      "5-24000 [D loss: 0.071651, acc.: 96.88%] [G loss: 4.196896]\n",
      "5-25000 [D loss: 0.194606, acc.: 95.83%] [G loss: 4.265205]\n",
      "5-26000 [D loss: 0.078377, acc.: 97.92%] [G loss: 6.349785]\n",
      "5-27000 [D loss: 0.044514, acc.: 97.92%] [G loss: 6.170094]\n",
      "5-28000 [D loss: 0.051041, acc.: 96.88%] [G loss: 5.233138]\n",
      "5-29000 [D loss: 0.162266, acc.: 96.88%] [G loss: 3.979326]\n",
      "5-30000 [D loss: 0.232981, acc.: 93.75%] [G loss: 2.990072]\n",
      "-----------------------------------------------------------------\n",
      "--- 12643.003291606903 seconds ---\n",
      "-----------------------------------------------------------------\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_3 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_12 (LeakyReLU)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 533,505\n",
      "Trainable params: 533,505\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_18 (Dense)             (None, 256)               25856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_13 (LeakyReLU)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_14 (LeakyReLU)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_15 (LeakyReLU)   (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 784)               803600    \n",
      "_________________________________________________________________\n",
      "reshape_3 (Reshape)          (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 1,493,520\n",
      "Trainable params: 1,489,936\n",
      "Non-trainable params: 3,584\n",
      "_________________________________________________________________\n",
      "6-0 [D loss: 0.937459, acc.: 40.62%] [G loss: 0.915241]\n",
      "6-1000 [D loss: 0.243600, acc.: 85.42%] [G loss: 3.068888]\n",
      "6-2000 [D loss: 0.273278, acc.: 88.54%] [G loss: 3.183471]\n",
      "6-3000 [D loss: 0.156275, acc.: 91.67%] [G loss: 3.047254]\n",
      "6-4000 [D loss: 0.203474, acc.: 86.46%] [G loss: 4.826177]\n",
      "6-5000 [D loss: 0.105947, acc.: 96.88%] [G loss: 4.103063]\n",
      "6-6000 [D loss: 0.327871, acc.: 77.08%] [G loss: 3.483113]\n",
      "6-7000 [D loss: 0.121377, acc.: 95.83%] [G loss: 6.811057]\n",
      "6-8000 [D loss: 0.134990, acc.: 92.71%] [G loss: 6.121214]\n",
      "6-9000 [D loss: 0.122767, acc.: 92.71%] [G loss: 7.609153]\n",
      "6-10000 [D loss: 0.259637, acc.: 81.25%] [G loss: 3.541131]\n",
      "6-11000 [D loss: 0.100179, acc.: 97.92%] [G loss: 7.226152]\n",
      "6-12000 [D loss: 0.204114, acc.: 91.67%] [G loss: 6.735401]\n",
      "6-13000 [D loss: 0.124593, acc.: 93.75%] [G loss: 6.408133]\n",
      "6-14000 [D loss: 0.151528, acc.: 96.88%] [G loss: 6.592419]\n",
      "6-15000 [D loss: 0.041225, acc.: 97.92%] [G loss: 5.426282]\n",
      "6-16000 [D loss: 0.072842, acc.: 97.92%] [G loss: 5.080504]\n",
      "6-17000 [D loss: 0.039218, acc.: 98.96%] [G loss: 4.720566]\n",
      "6-18000 [D loss: 0.217638, acc.: 93.75%] [G loss: 3.108974]\n",
      "6-19000 [D loss: 0.092501, acc.: 97.92%] [G loss: 3.837157]\n",
      "6-20000 [D loss: 0.153550, acc.: 95.83%] [G loss: 4.733122]\n",
      "6-21000 [D loss: 0.076276, acc.: 97.92%] [G loss: 4.768836]\n",
      "6-22000 [D loss: 0.127803, acc.: 97.92%] [G loss: 4.587227]\n",
      "6-23000 [D loss: 0.212733, acc.: 93.75%] [G loss: 2.955082]\n",
      "6-24000 [D loss: 0.182442, acc.: 93.75%] [G loss: 3.000496]\n",
      "6-25000 [D loss: 0.070406, acc.: 98.96%] [G loss: 4.844503]\n",
      "6-26000 [D loss: 0.095045, acc.: 97.92%] [G loss: 3.924874]\n",
      "6-27000 [D loss: 0.103142, acc.: 96.88%] [G loss: 4.249686]\n",
      "6-28000 [D loss: 0.198342, acc.: 94.79%] [G loss: 3.219075]\n",
      "6-29000 [D loss: 0.092199, acc.: 96.88%] [G loss: 3.889884]\n",
      "6-30000 [D loss: 0.181175, acc.: 94.79%] [G loss: 3.547388]\n",
      "-----------------------------------------------------------------\n",
      "--- 19416.00870370865 seconds ---\n",
      "-----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == '__main__':\n",
    "    start_time = time.time()\n",
    "    aksara=[4,5,6]\n",
    "    aksara_num=[8,19,24]\n",
    "    for i in range(0,3):\n",
    "        gan = GAN()\n",
    "        gan.train(aksara[i], aksara_num[i], epochs=30001, batch_size=48, sample_interval=1000)\n",
    "        print(\"-----------------------------------------------------------------\")\n",
    "        print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "        print(\"-----------------------------------------------------------------\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Sadrakh\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 533,505\n",
      "Trainable params: 533,505\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 256)               25856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 784)               803600    \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 1,493,520\n",
      "Trainable params: 1,489,936\n",
      "Non-trainable params: 3,584\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From C:\\Users\\Sadrakh\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sadrakh\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7-0 [D loss: 0.613881, acc.: 53.12%] [G loss: 0.391691]\n",
      "7-1000 [D loss: 0.111522, acc.: 96.88%] [G loss: 3.185141]\n",
      "7-2000 [D loss: 0.229851, acc.: 90.62%] [G loss: 2.785324]\n",
      "7-3000 [D loss: 0.378411, acc.: 82.29%] [G loss: 2.221233]\n",
      "7-4000 [D loss: 0.108123, acc.: 97.92%] [G loss: 3.934483]\n",
      "7-5000 [D loss: 0.201110, acc.: 91.67%] [G loss: 2.901339]\n",
      "7-6000 [D loss: 0.128235, acc.: 95.83%] [G loss: 4.594060]\n",
      "7-7000 [D loss: 0.093042, acc.: 94.79%] [G loss: 7.477497]\n",
      "7-8000 [D loss: 0.149597, acc.: 92.71%] [G loss: 4.417034]\n",
      "7-9000 [D loss: 0.100019, acc.: 93.75%] [G loss: 6.364870]\n",
      "7-10000 [D loss: 0.138190, acc.: 93.75%] [G loss: 4.190226]\n",
      "7-11000 [D loss: 0.243130, acc.: 92.71%] [G loss: 4.631813]\n",
      "7-12000 [D loss: 0.093751, acc.: 97.92%] [G loss: 5.355133]\n",
      "7-13000 [D loss: 0.170738, acc.: 90.62%] [G loss: 4.397361]\n",
      "7-14000 [D loss: 0.096704, acc.: 95.83%] [G loss: 5.205811]\n",
      "7-15000 [D loss: 0.193132, acc.: 91.67%] [G loss: 4.824945]\n",
      "7-16000 [D loss: 0.098284, acc.: 95.83%] [G loss: 5.268368]\n",
      "7-17000 [D loss: 0.069911, acc.: 97.92%] [G loss: 5.381516]\n",
      "7-18000 [D loss: 0.175456, acc.: 92.71%] [G loss: 6.602521]\n",
      "7-19000 [D loss: 0.089229, acc.: 96.88%] [G loss: 6.971452]\n",
      "7-20000 [D loss: 0.068105, acc.: 97.92%] [G loss: 8.140721]\n",
      "7-21000 [D loss: 0.473924, acc.: 91.67%] [G loss: 7.465951]\n",
      "7-22000 [D loss: 0.043735, acc.: 97.92%] [G loss: 6.702293]\n",
      "7-23000 [D loss: 0.121626, acc.: 96.88%] [G loss: 6.782640]\n",
      "7-24000 [D loss: 0.101631, acc.: 95.83%] [G loss: 6.478197]\n",
      "7-25000 [D loss: 0.081116, acc.: 97.92%] [G loss: 5.876663]\n",
      "7-26000 [D loss: 0.129858, acc.: 96.88%] [G loss: 4.182192]\n",
      "7-27000 [D loss: 0.075304, acc.: 97.92%] [G loss: 4.420001]\n",
      "7-28000 [D loss: 0.131995, acc.: 96.88%] [G loss: 4.993407]\n",
      "7-29000 [D loss: 0.119985, acc.: 96.88%] [G loss: 4.964036]\n",
      "7-30000 [D loss: 0.051901, acc.: 98.96%] [G loss: 4.242931]\n",
      "-----------------------------------------------------------------\n",
      "--- 5956.601973056793 seconds ---\n",
      "-----------------------------------------------------------------\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_2 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 533,505\n",
      "Trainable params: 533,505\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_11 (Dense)             (None, 256)               25856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)   (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 784)               803600    \n",
      "_________________________________________________________________\n",
      "reshape_2 (Reshape)          (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 1,493,520\n",
      "Trainable params: 1,489,936\n",
      "Non-trainable params: 3,584\n",
      "_________________________________________________________________\n",
      "12-0 [D loss: 1.210719, acc.: 37.50%] [G loss: 0.883896]\n",
      "12-1000 [D loss: 0.195706, acc.: 92.71%] [G loss: 3.274049]\n",
      "12-2000 [D loss: 0.303093, acc.: 88.54%] [G loss: 2.797080]\n",
      "12-3000 [D loss: 0.223446, acc.: 90.62%] [G loss: 2.982732]\n",
      "12-4000 [D loss: 0.232415, acc.: 91.67%] [G loss: 2.880023]\n",
      "12-5000 [D loss: 0.164521, acc.: 93.75%] [G loss: 3.938821]\n",
      "12-6000 [D loss: 0.263636, acc.: 88.54%] [G loss: 2.916161]\n",
      "12-7000 [D loss: 0.297614, acc.: 85.42%] [G loss: 4.680474]\n",
      "12-8000 [D loss: 0.264340, acc.: 86.46%] [G loss: 4.148704]\n",
      "12-9000 [D loss: 0.184878, acc.: 94.79%] [G loss: 4.453087]\n",
      "12-10000 [D loss: 0.097027, acc.: 97.92%] [G loss: 4.356152]\n",
      "12-11000 [D loss: 0.158984, acc.: 95.83%] [G loss: 3.090884]\n",
      "12-12000 [D loss: 0.107133, acc.: 93.75%] [G loss: 5.821999]\n",
      "12-13000 [D loss: 0.191464, acc.: 92.71%] [G loss: 4.143780]\n",
      "12-14000 [D loss: 0.124889, acc.: 94.79%] [G loss: 6.156942]\n",
      "12-15000 [D loss: 0.137551, acc.: 94.79%] [G loss: 5.530186]\n",
      "12-16000 [D loss: 0.258233, acc.: 88.54%] [G loss: 5.041167]\n",
      "12-17000 [D loss: 0.202538, acc.: 90.62%] [G loss: 3.897241]\n",
      "12-18000 [D loss: 0.127483, acc.: 93.75%] [G loss: 5.204988]\n",
      "12-19000 [D loss: 0.102276, acc.: 95.83%] [G loss: 5.292191]\n",
      "12-20000 [D loss: 0.040656, acc.: 98.96%] [G loss: 7.091648]\n",
      "12-21000 [D loss: 0.131838, acc.: 93.75%] [G loss: 7.040596]\n",
      "12-22000 [D loss: 0.066553, acc.: 97.92%] [G loss: 6.394616]\n",
      "12-23000 [D loss: 0.206571, acc.: 90.62%] [G loss: 4.848463]\n",
      "12-24000 [D loss: 0.212761, acc.: 91.67%] [G loss: 5.904721]\n",
      "12-25000 [D loss: 0.057278, acc.: 96.88%] [G loss: 4.968210]\n",
      "12-26000 [D loss: 0.141783, acc.: 93.75%] [G loss: 3.288199]\n",
      "12-27000 [D loss: 0.061868, acc.: 96.88%] [G loss: 6.663773]\n",
      "12-28000 [D loss: 0.220461, acc.: 93.75%] [G loss: 5.578005]\n",
      "12-29000 [D loss: 0.126342, acc.: 96.88%] [G loss: 3.987002]\n",
      "12-30000 [D loss: 0.135696, acc.: 93.75%] [G loss: 6.442898]\n",
      "-----------------------------------------------------------------\n",
      "--- 11713.339524269104 seconds ---\n",
      "-----------------------------------------------------------------\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_3 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_12 (LeakyReLU)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 533,505\n",
      "Trainable params: 533,505\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_18 (Dense)             (None, 256)               25856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_13 (LeakyReLU)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_14 (LeakyReLU)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_15 (LeakyReLU)   (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 784)               803600    \n",
      "_________________________________________________________________\n",
      "reshape_3 (Reshape)          (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 1,493,520\n",
      "Trainable params: 1,489,936\n",
      "Non-trainable params: 3,584\n",
      "_________________________________________________________________\n",
      "13-0 [D loss: 0.587370, acc.: 68.75%] [G loss: 0.555967]\n",
      "13-1000 [D loss: 0.334043, acc.: 85.42%] [G loss: 2.259842]\n",
      "13-2000 [D loss: 0.170418, acc.: 92.71%] [G loss: 2.708074]\n",
      "13-3000 [D loss: 0.295946, acc.: 88.54%] [G loss: 2.181596]\n",
      "13-4000 [D loss: 0.206210, acc.: 89.58%] [G loss: 2.879141]\n",
      "13-5000 [D loss: 0.314314, acc.: 84.38%] [G loss: 1.924340]\n",
      "13-6000 [D loss: 0.143719, acc.: 92.71%] [G loss: 3.994699]\n",
      "13-7000 [D loss: 0.183353, acc.: 92.71%] [G loss: 3.326486]\n",
      "13-8000 [D loss: 0.216738, acc.: 94.79%] [G loss: 2.996053]\n",
      "13-9000 [D loss: 0.203113, acc.: 89.58%] [G loss: 3.288648]\n",
      "13-10000 [D loss: 0.189734, acc.: 95.83%] [G loss: 3.720930]\n",
      "13-11000 [D loss: 0.190237, acc.: 92.71%] [G loss: 3.615553]\n",
      "13-12000 [D loss: 0.127262, acc.: 94.79%] [G loss: 4.197227]\n",
      "13-13000 [D loss: 0.178035, acc.: 94.79%] [G loss: 3.303855]\n",
      "13-14000 [D loss: 0.043061, acc.: 100.00%] [G loss: 5.403156]\n",
      "13-15000 [D loss: 0.161280, acc.: 95.83%] [G loss: 5.047343]\n",
      "13-16000 [D loss: 0.118939, acc.: 97.92%] [G loss: 3.477617]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == '__main__':\n",
    "    start_time = time.time()\n",
    "    aksara=[7,12,13]\n",
    "    aksara_num=[24,21,24]\n",
    "    for i in range(0,3):\n",
    "        gan = GAN()\n",
    "        gan.train(aksara[i], aksara_num[i], epochs=30001, batch_size=48, sample_interval=1000)\n",
    "        print(\"-----------------------------------------------------------------\")\n",
    "        print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "        print(\"-----------------------------------------------------------------\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Sadrakh\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 533,505\n",
      "Trainable params: 533,505\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 256)               25856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 784)               803600    \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 1,493,520\n",
      "Trainable params: 1,489,936\n",
      "Non-trainable params: 3,584\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From C:\\Users\\Sadrakh\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sadrakh\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18-0 [D loss: 0.907992, acc.: 7.03%] [G loss: 0.618043]\n",
      "18-1000 [D loss: 0.209905, acc.: 95.31%] [G loss: 2.487604]\n",
      "18-2000 [D loss: 0.255851, acc.: 85.55%] [G loss: 2.927588]\n",
      "18-3000 [D loss: 0.277628, acc.: 88.28%] [G loss: 2.632940]\n",
      "18-4000 [D loss: 0.227184, acc.: 82.03%] [G loss: 2.976219]\n",
      "18-5000 [D loss: 0.196316, acc.: 91.02%] [G loss: 4.396988]\n",
      "18-6000 [D loss: 0.239828, acc.: 86.33%] [G loss: 3.966166]\n",
      "18-7000 [D loss: 0.179957, acc.: 94.53%] [G loss: 4.397008]\n",
      "18-8000 [D loss: 0.168606, acc.: 92.97%] [G loss: 3.747859]\n",
      "18-9000 [D loss: 0.152018, acc.: 89.45%] [G loss: 5.489285]\n",
      "18-10000 [D loss: 0.133651, acc.: 95.70%] [G loss: 4.235358]\n",
      "18-11000 [D loss: 0.203640, acc.: 90.62%] [G loss: 3.799554]\n",
      "18-12000 [D loss: 0.281951, acc.: 92.58%] [G loss: 6.008434]\n",
      "18-13000 [D loss: 0.154103, acc.: 98.05%] [G loss: 4.255429]\n",
      "18-14000 [D loss: 0.143084, acc.: 96.48%] [G loss: 5.156842]\n",
      "18-15000 [D loss: 0.149226, acc.: 95.70%] [G loss: 5.099802]\n",
      "18-16000 [D loss: 0.158645, acc.: 94.53%] [G loss: 4.224014]\n",
      "18-17000 [D loss: 0.291460, acc.: 86.72%] [G loss: 3.771049]\n",
      "18-18000 [D loss: 0.141442, acc.: 93.75%] [G loss: 5.996325]\n",
      "18-19000 [D loss: 0.147120, acc.: 91.80%] [G loss: 4.174086]\n",
      "18-20000 [D loss: 0.246893, acc.: 92.19%] [G loss: 3.904228]\n",
      "18-21000 [D loss: 0.131999, acc.: 93.75%] [G loss: 7.243038]\n",
      "18-22000 [D loss: 0.143869, acc.: 94.14%] [G loss: 4.751493]\n",
      "18-23000 [D loss: 0.145993, acc.: 95.31%] [G loss: 3.935245]\n",
      "18-24000 [D loss: 0.159811, acc.: 93.75%] [G loss: 5.236197]\n",
      "18-25000 [D loss: 0.129079, acc.: 94.92%] [G loss: 4.176482]\n",
      "18-26000 [D loss: 0.134346, acc.: 94.53%] [G loss: 4.561479]\n",
      "18-27000 [D loss: 0.216638, acc.: 94.92%] [G loss: 6.141238]\n",
      "18-28000 [D loss: 0.198613, acc.: 93.75%] [G loss: 3.360356]\n",
      "18-29000 [D loss: 0.075854, acc.: 96.88%] [G loss: 6.722688]\n",
      "18-30000 [D loss: 0.251987, acc.: 92.58%] [G loss: 4.249759]\n",
      "-----------------------------------------------------------------\n",
      "--- 9621.638617753983 seconds ---\n",
      "-----------------------------------------------------------------\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_2 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 533,505\n",
      "Trainable params: 533,505\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_11 (Dense)             (None, 256)               25856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)   (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 784)               803600    \n",
      "_________________________________________________________________\n",
      "reshape_2 (Reshape)          (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 1,493,520\n",
      "Trainable params: 1,489,936\n",
      "Non-trainable params: 3,584\n",
      "_________________________________________________________________\n",
      "25-0 [D loss: 1.145440, acc.: 43.75%] [G loss: 1.031198]\n",
      "25-1000 [D loss: 0.309358, acc.: 90.62%] [G loss: 2.628429]\n",
      "25-2000 [D loss: 0.259857, acc.: 88.67%] [G loss: 2.786038]\n",
      "25-3000 [D loss: 0.335539, acc.: 83.59%] [G loss: 3.131589]\n",
      "25-4000 [D loss: 0.222921, acc.: 89.45%] [G loss: 3.294040]\n",
      "25-5000 [D loss: 0.104350, acc.: 94.53%] [G loss: 5.039821]\n",
      "25-6000 [D loss: 0.107305, acc.: 97.66%] [G loss: 4.354774]\n",
      "25-7000 [D loss: 0.146312, acc.: 94.14%] [G loss: 4.287032]\n",
      "25-8000 [D loss: 0.166640, acc.: 95.70%] [G loss: 5.096658]\n",
      "25-9000 [D loss: 0.160131, acc.: 95.70%] [G loss: 5.409762]\n",
      "25-10000 [D loss: 0.113048, acc.: 96.48%] [G loss: 3.891697]\n",
      "25-11000 [D loss: 0.056422, acc.: 98.83%] [G loss: 4.804694]\n",
      "25-12000 [D loss: 0.157177, acc.: 95.70%] [G loss: 4.047874]\n",
      "25-13000 [D loss: 0.135295, acc.: 96.09%] [G loss: 4.333709]\n",
      "25-14000 [D loss: 0.157098, acc.: 96.88%] [G loss: 3.354763]\n",
      "25-15000 [D loss: 0.124462, acc.: 96.48%] [G loss: 4.239089]\n",
      "25-16000 [D loss: 0.090273, acc.: 97.66%] [G loss: 5.468383]\n",
      "25-17000 [D loss: 0.107442, acc.: 96.48%] [G loss: 5.490879]\n",
      "25-18000 [D loss: 0.066862, acc.: 96.88%] [G loss: 5.404817]\n",
      "25-19000 [D loss: 0.068857, acc.: 99.22%] [G loss: 3.556110]\n",
      "25-20000 [D loss: 0.150363, acc.: 96.48%] [G loss: 3.324736]\n",
      "25-21000 [D loss: 0.064080, acc.: 98.83%] [G loss: 4.021717]\n",
      "25-22000 [D loss: 0.130245, acc.: 96.48%] [G loss: 3.391051]\n",
      "25-23000 [D loss: 0.090007, acc.: 98.05%] [G loss: 3.419858]\n",
      "25-24000 [D loss: 0.102564, acc.: 97.27%] [G loss: 3.963012]\n",
      "25-25000 [D loss: 0.142028, acc.: 96.88%] [G loss: 3.232216]\n",
      "25-26000 [D loss: 0.075400, acc.: 98.44%] [G loss: 3.490840]\n",
      "25-27000 [D loss: 0.110071, acc.: 97.27%] [G loss: 3.199355]\n",
      "25-28000 [D loss: 0.061418, acc.: 98.83%] [G loss: 3.434327]\n",
      "25-29000 [D loss: 0.097535, acc.: 96.88%] [G loss: 4.371329]\n",
      "25-30000 [D loss: 0.086018, acc.: 98.05%] [G loss: 3.296061]\n",
      "-----------------------------------------------------------------\n",
      "--- 19936.57821917534 seconds ---\n",
      "-----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == '__main__':\n",
    "    start_time = time.time()\n",
    "    aksara=[18,25]\n",
    "    aksara_num=[22,24]\n",
    "    for i in range(0,2):\n",
    "        gan = GAN()\n",
    "        gan.train(aksara[i], aksara_num[i], epochs=30001, batch_size=128, sample_interval=1000)\n",
    "        print(\"-----------------------------------------------------------------\")\n",
    "        print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "        print(\"-----------------------------------------------------------------\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Sadrakh\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 533,505\n",
      "Trainable params: 533,505\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 256)               25856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 784)               803600    \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 1,493,520\n",
      "Trainable params: 1,489,936\n",
      "Non-trainable params: 3,584\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From C:\\Users\\Sadrakh\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sadrakh\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26-0 [D loss: 0.531538, acc.: 69.17%] [G loss: 0.644982]\n",
      "26-1000 [D loss: 0.097867, acc.: 95.83%] [G loss: 3.948570]\n",
      "26-2000 [D loss: 0.125469, acc.: 96.67%] [G loss: 4.371845]\n",
      "26-3000 [D loss: 0.253014, acc.: 90.00%] [G loss: 3.094129]\n",
      "26-4000 [D loss: 0.192931, acc.: 91.67%] [G loss: 3.183110]\n",
      "26-5000 [D loss: 0.179168, acc.: 93.33%] [G loss: 5.853185]\n",
      "26-6000 [D loss: 0.146033, acc.: 95.00%] [G loss: 3.779521]\n",
      "26-7000 [D loss: 0.128046, acc.: 94.17%] [G loss: 4.332162]\n",
      "26-8000 [D loss: 0.142244, acc.: 95.00%] [G loss: 3.665571]\n",
      "26-9000 [D loss: 0.141546, acc.: 95.83%] [G loss: 4.987321]\n",
      "26-10000 [D loss: 0.227880, acc.: 93.33%] [G loss: 3.916706]\n",
      "26-11000 [D loss: 0.097124, acc.: 96.67%] [G loss: 4.768473]\n",
      "26-12000 [D loss: 0.181296, acc.: 92.50%] [G loss: 3.462322]\n",
      "26-13000 [D loss: 0.088834, acc.: 95.83%] [G loss: 6.133641]\n",
      "26-14000 [D loss: 0.163759, acc.: 94.17%] [G loss: 4.200624]\n",
      "26-15000 [D loss: 0.199006, acc.: 92.50%] [G loss: 4.168287]\n",
      "26-16000 [D loss: 0.094765, acc.: 97.50%] [G loss: 4.796974]\n",
      "26-17000 [D loss: 0.057914, acc.: 97.50%] [G loss: 5.495949]\n",
      "26-18000 [D loss: 0.120592, acc.: 97.50%] [G loss: 4.706874]\n",
      "26-19000 [D loss: 0.107789, acc.: 96.67%] [G loss: 4.874159]\n",
      "26-20000 [D loss: 0.150756, acc.: 91.67%] [G loss: 4.988113]\n",
      "26-21000 [D loss: 0.092368, acc.: 95.83%] [G loss: 3.565859]\n",
      "26-22000 [D loss: 0.065367, acc.: 98.33%] [G loss: 4.935019]\n",
      "26-23000 [D loss: 0.117737, acc.: 95.83%] [G loss: 3.839171]\n",
      "26-24000 [D loss: 0.075431, acc.: 98.33%] [G loss: 4.455802]\n",
      "26-25000 [D loss: 0.131903, acc.: 95.83%] [G loss: 3.532491]\n",
      "26-26000 [D loss: 0.092083, acc.: 96.67%] [G loss: 5.066835]\n",
      "26-27000 [D loss: 0.268374, acc.: 92.50%] [G loss: 3.974247]\n",
      "26-28000 [D loss: 0.123946, acc.: 94.17%] [G loss: 3.391093]\n",
      "26-29000 [D loss: 0.088323, acc.: 97.50%] [G loss: 4.044559]\n",
      "26-30000 [D loss: 0.220572, acc.: 93.33%] [G loss: 3.421252]\n",
      "-----------------------------------------------------------------\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_2 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 533,505\n",
      "Trainable params: 533,505\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_11 (Dense)             (None, 256)               25856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)   (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 784)               803600    \n",
      "_________________________________________________________________\n",
      "reshape_2 (Reshape)          (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 1,493,520\n",
      "Trainable params: 1,489,936\n",
      "Non-trainable params: 3,584\n",
      "_________________________________________________________________\n",
      "27-0 [D loss: 0.705607, acc.: 57.50%] [G loss: 0.923397]\n",
      "27-1000 [D loss: 0.218950, acc.: 90.83%] [G loss: 3.913960]\n",
      "27-2000 [D loss: 0.322146, acc.: 86.67%] [G loss: 2.542232]\n",
      "27-3000 [D loss: 0.228608, acc.: 90.83%] [G loss: 3.020438]\n",
      "27-4000 [D loss: 0.254715, acc.: 84.17%] [G loss: 2.698087]\n",
      "27-5000 [D loss: 0.269527, acc.: 86.67%] [G loss: 3.013236]\n",
      "27-6000 [D loss: 0.157730, acc.: 91.67%] [G loss: 3.817128]\n",
      "27-7000 [D loss: 0.277022, acc.: 84.17%] [G loss: 3.929693]\n",
      "27-8000 [D loss: 0.191890, acc.: 94.17%] [G loss: 3.729800]\n",
      "27-9000 [D loss: 0.230408, acc.: 91.67%] [G loss: 4.301263]\n",
      "27-10000 [D loss: 0.155515, acc.: 92.50%] [G loss: 3.913834]\n",
      "27-11000 [D loss: 0.249333, acc.: 86.67%] [G loss: 2.753462]\n",
      "27-12000 [D loss: 0.180457, acc.: 94.17%] [G loss: 3.461237]\n",
      "27-13000 [D loss: 0.160539, acc.: 95.83%] [G loss: 4.088794]\n",
      "27-14000 [D loss: 0.218507, acc.: 85.83%] [G loss: 5.761946]\n",
      "27-15000 [D loss: 0.157141, acc.: 95.00%] [G loss: 4.919710]\n",
      "27-16000 [D loss: 0.215235, acc.: 89.17%] [G loss: 6.373223]\n",
      "27-17000 [D loss: 0.109916, acc.: 94.17%] [G loss: 4.705228]\n",
      "27-18000 [D loss: 0.081755, acc.: 100.00%] [G loss: 4.747838]\n",
      "27-19000 [D loss: 0.139702, acc.: 100.00%] [G loss: 5.486291]\n",
      "27-20000 [D loss: 0.121901, acc.: 94.17%] [G loss: 6.496187]\n",
      "27-21000 [D loss: 0.082722, acc.: 98.33%] [G loss: 6.205002]\n",
      "27-22000 [D loss: 0.242619, acc.: 85.83%] [G loss: 6.836701]\n",
      "27-23000 [D loss: 0.152467, acc.: 87.50%] [G loss: 6.231544]\n",
      "27-24000 [D loss: 0.271956, acc.: 87.50%] [G loss: 6.151049]\n",
      "27-25000 [D loss: 0.112961, acc.: 95.83%] [G loss: 5.673104]\n",
      "27-26000 [D loss: 0.097014, acc.: 96.67%] [G loss: 5.639245]\n",
      "27-27000 [D loss: 0.111271, acc.: 97.50%] [G loss: 6.318949]\n",
      "27-28000 [D loss: 0.195934, acc.: 92.50%] [G loss: 5.047119]\n",
      "27-29000 [D loss: 0.128827, acc.: 96.67%] [G loss: 4.534036]\n",
      "27-30000 [D loss: 0.061362, acc.: 99.17%] [G loss: 6.502620]\n",
      "-----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == '__main__':\n",
    "#     %%time\n",
    "    aksara=[26,27]\n",
    "    aksara_num=[22,24]\n",
    "    for i in range(0,2):\n",
    "        gan = GAN()\n",
    "        gan.train(aksara[i], aksara_num[i], epochs=30001, batch_size=60, sample_interval=1000)\n",
    "        print(\"-----------------------------------------------------------------\")\n",
    "#     a = [0,2,4,5,6,7,12,13,18,19,20,25,26,27]\n",
    "#     for i in a:\n",
    "#         gan.train(aksara=i, epochs=30001, batch_size=32, sample_interval=2000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
